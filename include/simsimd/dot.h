/**
 *  @brief SIMD-accelerated Dot Products for Real and Complex Numbers.
 *  @file include/simsimd/dot.h
 *  @author Ash Vardanian
 *  @date February 24, 2024
 *
 *  Contains:
 *
 *  - Dot Product for Real and Complex vectors
 *  - Conjugate Dot Product for Complex vectors
 *
 *  For datatypes:
 *
 *  - 64-bit IEEE floating point numbers
 *  - 32-bit IEEE floating point numbers
 *  - 16-bit IEEE floating point numbers
 *  - 16-bit brain floating point numbers
 *  - 8-bit unsigned integers
 *  - 8-bit signed integers
 *
 *  For hardware architectures:
 *
 *  - Arm: NEON, SVE
 *  - x86: Haswell, Ice Lake, Skylake, Genoa, Sapphire, Sierra
 *
 *  @section streaming_api Streaming API
 *
 *  For compile-time dispatch and vector-at-a-time accumulation, we provide streaming helpers
 *  that accept two `simsimd_b512_vec_t` blocks and update a running sum for non-complex dot
 *  products. The `<count>` suffix reflects how many scalars of that type fit in a 512-bit block.
 *  The helpers are exposed per scalar type as:
 *
 *  - simsimd_dot_<type>x<count>_state_<isa>_t
 *  - simsimd_dot_<type>x<count>_init_<isa>
 *  - simsimd_dot_<type>x<count>_update_<isa>
 *  - simsimd_dot_<type>x<count>_finalize_<isa>
 *
 *  @section x86_instructions Relevant x86 Instructions
 *
 *  Floating-point dot products use FMA (VFMADD231PS/PD) for sum += a[i]*b[i] accumulation.
 *  Integer i8 dot products use VPMADDUBSW (u8*i8->i16) + VPMADDWD (i16*1->i32) on Haswell,
 *  or the newer VNNI instructions VPDPBUSD/VPDPWSSD on Ice Lake+ for direct u8*i8->i32.
 *  BF16 dot products (VDPBF16PS) are Genoa-only, accumulating bf16 pairs directly to f32.
 *  Genoa shows 40% faster integer multiply-add (3c vs 5c) than Ice Lake.
 *
 *      Intrinsic               Instruction                     Haswell     Ice         Genoa
 *      _mm256_fmadd_ps         VFMADD231PS (YMM, YMM, YMM)     5c @ p01    4c @ p01    4c @ p01
 *      _mm256_fmadd_pd         VFMADD231PD (YMM, YMM, YMM)     5c @ p01    4c @ p01    4c @ p01
 *      _mm256_maddubs_epi16    VPMADDUBSW (YMM, YMM, YMM)      5c @ p0     5c @ p01    3c @ p01
 *      _mm256_madd_epi16       VPMADDWD (YMM, YMM, YMM)        5c @ p0     5c @ p01    3c @ p01
 *      _mm256_dpbusd_epi32     VPDPBUSD (YMM, YMM, YMM)        N/A         5c @ p01    4c @ p01
 *      _mm512_dpwssd_epi32     VPDPWSSD (ZMM, ZMM, ZMM)        N/A         5c @ p0     4c @ p01
 *      _mm512_dpbf16_ps        VDPBF16PS (ZMM, ZMM, ZMM)       N/A         N/A         6c @ p01
 *
 *  @section arm_neon_instructions Relevant ARM NEON Instructions
 *
 *  NEON integer dot products use SDOT/UDOT (ARMv8.2 dotprod) for direct i8*i8->i32 or u8*u8->u32
 *  accumulation - 4x faster than the multiply-add sequence on older cores. BFDOT (ARMv8.6 bf16)
 *  provides native bf16 dot products on Graviton 3+. Complex dot products use LD2 for deinterleaved
 *  loads of real/imag pairs, though its L01+V throughput can bottleneck on memory-bound workloads.
 *
 *      Intrinsic               Instruction     M1 Firestorm    Graviton 3      Graviton 4
 *      vfmaq_f32               FMLA.S (vec)    4c @ V0123      4c @ V0123      4c @ V0123
 *      vfmaq_f64               FMLA.D (vec)    4c @ V0123      4c @ V0123      4c @ V0123
 *      vdotq_s32               SDOT (vec)      3c @ V0123      3c @ V0123      3c @ V0123
 *      vdotq_u32               UDOT (vec)      3c @ V0123      3c @ V0123      3c @ V0123
 *      vbfdotq_f32             BFDOT (vec)     N/A             4c @ V0123      5c @ V0123
 *      vld2q_f32               LD2 (Q-form)    5c @ L01+V      8c @ L01+V      8c @ L01+V
 *
 *  @section arm_sve_instructions Relevant ARM SVE Instructions
 *
 *  SVE implementations use predicated FMA (svmla_f32_x) with WHILELT for tail masking, avoiding
 *  scalar cleanup loops. FADDV performs horizontal reduction; notably 45% faster on Graviton 4
 *  (6c) than Graviton 3 (11c). SVE complex dot products use svld2 for structure loads.
 *
 *      Intrinsic               Instruction     Graviton 3      Graviton 4
 *      svmla_f32_x             FMLA (pred)     4c @ V0123      4c @ V0123
 *      svmls_f32_x             FMLS (pred)     4c @ V0123      4c @ V0123
 *      svwhilelt_b32           WHILELT         3c @ M0         3c @ M0
 *      svld2_f32               LD2 (SVE)       8c @ L01+V      8c @ L01+V
 *      svaddv_f32              FADDV           11c @ V0123     6c @ V0123
 *
 *  @section references References
 *
 *  - x86 intrinsics: https://www.intel.com/content/www/us/en/docs/intrinsics-guide/
 *  - Arm intrinsics: https://developer.arm.com/architectures/instruction-sets/intrinsics/
 *
 */
#ifndef SIMSIMD_DOT_H
#define SIMSIMD_DOT_H

#include "types.h"

#if defined(__cplusplus)
extern "C" {
#endif

/**
 *  @brief Dot product computing the sum of elementwise products between two vectors.
 *
 *  @param[in] a The first vector.
 *  @param[in] b The second vector.
 *  @param[in] n The number of elements in the vectors.
 *  @param[out] result The output dot product value.
 *
 *  @note The output value can be negative.
 *  @note The output value is zero if and only if the two vectors are orthogonal.
 *  @note Defined only for floating-point and integer data types.
 */
SIMSIMD_DYNAMIC void simsimd_dot_f32(simsimd_f32_t const *a, simsimd_f32_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *result);
/** @copydoc simsimd_dot_f32 */
SIMSIMD_DYNAMIC void simsimd_dot_f64(simsimd_f64_t const *a, simsimd_f64_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *result);
/** @copydoc simsimd_dot_f32 */
SIMSIMD_DYNAMIC void simsimd_dot_f16(simsimd_f16_t const *a, simsimd_f16_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *result);
/** @copydoc simsimd_dot_f32 */
SIMSIMD_DYNAMIC void simsimd_dot_bf16(simsimd_bf16_t const *a, simsimd_bf16_t const *b, simsimd_size_t n,
                                      simsimd_distance_t *result);
/** @copydoc simsimd_dot_f32 */
SIMSIMD_DYNAMIC void simsimd_dot_i8(simsimd_i8_t const *a, simsimd_i8_t const *b, simsimd_size_t n,
                                    simsimd_distance_t *result);
/** @copydoc simsimd_dot_f32 */
SIMSIMD_DYNAMIC void simsimd_dot_u8(simsimd_u8_t const *a, simsimd_u8_t const *b, simsimd_size_t n,
                                    simsimd_distance_t *result);
/** @copydoc simsimd_dot_f32 */
SIMSIMD_DYNAMIC void simsimd_dot_e4m3(simsimd_e4m3_t const *a, simsimd_e4m3_t const *b, simsimd_size_t n,
                                      simsimd_distance_t *result);
/** @copydoc simsimd_dot_f32 */
SIMSIMD_DYNAMIC void simsimd_dot_e5m2(simsimd_e5m2_t const *a, simsimd_e5m2_t const *b, simsimd_size_t n,
                                      simsimd_distance_t *result);

/**
 *  @brief Complex dot product computing the sum of elementwise products between two complex vectors.
 *
 *  @param[in] a_pairs The first complex vector.
 *  @param[in] b_pairs The second complex vector.
 *  @param[in] count_pairs The number of complex pairs in the vectors.
 *  @param[out] results The output complex value as {real, imag}.
 *
 *  @note The output value can be negative.
 */
SIMSIMD_DYNAMIC void simsimd_dot_f32c(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                      simsimd_size_t count_pairs, simsimd_distance_t *results);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_DYNAMIC void simsimd_dot_f64c(simsimd_f64c_t const *a_pairs, simsimd_f64c_t const *b_pairs,
                                      simsimd_size_t count_pairs, simsimd_distance_t *results);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_DYNAMIC void simsimd_dot_f16c(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                      simsimd_size_t count_pairs, simsimd_distance_t *results);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_DYNAMIC void simsimd_dot_bf16c(simsimd_bf16c_t const *a_pairs, simsimd_bf16c_t const *b_pairs,
                                       simsimd_size_t count_pairs, simsimd_distance_t *results);

/**
 *  @brief Complex conjugate dot product between two complex vectors.
 *
 *  @param[in] a_pairs The first complex vector.
 *  @param[in] b_pairs The second complex vector.
 *  @param[in] count_pairs The number of complex pairs in the vectors.
 *  @param[out] results The output complex value as {real, imag}.
 *
 *  @note The output value can be negative.
 */
SIMSIMD_DYNAMIC void simsimd_vdot_f32c(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                       simsimd_size_t count_pairs, simsimd_distance_t *results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_DYNAMIC void simsimd_vdot_f64c(simsimd_f64c_t const *a_pairs, simsimd_f64c_t const *b_pairs,
                                       simsimd_size_t count_pairs, simsimd_distance_t *results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_DYNAMIC void simsimd_vdot_f16c(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                       simsimd_size_t count_pairs, simsimd_distance_t *results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_DYNAMIC void simsimd_vdot_bf16c(simsimd_bf16c_t const *a_pairs, simsimd_bf16c_t const *b_pairs,
                                        simsimd_size_t count_pairs, simsimd_distance_t *results);

// clang-format off

/** @copydoc simsimd_dot_f64 */
SIMSIMD_PUBLIC void simsimd_dot_f64_serial(simsimd_f64_t const* a, simsimd_f64_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f64c */
SIMSIMD_PUBLIC void simsimd_dot_f64c_serial(simsimd_f64c_t const* a, simsimd_f64c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f64c */
SIMSIMD_PUBLIC void simsimd_vdot_f64c_serial(simsimd_f64c_t const* a, simsimd_f64c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_f32 */
SIMSIMD_PUBLIC void simsimd_dot_f32_serial(simsimd_f32_t const* a, simsimd_f32_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_PUBLIC void simsimd_dot_f32c_serial(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_PUBLIC void simsimd_vdot_f32c_serial(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_f16 */
SIMSIMD_PUBLIC void simsimd_dot_f16_serial(simsimd_f16_t const* a, simsimd_f16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f16c */
SIMSIMD_PUBLIC void simsimd_dot_f16c_serial(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f16c */
SIMSIMD_PUBLIC void simsimd_vdot_f16c_serial(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_bf16 */
SIMSIMD_PUBLIC void simsimd_dot_bf16_serial(simsimd_bf16_t const* a, simsimd_bf16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_bf16c */
SIMSIMD_PUBLIC void simsimd_dot_bf16c_serial(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_bf16c */
SIMSIMD_PUBLIC void simsimd_vdot_bf16c_serial(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_i8 */
SIMSIMD_PUBLIC void simsimd_dot_i8_serial(simsimd_i8_t const* a, simsimd_i8_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_u8 */
SIMSIMD_PUBLIC void simsimd_dot_u8_serial(simsimd_u8_t const* a, simsimd_u8_t const* b, simsimd_size_t n, simsimd_distance_t* result);

/** @copydoc simsimd_dot_e4m3 */
SIMSIMD_PUBLIC void simsimd_dot_e4m3_serial(simsimd_e4m3_t const* a, simsimd_e4m3_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_e5m2 */
SIMSIMD_PUBLIC void simsimd_dot_e5m2_serial(simsimd_e5m2_t const* a, simsimd_e5m2_t const* b, simsimd_size_t n, simsimd_distance_t* result);

/** @copydoc simsimd_dot_f32 */
SIMSIMD_PUBLIC void simsimd_dot_f32_accurate(simsimd_f32_t const* a, simsimd_f32_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_PUBLIC void simsimd_dot_f32c_accurate(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_PUBLIC void simsimd_vdot_f32c_accurate(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_f16 */
SIMSIMD_PUBLIC void simsimd_dot_f16_accurate(simsimd_f16_t const* a, simsimd_f16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f16c */
SIMSIMD_PUBLIC void simsimd_dot_f16c_accurate(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f16c */
SIMSIMD_PUBLIC void simsimd_vdot_f16c_accurate(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_bf16 */
SIMSIMD_PUBLIC void simsimd_dot_bf16_accurate(simsimd_bf16_t const* a, simsimd_bf16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_bf16c */
SIMSIMD_PUBLIC void simsimd_dot_bf16c_accurate(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_bf16c */
SIMSIMD_PUBLIC void simsimd_vdot_bf16c_accurate(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

#if SIMSIMD_TARGET_NEON
/** @copydoc simsimd_dot_f32 */
SIMSIMD_PUBLIC void simsimd_dot_f32_neon(simsimd_f32_t const* a, simsimd_f32_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_PUBLIC void simsimd_dot_f32c_neon(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_PUBLIC void simsimd_vdot_f32c_neon(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results)

typedef struct simsimd_dot_f32x16_state_neon_t simsimd_dot_f32x16_state_neon_t;
/** @copydoc simsimd_dot_f32x16_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_neon(simsimd_dot_f32x16_state_neon_t *state);
/** @copydoc simsimd_dot_f32x16_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_neon(simsimd_dot_f32x16_state_neon_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f32x16_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_neon(simsimd_dot_f32x16_state_neon_t const *s0, simsimd_dot_f32x16_state_neon_t const *s1, simsimd_dot_f32x16_state_neon_t const *s2, simsimd_dot_f32x16_state_neon_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_NEON

#if SIMSIMD_TARGET_NEON_F16
/** @copydoc simsimd_dot_f16 */
SIMSIMD_PUBLIC void simsimd_dot_f16_neon(simsimd_f16_t const* a, simsimd_f16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f16c */
SIMSIMD_PUBLIC void simsimd_dot_f16c_neon(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f16c */
SIMSIMD_PUBLIC void simsimd_vdot_f16c_neon(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results)

typedef struct simsimd_dot_f16x32_state_neon_t simsimd_dot_f16x32_state_neon_t;
/** @copydoc simsimd_dot_f16x32_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_neon(simsimd_dot_f16x32_state_neon_t *state);
/** @copydoc simsimd_dot_f16x32_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_neon(simsimd_dot_f16x32_state_neon_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f16x32_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_neon(simsimd_dot_f16x32_state_neon_t const *s0, simsimd_dot_f16x32_state_neon_t const *s1, simsimd_dot_f16x32_state_neon_t const *s2, simsimd_dot_f16x32_state_neon_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_NEON_F16

#if SIMSIMD_TARGET_NEON_I8
/** @copydoc simsimd_dot_i8 */
SIMSIMD_PUBLIC void simsimd_dot_i8_neon(simsimd_i8_t const* a, simsimd_i8_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_u8 */
SIMSIMD_PUBLIC void simsimd_dot_u8_neon(simsimd_u8_t const* a, simsimd_u8_t const* b, simsimd_size_t n, simsimd_distance_t* result);

typedef struct simsimd_dot_i8x64_state_neon_t simsimd_dot_i8x64_state_neon_t;
/** @copydoc simsimd_dot_i8x64_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_neon(simsimd_dot_i8x64_state_neon_t *state);
/** @copydoc simsimd_dot_i8x64_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_neon(simsimd_dot_i8x64_state_neon_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_i8x64_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_neon(simsimd_dot_i8x64_state_neon_t const *s0, simsimd_dot_i8x64_state_neon_t const *s1, simsimd_dot_i8x64_state_neon_t const *s2, simsimd_dot_i8x64_state_neon_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_u8x64_state_neon_t simsimd_dot_u8x64_state_neon_t;
/** @copydoc simsimd_dot_u8x64_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_neon(simsimd_dot_u8x64_state_neon_t *state);
/** @copydoc simsimd_dot_u8x64_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_neon(simsimd_dot_u8x64_state_neon_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_u8x64_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_neon(simsimd_dot_u8x64_state_neon_t const *s0, simsimd_dot_u8x64_state_neon_t const *s1, simsimd_dot_u8x64_state_neon_t const *s2, simsimd_dot_u8x64_state_neon_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_NEON_I8

#if SIMSIMD_TARGET_NEON_BF16
/** @copydoc simsimd_dot_bf16 */
SIMSIMD_PUBLIC void simsimd_dot_bf16_neon(simsimd_bf16_t const* a, simsimd_bf16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_bf16c */
SIMSIMD_PUBLIC void simsimd_dot_bf16c_neon(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_bf16c */
SIMSIMD_PUBLIC void simsimd_vdot_bf16c_neon(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results)

typedef struct simsimd_dot_bf16x32_state_neon_t simsimd_dot_bf16x32_state_neon_t;
/** @copydoc simsimd_dot_bf16x32_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_neon(simsimd_dot_bf16x32_state_neon_t *state);
/** @copydoc simsimd_dot_bf16x32_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_neon(simsimd_dot_bf16x32_state_neon_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_bf16x32_state_neon_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_neon(simsimd_dot_bf16x32_state_neon_t const *s0, simsimd_dot_bf16x32_state_neon_t const *s1, simsimd_dot_bf16x32_state_neon_t const *s2, simsimd_dot_bf16x32_state_neon_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_NEON_BF16

#if SIMSIMD_TARGET_SVE
/** @copydoc simsimd_dot_f32 */
SIMSIMD_PUBLIC void simsimd_dot_f32_sve(simsimd_f32_t const* a, simsimd_f32_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_PUBLIC void simsimd_dot_f32c_sve(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_PUBLIC void simsimd_vdot_f32c_sve(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_f16 */
SIMSIMD_PUBLIC void simsimd_dot_f16_sve(simsimd_f16_t const* a, simsimd_f16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f16c */
SIMSIMD_PUBLIC void simsimd_dot_f16c_sve(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f16c */
SIMSIMD_PUBLIC void simsimd_vdot_f16c_sve(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_f64 */
SIMSIMD_PUBLIC void simsimd_dot_f64_sve(simsimd_f64_t const* a, simsimd_f64_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f64c */
SIMSIMD_PUBLIC void simsimd_dot_f64c_sve(simsimd_f64c_t const* a, simsimd_f64c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f64c */
SIMSIMD_PUBLIC void simsimd_vdot_f64c_sve(simsimd_f64c_t const* a, simsimd_f64c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
#endif // SIMSIMD_TARGET_SVE

#if SIMSIMD_TARGET_HASWELL
/** @copydoc simsimd_dot_f32 */
SIMSIMD_PUBLIC void simsimd_dot_f32_haswell(simsimd_f32_t const* a, simsimd_f32_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_PUBLIC void simsimd_dot_f32c_haswell(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_PUBLIC void simsimd_vdot_f32c_haswell(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_f16 */
SIMSIMD_PUBLIC void simsimd_dot_f16_haswell(simsimd_f16_t const* a, simsimd_f16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f16c */
SIMSIMD_PUBLIC void simsimd_dot_f16c_haswell(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f16c */
SIMSIMD_PUBLIC void simsimd_vdot_f16c_haswell(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_bf16 */
SIMSIMD_PUBLIC void simsimd_dot_bf16_haswell(simsimd_bf16_t const* a, simsimd_bf16_t const* b, simsimd_size_t n, simsimd_distance_t* result);

/** @copydoc simsimd_dot_e4m3 */
SIMSIMD_PUBLIC void simsimd_dot_e4m3_haswell(simsimd_e4m3_t const* a, simsimd_e4m3_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_e5m2 */
SIMSIMD_PUBLIC void simsimd_dot_e5m2_haswell(simsimd_e5m2_t const* a, simsimd_e5m2_t const* b, simsimd_size_t n, simsimd_distance_t* result);

/** @copydoc simsimd_dot_i8 */
SIMSIMD_PUBLIC void simsimd_dot_i8_haswell(simsimd_i8_t const* a, simsimd_i8_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_u8 */
SIMSIMD_PUBLIC void simsimd_dot_u8_haswell(simsimd_u8_t const* a, simsimd_u8_t const* b, simsimd_size_t n, simsimd_distance_t* result);

typedef struct simsimd_dot_f32x16_state_haswell_t simsimd_dot_f32x16_state_haswell_t;
/** @copydoc simsimd_dot_f32x16_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_haswell(simsimd_dot_f32x16_state_haswell_t *state);
/** @copydoc simsimd_dot_f32x16_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_haswell(simsimd_dot_f32x16_state_haswell_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f32x16_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_haswell(simsimd_dot_f32x16_state_haswell_t const *s0, simsimd_dot_f32x16_state_haswell_t const *s1, simsimd_dot_f32x16_state_haswell_t const *s2, simsimd_dot_f32x16_state_haswell_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_f16x32_state_haswell_t simsimd_dot_f16x32_state_haswell_t;
/** @copydoc simsimd_dot_f16x32_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_haswell(simsimd_dot_f16x32_state_haswell_t *state);
/** @copydoc simsimd_dot_f16x32_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_haswell(simsimd_dot_f16x32_state_haswell_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f16x32_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_haswell(simsimd_dot_f16x32_state_haswell_t const *s0, simsimd_dot_f16x32_state_haswell_t const *s1, simsimd_dot_f16x32_state_haswell_t const *s2, simsimd_dot_f16x32_state_haswell_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_bf16x32_state_haswell_t simsimd_dot_bf16x32_state_haswell_t;
/** @copydoc simsimd_dot_bf16x32_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_haswell(simsimd_dot_bf16x32_state_haswell_t *state);
/** @copydoc simsimd_dot_bf16x32_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_haswell(simsimd_dot_bf16x32_state_haswell_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_bf16x32_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_haswell(simsimd_dot_bf16x32_state_haswell_t const *s0, simsimd_dot_bf16x32_state_haswell_t const *s1, simsimd_dot_bf16x32_state_haswell_t const *s2, simsimd_dot_bf16x32_state_haswell_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e4m3x64_state_haswell_t simsimd_dot_e4m3x64_state_haswell_t;
/** @copydoc simsimd_dot_e4m3x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_haswell(simsimd_dot_e4m3x64_state_haswell_t *state);
/** @copydoc simsimd_dot_e4m3x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_haswell(simsimd_dot_e4m3x64_state_haswell_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e4m3x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_haswell(simsimd_dot_e4m3x64_state_haswell_t const *s0, simsimd_dot_e4m3x64_state_haswell_t const *s1, simsimd_dot_e4m3x64_state_haswell_t const *s2, simsimd_dot_e4m3x64_state_haswell_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e5m2x64_state_haswell_t simsimd_dot_e5m2x64_state_haswell_t;
/** @copydoc simsimd_dot_e5m2x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_haswell(simsimd_dot_e5m2x64_state_haswell_t *state);
/** @copydoc simsimd_dot_e5m2x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_haswell(simsimd_dot_e5m2x64_state_haswell_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e5m2x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_haswell(simsimd_dot_e5m2x64_state_haswell_t const *s0, simsimd_dot_e5m2x64_state_haswell_t const *s1, simsimd_dot_e5m2x64_state_haswell_t const *s2, simsimd_dot_e5m2x64_state_haswell_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_i8x64_state_haswell_t simsimd_dot_i8x64_state_haswell_t;
/** @copydoc simsimd_dot_i8x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_haswell(simsimd_dot_i8x64_state_haswell_t *state);
/** @copydoc simsimd_dot_i8x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_haswell(simsimd_dot_i8x64_state_haswell_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_i8x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_haswell(simsimd_dot_i8x64_state_haswell_t const *s0, simsimd_dot_i8x64_state_haswell_t const *s1, simsimd_dot_i8x64_state_haswell_t const *s2, simsimd_dot_i8x64_state_haswell_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_u8x64_state_haswell_t simsimd_dot_u8x64_state_haswell_t;
/** @copydoc simsimd_dot_u8x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_haswell(simsimd_dot_u8x64_state_haswell_t *state);
/** @copydoc simsimd_dot_u8x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_haswell(simsimd_dot_u8x64_state_haswell_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_u8x64_state_haswell_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_haswell(simsimd_dot_u8x64_state_haswell_t const *s0, simsimd_dot_u8x64_state_haswell_t const *s1, simsimd_dot_u8x64_state_haswell_t const *s2, simsimd_dot_u8x64_state_haswell_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_HASWELL

#if SIMSIMD_TARGET_SKYLAKE
/** @copydoc simsimd_dot_f64 */
SIMSIMD_PUBLIC void simsimd_dot_f64_skylake(simsimd_f64_t const* a, simsimd_f64_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f64c */
SIMSIMD_PUBLIC void simsimd_dot_f64c_skylake(simsimd_f64c_t const* a, simsimd_f64c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f64c */
SIMSIMD_PUBLIC void simsimd_vdot_f64c_skylake(simsimd_f64c_t const* a, simsimd_f64c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_f32 */
SIMSIMD_PUBLIC void simsimd_dot_f32_skylake(simsimd_f32_t const* a, simsimd_f32_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f32c */
SIMSIMD_PUBLIC void simsimd_dot_f32c_skylake(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f32c */
SIMSIMD_PUBLIC void simsimd_vdot_f32c_skylake(simsimd_f32c_t const* a, simsimd_f32c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_e4m3 */
SIMSIMD_PUBLIC void simsimd_dot_e4m3_skylake(simsimd_e4m3_t const* a, simsimd_e4m3_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_e5m2 */
SIMSIMD_PUBLIC void simsimd_dot_e5m2_skylake(simsimd_e5m2_t const* a, simsimd_e5m2_t const* b, simsimd_size_t n, simsimd_distance_t* result);

typedef struct simsimd_dot_f64x8_state_skylake_t simsimd_dot_f64x8_state_skylake_t;
/** @copydoc simsimd_dot_f64x8_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_f64x8_init_skylake(simsimd_dot_f64x8_state_skylake_t *state);
/** @copydoc simsimd_dot_f64x8_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_f64x8_update_skylake(simsimd_dot_f64x8_state_skylake_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f64x8_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_f64x8_finalize_skylake(simsimd_dot_f64x8_state_skylake_t const *s0, simsimd_dot_f64x8_state_skylake_t const *s1, simsimd_dot_f64x8_state_skylake_t const *s2, simsimd_dot_f64x8_state_skylake_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_f32x16_state_skylake_t simsimd_dot_f32x16_state_skylake_t;
/** @copydoc simsimd_dot_f32x16_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_skylake(simsimd_dot_f32x16_state_skylake_t *state);
/** @copydoc simsimd_dot_f32x16_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_skylake(simsimd_dot_f32x16_state_skylake_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f32x16_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_skylake(simsimd_dot_f32x16_state_skylake_t const *s0, simsimd_dot_f32x16_state_skylake_t const *s1, simsimd_dot_f32x16_state_skylake_t const *s2, simsimd_dot_f32x16_state_skylake_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e4m3x64_state_skylake_t simsimd_dot_e4m3x64_state_skylake_t;
/** @copydoc simsimd_dot_e4m3x64_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_skylake(simsimd_dot_e4m3x64_state_skylake_t *state);
/** @copydoc simsimd_dot_e4m3x64_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_skylake(simsimd_dot_e4m3x64_state_skylake_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e4m3x64_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_skylake(simsimd_dot_e4m3x64_state_skylake_t const *s0, simsimd_dot_e4m3x64_state_skylake_t const *s1, simsimd_dot_e4m3x64_state_skylake_t const *s2, simsimd_dot_e4m3x64_state_skylake_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e5m2x64_state_skylake_t simsimd_dot_e5m2x64_state_skylake_t;
/** @copydoc simsimd_dot_e5m2x64_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_skylake(simsimd_dot_e5m2x64_state_skylake_t *state);
/** @copydoc simsimd_dot_e5m2x64_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_skylake(simsimd_dot_e5m2x64_state_skylake_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e5m2x64_state_skylake_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_skylake(simsimd_dot_e5m2x64_state_skylake_t const *s0, simsimd_dot_e5m2x64_state_skylake_t const *s1, simsimd_dot_e5m2x64_state_skylake_t const *s2, simsimd_dot_e5m2x64_state_skylake_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_SKYLAKE

#if SIMSIMD_TARGET_ICE
/** @copydoc simsimd_dot_i8 */
SIMSIMD_PUBLIC void simsimd_dot_i8_ice(simsimd_i8_t const* a, simsimd_i8_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_u8 */
SIMSIMD_PUBLIC void simsimd_dot_u8_ice(simsimd_u8_t const* a, simsimd_u8_t const* b, simsimd_size_t n, simsimd_distance_t* result);

typedef struct simsimd_dot_i8x64_state_ice_t simsimd_dot_i8x64_state_ice_t;
/** @copydoc simsimd_dot_i8x64_state_ice_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_ice(simsimd_dot_i8x64_state_ice_t *state);
/** @copydoc simsimd_dot_i8x64_state_ice_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_ice(simsimd_dot_i8x64_state_ice_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_i8x64_state_ice_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_ice(simsimd_dot_i8x64_state_ice_t const *s0, simsimd_dot_i8x64_state_ice_t const *s1, simsimd_dot_i8x64_state_ice_t const *s2, simsimd_dot_i8x64_state_ice_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_u8x64_state_ice_t simsimd_dot_u8x64_state_ice_t;
/** @copydoc simsimd_dot_u8x64_state_ice_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_ice(simsimd_dot_u8x64_state_ice_t *state);
/** @copydoc simsimd_dot_u8x64_state_ice_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_ice(simsimd_dot_u8x64_state_ice_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_u8x64_state_ice_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_ice(simsimd_dot_u8x64_state_ice_t const *s0, simsimd_dot_u8x64_state_ice_t const *s1, simsimd_dot_u8x64_state_ice_t const *s2, simsimd_dot_u8x64_state_ice_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_ICE

#if SIMSIMD_TARGET_GENOA
/** @copydoc simsimd_dot_bf16 */
SIMSIMD_PUBLIC void simsimd_dot_bf16_genoa(simsimd_bf16_t const* a, simsimd_bf16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_bf16c */
SIMSIMD_PUBLIC void simsimd_dot_bf16c_genoa(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_bf16c */
SIMSIMD_PUBLIC void simsimd_vdot_bf16c_genoa(simsimd_bf16c_t const* a, simsimd_bf16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_e4m3 */
SIMSIMD_PUBLIC void simsimd_dot_e4m3_genoa(simsimd_e4m3_t const* a, simsimd_e4m3_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_e5m2 */
SIMSIMD_PUBLIC void simsimd_dot_e5m2_genoa(simsimd_e5m2_t const* a, simsimd_e5m2_t const* b, simsimd_size_t n, simsimd_distance_t* result);

typedef struct simsimd_dot_bf16x32_state_genoa_t simsimd_dot_bf16x32_state_genoa_t;
/** @copydoc simsimd_dot_bf16x32_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_genoa(simsimd_dot_bf16x32_state_genoa_t *state);
/** @copydoc simsimd_dot_bf16x32_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_genoa(simsimd_dot_bf16x32_state_genoa_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_bf16x32_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_genoa(simsimd_dot_bf16x32_state_genoa_t const *s0, simsimd_dot_bf16x32_state_genoa_t const *s1, simsimd_dot_bf16x32_state_genoa_t const *s2, simsimd_dot_bf16x32_state_genoa_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e4m3x64_state_genoa_t simsimd_dot_e4m3x64_state_genoa_t;
/** @copydoc simsimd_dot_e4m3x64_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_genoa(simsimd_dot_e4m3x64_state_genoa_t *state);
/** @copydoc simsimd_dot_e4m3x64_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_genoa(simsimd_dot_e4m3x64_state_genoa_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e4m3x64_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_genoa(simsimd_dot_e4m3x64_state_genoa_t const *s0, simsimd_dot_e4m3x64_state_genoa_t const *s1, simsimd_dot_e4m3x64_state_genoa_t const *s2, simsimd_dot_e4m3x64_state_genoa_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e5m2x64_state_genoa_t simsimd_dot_e5m2x64_state_genoa_t;
/** @copydoc simsimd_dot_e5m2x64_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_genoa(simsimd_dot_e5m2x64_state_genoa_t *state);
/** @copydoc simsimd_dot_e5m2x64_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_genoa(simsimd_dot_e5m2x64_state_genoa_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e5m2x64_state_genoa_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_genoa(simsimd_dot_e5m2x64_state_genoa_t const *s0, simsimd_dot_e5m2x64_state_genoa_t const *s1, simsimd_dot_e5m2x64_state_genoa_t const *s2, simsimd_dot_e5m2x64_state_genoa_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_GENOA

#if SIMSIMD_TARGET_SAPPHIRE
/** @copydoc simsimd_dot_f16 */
SIMSIMD_PUBLIC void simsimd_dot_f16_sapphire(simsimd_f16_t const* a, simsimd_f16_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_f16c */
SIMSIMD_PUBLIC void simsimd_dot_f16c_sapphire(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);
/** @copydoc simsimd_vdot_f16c */
SIMSIMD_PUBLIC void simsimd_vdot_f16c_sapphire(simsimd_f16c_t const* a, simsimd_f16c_t const* b, simsimd_size_t n, simsimd_distance_t* results);

/** @copydoc simsimd_dot_e4m3 */
SIMSIMD_PUBLIC void simsimd_dot_e4m3_sapphire(simsimd_e4m3_t const* a, simsimd_e4m3_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_e5m2 */
SIMSIMD_PUBLIC void simsimd_dot_e5m2_sapphire(simsimd_e5m2_t const* a, simsimd_e5m2_t const* b, simsimd_size_t n, simsimd_distance_t* result);
/** @copydoc simsimd_dot_e5m2 */
SIMSIMD_PUBLIC void simsimd_dot_e5m2_sapphire_lut(simsimd_e5m2_t const* a, simsimd_e5m2_t const* b, simsimd_size_t n, simsimd_distance_t* result);

typedef struct simsimd_dot_f16x32_state_sapphire_t simsimd_dot_f16x32_state_sapphire_t;
/** @copydoc simsimd_dot_f16x32_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_sapphire(simsimd_dot_f16x32_state_sapphire_t *state);
/** @copydoc simsimd_dot_f16x32_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_sapphire(simsimd_dot_f16x32_state_sapphire_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f16x32_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_sapphire(simsimd_dot_f16x32_state_sapphire_t const *s0, simsimd_dot_f16x32_state_sapphire_t const *s1, simsimd_dot_f16x32_state_sapphire_t const *s2, simsimd_dot_f16x32_state_sapphire_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e4m3x64_state_sapphire_t simsimd_dot_e4m3x64_state_sapphire_t;
/** @copydoc simsimd_dot_e4m3x64_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_sapphire(simsimd_dot_e4m3x64_state_sapphire_t *state);
/** @copydoc simsimd_dot_e4m3x64_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_sapphire(simsimd_dot_e4m3x64_state_sapphire_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e4m3x64_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_sapphire(simsimd_dot_e4m3x64_state_sapphire_t const *s0, simsimd_dot_e4m3x64_state_sapphire_t const *s1, simsimd_dot_e4m3x64_state_sapphire_t const *s2, simsimd_dot_e4m3x64_state_sapphire_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e5m2x64_state_sapphire_t simsimd_dot_e5m2x64_state_sapphire_t;
/** @copydoc simsimd_dot_e5m2x64_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_sapphire(simsimd_dot_e5m2x64_state_sapphire_t *state);
/** @copydoc simsimd_dot_e5m2x64_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_sapphire(simsimd_dot_e5m2x64_state_sapphire_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e5m2x64_state_sapphire_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_sapphire(simsimd_dot_e5m2x64_state_sapphire_t const *s0, simsimd_dot_e5m2x64_state_sapphire_t const *s1, simsimd_dot_e5m2x64_state_sapphire_t const *s2, simsimd_dot_e5m2x64_state_sapphire_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_SAPPHIRE

#if SIMSIMD_TARGET_SIERRA
/** @copydoc simsimd_dot_i8 */
SIMSIMD_PUBLIC void simsimd_dot_i8_sierra(simsimd_i8_t const* a, simsimd_i8_t const* b, simsimd_size_t n, simsimd_distance_t* result);

typedef struct simsimd_dot_i8x64_state_sierra_t simsimd_dot_i8x64_state_sierra_t;
/** @copydoc simsimd_dot_i8x64_state_sierra_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_sierra(simsimd_dot_i8x64_state_sierra_t *state);
/** @copydoc simsimd_dot_i8x64_state_sierra_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_sierra(simsimd_dot_i8x64_state_sierra_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_i8x64_state_sierra_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_sierra(simsimd_dot_i8x64_state_sierra_t const *s0, simsimd_dot_i8x64_state_sierra_t const *s1, simsimd_dot_i8x64_state_sierra_t const *s2, simsimd_dot_i8x64_state_sierra_t const *s3, simsimd_f32_t *results);
#endif // SIMSIMD_TARGET_SIERRA

typedef struct simsimd_dot_f64x8_state_serial_t simsimd_dot_f64x8_state_serial_t;
/** @copydoc simsimd_dot_f64x8_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f64x8_init_serial(simsimd_dot_f64x8_state_serial_t *state);
/** @copydoc simsimd_dot_f64x8_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f64x8_update_serial(simsimd_dot_f64x8_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f64x8_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f64x8_finalize_serial(simsimd_dot_f64x8_state_serial_t const *s0, simsimd_dot_f64x8_state_serial_t const *s1, simsimd_dot_f64x8_state_serial_t const *s2, simsimd_dot_f64x8_state_serial_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_f32x16_state_serial_t simsimd_dot_f32x16_state_serial_t;
/** @copydoc simsimd_dot_f32x16_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_serial(simsimd_dot_f32x16_state_serial_t *state);
/** @copydoc simsimd_dot_f32x16_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_serial(simsimd_dot_f32x16_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f32x16_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_serial(simsimd_dot_f32x16_state_serial_t const *s0, simsimd_dot_f32x16_state_serial_t const *s1, simsimd_dot_f32x16_state_serial_t const *s2, simsimd_dot_f32x16_state_serial_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_f16x32_state_serial_t simsimd_dot_f16x32_state_serial_t;
/** @copydoc simsimd_dot_f16x32_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_serial(simsimd_dot_f16x32_state_serial_t *state);
/** @copydoc simsimd_dot_f16x32_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_serial(simsimd_dot_f16x32_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_f16x32_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_serial(simsimd_dot_f16x32_state_serial_t const *s0, simsimd_dot_f16x32_state_serial_t const *s1, simsimd_dot_f16x32_state_serial_t const *s2, simsimd_dot_f16x32_state_serial_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_bf16x32_state_serial_t simsimd_dot_bf16x32_state_serial_t;
/** @copydoc simsimd_dot_bf16x32_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_serial(simsimd_dot_bf16x32_state_serial_t *state);
/** @copydoc simsimd_dot_bf16x32_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_serial(simsimd_dot_bf16x32_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_bf16x32_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_serial(simsimd_dot_bf16x32_state_serial_t const *s0, simsimd_dot_bf16x32_state_serial_t const *s1, simsimd_dot_bf16x32_state_serial_t const *s2, simsimd_dot_bf16x32_state_serial_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_i8x64_state_serial_t simsimd_dot_i8x64_state_serial_t;
/** @copydoc simsimd_dot_i8x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_serial(simsimd_dot_i8x64_state_serial_t *state);
/** @copydoc simsimd_dot_i8x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_serial(simsimd_dot_i8x64_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_i8x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_serial(simsimd_dot_i8x64_state_serial_t const *s0, simsimd_dot_i8x64_state_serial_t const *s1, simsimd_dot_i8x64_state_serial_t const *s2, simsimd_dot_i8x64_state_serial_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_u8x64_state_serial_t simsimd_dot_u8x64_state_serial_t;
/** @copydoc simsimd_dot_u8x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_serial(simsimd_dot_u8x64_state_serial_t *state);
/** @copydoc simsimd_dot_u8x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_serial(simsimd_dot_u8x64_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_u8x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_serial(simsimd_dot_u8x64_state_serial_t const *s0, simsimd_dot_u8x64_state_serial_t const *s1, simsimd_dot_u8x64_state_serial_t const *s2, simsimd_dot_u8x64_state_serial_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e4m3x64_state_serial_t simsimd_dot_e4m3x64_state_serial_t;
/** @copydoc simsimd_dot_e4m3x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_serial(simsimd_dot_e4m3x64_state_serial_t *state);
/** @copydoc simsimd_dot_e4m3x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_serial(simsimd_dot_e4m3x64_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e4m3x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_serial(simsimd_dot_e4m3x64_state_serial_t const *s0, simsimd_dot_e4m3x64_state_serial_t const *s1, simsimd_dot_e4m3x64_state_serial_t const *s2, simsimd_dot_e4m3x64_state_serial_t const *s3, simsimd_f32_t *results);

typedef struct simsimd_dot_e5m2x64_state_serial_t simsimd_dot_e5m2x64_state_serial_t;
/** @copydoc simsimd_dot_e5m2x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_serial(simsimd_dot_e5m2x64_state_serial_t *state);
/** @copydoc simsimd_dot_e5m2x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_serial(simsimd_dot_e5m2x64_state_serial_t *state, simsimd_b512_vec_t a, simsimd_b512_vec_t b);
/** @copydoc simsimd_dot_e5m2x64_state_serial_t */
SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_serial(simsimd_dot_e5m2x64_state_serial_t const *s0, simsimd_dot_e5m2x64_state_serial_t const *s1, simsimd_dot_e5m2x64_state_serial_t const *s2, simsimd_dot_e5m2x64_state_serial_t const *s3, simsimd_f32_t *results);
// clang-format on

#define SIMSIMD_MAKE_DOT(name, input_type, accumulator_type, load_and_convert)                                 \
    SIMSIMD_PUBLIC void simsimd_dot_##input_type##_##name(simsimd_##input_type##_t const *a,                   \
                                                          simsimd_##input_type##_t const *b, simsimd_size_t n, \
                                                          simsimd_distance_t *result) {                        \
        simsimd_##accumulator_type##_t ab = 0, ai, bi;                                                         \
        for (simsimd_size_t i = 0; i != n; ++i) {                                                              \
            load_and_convert(a + i, &ai);                                                                      \
            load_and_convert(b + i, &bi);                                                                      \
            ab += ai * bi;                                                                                     \
        }                                                                                                      \
        *result = ab;                                                                                          \
    }

#define SIMSIMD_MAKE_COMPLEX_DOT(name, input_type, accumulator_type, load_and_convert)                               \
    SIMSIMD_PUBLIC void simsimd_dot_##input_type##_##name(simsimd_##input_type##_t const *a_pairs,                   \
                                                          simsimd_##input_type##_t const *b_pairs,                   \
                                                          simsimd_size_t count_pairs, simsimd_distance_t *results) { \
        simsimd_##accumulator_type##_t ab_real = 0, ab_imag = 0, ar, br, ai, bi;                                     \
        for (simsimd_size_t i = 0; i != count_pairs; ++i) {                                                          \
            load_and_convert(&(a_pairs + i)->real, &ar);                                                             \
            load_and_convert(&(b_pairs + i)->real, &br);                                                             \
            load_and_convert(&(a_pairs + i)->imag, &ai);                                                             \
            load_and_convert(&(b_pairs + i)->imag, &bi);                                                             \
            ab_real += ar * br - ai * bi;                                                                            \
            ab_imag += ar * bi + ai * br;                                                                            \
        }                                                                                                            \
        results[0] = ab_real;                                                                                        \
        results[1] = ab_imag;                                                                                        \
    }

#define SIMSIMD_MAKE_COMPLEX_VDOT(name, input_type, accumulator_type, load_and_convert)                               \
    SIMSIMD_PUBLIC void simsimd_vdot_##input_type##_##name(simsimd_##input_type##_t const *a_pairs,                   \
                                                           simsimd_##input_type##_t const *b_pairs,                   \
                                                           simsimd_size_t count_pairs, simsimd_distance_t *results) { \
        simsimd_##accumulator_type##_t ab_real = 0, ab_imag = 0, ar, br, ai, bi;                                      \
        for (simsimd_size_t i = 0; i != count_pairs; ++i) {                                                           \
            load_and_convert(&(a_pairs + i)->real, &ar);                                                              \
            load_and_convert(&(b_pairs + i)->real, &br);                                                              \
            load_and_convert(&(a_pairs + i)->imag, &ai);                                                              \
            load_and_convert(&(b_pairs + i)->imag, &bi);                                                              \
            ab_real += ar * br + ai * bi;                                                                             \
            ab_imag += ar * bi - ai * br;                                                                             \
        }                                                                                                             \
        results[0] = ab_real;                                                                                         \
        results[1] = ab_imag;                                                                                         \
    }

SIMSIMD_MAKE_DOT(serial, f64, f64, SIMSIMD_ASSIGN_FROM_TO)           // simsimd_dot_f64_serial
SIMSIMD_MAKE_COMPLEX_DOT(serial, f64c, f64, SIMSIMD_ASSIGN_FROM_TO)  // simsimd_dot_f64c_serial
SIMSIMD_MAKE_COMPLEX_VDOT(serial, f64c, f64, SIMSIMD_ASSIGN_FROM_TO) // simsimd_vdot_f64c_serial

SIMSIMD_MAKE_DOT(serial, f32, f32, SIMSIMD_ASSIGN_FROM_TO)           // simsimd_dot_f32_serial
SIMSIMD_MAKE_COMPLEX_DOT(serial, f32c, f32, SIMSIMD_ASSIGN_FROM_TO)  // simsimd_dot_f32c_serial
SIMSIMD_MAKE_COMPLEX_VDOT(serial, f32c, f32, SIMSIMD_ASSIGN_FROM_TO) // simsimd_vdot_f32c_serial

SIMSIMD_MAKE_DOT(serial, f16, f32, simsimd_f16_to_f32)           // simsimd_dot_f16_serial
SIMSIMD_MAKE_COMPLEX_DOT(serial, f16c, f32, simsimd_f16_to_f32)  // simsimd_dot_f16c_serial
SIMSIMD_MAKE_COMPLEX_VDOT(serial, f16c, f32, simsimd_f16_to_f32) // simsimd_vdot_f16c_serial

SIMSIMD_MAKE_DOT(serial, bf16, f32, simsimd_bf16_to_f32)           // simsimd_dot_bf16_serial
SIMSIMD_MAKE_COMPLEX_DOT(serial, bf16c, f32, simsimd_bf16_to_f32)  // simsimd_dot_bf16c_serial
SIMSIMD_MAKE_COMPLEX_VDOT(serial, bf16c, f32, simsimd_bf16_to_f32) // simsimd_vdot_bf16c_serial

SIMSIMD_MAKE_DOT(serial, i8, i64, SIMSIMD_ASSIGN_FROM_TO) // simsimd_dot_i8_serial
SIMSIMD_MAKE_DOT(serial, u8, i64, SIMSIMD_ASSIGN_FROM_TO) // simsimd_dot_u8_serial

SIMSIMD_PUBLIC void simsimd_dot_e4m3_serial(simsimd_e4m3_t const *a, simsimd_e4m3_t const *b, simsimd_size_t n,
                                            simsimd_distance_t *result) {
    simsimd_f32_t ab = 0, ai, bi;
    for (simsimd_size_t i = 0; i != n; ++i) {
        simsimd_e4m3_to_f32(a + i, &ai);
        simsimd_e4m3_to_f32(b + i, &bi);
        ab += ai * bi;
    }
    *result = ab;
}

SIMSIMD_PUBLIC void simsimd_dot_e5m2_serial(simsimd_e5m2_t const *a, simsimd_e5m2_t const *b, simsimd_size_t n,
                                            simsimd_distance_t *result) {
    simsimd_f32_t ab = 0, ai, bi;
    for (simsimd_size_t i = 0; i != n; ++i) {
        simsimd_e5m2_to_f32(a + i, &ai);
        simsimd_e5m2_to_f32(b + i, &bi);
        ab += ai * bi;
    }
    *result = ab;
}

SIMSIMD_MAKE_DOT(accurate, f32, f64, SIMSIMD_ASSIGN_FROM_TO)           // simsimd_dot_f32_accurate
SIMSIMD_MAKE_COMPLEX_DOT(accurate, f32c, f64, SIMSIMD_ASSIGN_FROM_TO)  // simsimd_dot_f32c_accurate
SIMSIMD_MAKE_COMPLEX_VDOT(accurate, f32c, f64, SIMSIMD_ASSIGN_FROM_TO) // simsimd_vdot_f32c_accurate

SIMSIMD_MAKE_DOT(accurate, f16, f64, simsimd_f16_to_f64)           // simsimd_dot_f16_accurate
SIMSIMD_MAKE_COMPLEX_DOT(accurate, f16c, f64, simsimd_f16_to_f64)  // simsimd_dot_f16c_accurate
SIMSIMD_MAKE_COMPLEX_VDOT(accurate, f16c, f64, simsimd_f16_to_f64) // simsimd_vdot_f16c_accurate

SIMSIMD_MAKE_DOT(accurate, bf16, f64, simsimd_bf16_to_f64)           // simsimd_dot_bf16_accurate
SIMSIMD_MAKE_COMPLEX_DOT(accurate, bf16c, f64, simsimd_bf16_to_f64)  // simsimd_dot_bf16c_accurate
SIMSIMD_MAKE_COMPLEX_VDOT(accurate, bf16c, f64, simsimd_bf16_to_f64) // simsimd_vdot_bf16c_accurate

/**
 *  @brief Running state for 512-bit dot accumulation over f64 scalars.
 */
typedef struct simsimd_dot_f64x8_state_serial_t {
    simsimd_f64_t sums[2];
} simsimd_dot_f64x8_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_f64x8_init_serial(simsimd_dot_f64x8_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_f64x8_update_serial(simsimd_dot_f64x8_state_serial_t *state, simsimd_b512_vec_t a,
                                                      simsimd_b512_vec_t b) {
    simsimd_f64_t sum0 = state->sums[0];
    simsimd_f64_t sum1 = state->sums[1];

    sum0 += a.f64s[0] * b.f64s[0], sum1 += a.f64s[1] * b.f64s[1];
    sum0 += a.f64s[2] * b.f64s[2], sum1 += a.f64s[3] * b.f64s[3];
    sum0 += a.f64s[4] * b.f64s[4], sum1 += a.f64s[5] * b.f64s[5];
    sum0 += a.f64s[6] * b.f64s[6], sum1 += a.f64s[7] * b.f64s[7];

    state->sums[0] = sum0, state->sums[1] = sum1;
}

SIMSIMD_INTERNAL void simsimd_dot_f64x8_finalize_serial(                                    //
    simsimd_dot_f64x8_state_serial_t const *s0, simsimd_dot_f64x8_state_serial_t const *s1, //
    simsimd_dot_f64x8_state_serial_t const *s2, simsimd_dot_f64x8_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = (simsimd_f32_t)(s0->sums[0] + s0->sums[1]);
    results[1] = (simsimd_f32_t)(s1->sums[0] + s1->sums[1]);
    results[2] = (simsimd_f32_t)(s2->sums[0] + s2->sums[1]);
    results[3] = (simsimd_f32_t)(s3->sums[0] + s3->sums[1]);
}

/**
 *  @brief Running state for 512-bit dot accumulation over f32 scalars.
 */
typedef struct simsimd_dot_f32x16_state_serial_t {
    simsimd_f32_t sums[4];
} simsimd_dot_f32x16_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_serial(simsimd_dot_f32x16_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
    state->sums[2] = 0;
    state->sums[3] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_serial(simsimd_dot_f32x16_state_serial_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    simsimd_f32_t sum0 = state->sums[0];
    simsimd_f32_t sum1 = state->sums[1];
    simsimd_f32_t sum2 = state->sums[2];
    simsimd_f32_t sum3 = state->sums[3];

    sum0 += a.f32s[0] * b.f32s[0], sum1 += a.f32s[1] * b.f32s[1], sum2 += a.f32s[2] * b.f32s[2],
        sum3 += a.f32s[3] * b.f32s[3];
    sum0 += a.f32s[4] * b.f32s[4], sum1 += a.f32s[5] * b.f32s[5], sum2 += a.f32s[6] * b.f32s[6],
        sum3 += a.f32s[7] * b.f32s[7];
    sum0 += a.f32s[8] * b.f32s[8], sum1 += a.f32s[9] * b.f32s[9], sum2 += a.f32s[10] * b.f32s[10],
        sum3 += a.f32s[11] * b.f32s[11];
    sum0 += a.f32s[12] * b.f32s[12], sum1 += a.f32s[13] * b.f32s[13], sum2 += a.f32s[14] * b.f32s[14],
        sum3 += a.f32s[15] * b.f32s[15];

    state->sums[0] = sum0, state->sums[1] = sum1, state->sums[2] = sum2, state->sums[3] = sum3;
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_serial(                                     //
    simsimd_dot_f32x16_state_serial_t const *s0, simsimd_dot_f32x16_state_serial_t const *s1, //
    simsimd_dot_f32x16_state_serial_t const *s2, simsimd_dot_f32x16_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = s0->sums[0] + s0->sums[1] + s0->sums[2] + s0->sums[3];
    results[1] = s1->sums[0] + s1->sums[1] + s1->sums[2] + s1->sums[3];
    results[2] = s2->sums[0] + s2->sums[1] + s2->sums[2] + s2->sums[3];
    results[3] = s3->sums[0] + s3->sums[1] + s3->sums[2] + s3->sums[3];
}

/**
 *  @brief Running state for 512-bit dot accumulation over f16 scalars.
 */
typedef struct simsimd_dot_f16x32_state_serial_t {
    simsimd_f32_t sums[4];
} simsimd_dot_f16x32_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_serial(simsimd_dot_f16x32_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
    state->sums[2] = 0;
    state->sums[3] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_serial(simsimd_dot_f16x32_state_serial_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    simsimd_f32_t sum0 = state->sums[0], sum1 = state->sums[1], sum2 = state->sums[2], sum3 = state->sums[3];
    for (simsimd_size_t i = 0; i < 32; i += 4) {
        simsimd_f32_t a0, a1, a2, a3, b0, b1, b2, b3;
        simsimd_f16_to_f32(a.f16s + i + 0, &a0);
        simsimd_f16_to_f32(a.f16s + i + 1, &a1);
        simsimd_f16_to_f32(a.f16s + i + 2, &a2);
        simsimd_f16_to_f32(a.f16s + i + 3, &a3);
        simsimd_f16_to_f32(b.f16s + i + 0, &b0);
        simsimd_f16_to_f32(b.f16s + i + 1, &b1);
        simsimd_f16_to_f32(b.f16s + i + 2, &b2);
        simsimd_f16_to_f32(b.f16s + i + 3, &b3);
        sum0 += a0 * b0;
        sum1 += a1 * b1;
        sum2 += a2 * b2;
        sum3 += a3 * b3;
    }
    state->sums[0] = sum0, state->sums[1] = sum1, state->sums[2] = sum2, state->sums[3] = sum3;
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_serial(                                     //
    simsimd_dot_f16x32_state_serial_t const *s0, simsimd_dot_f16x32_state_serial_t const *s1, //
    simsimd_dot_f16x32_state_serial_t const *s2, simsimd_dot_f16x32_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = s0->sums[0] + s0->sums[1] + s0->sums[2] + s0->sums[3];
    results[1] = s1->sums[0] + s1->sums[1] + s1->sums[2] + s1->sums[3];
    results[2] = s2->sums[0] + s2->sums[1] + s2->sums[2] + s2->sums[3];
    results[3] = s3->sums[0] + s3->sums[1] + s3->sums[2] + s3->sums[3];
}

/**
 *  @brief Running state for 512-bit dot accumulation over bf16 scalars.
 */
typedef struct simsimd_dot_bf16x32_state_serial_t {
    simsimd_f32_t sums[4];
} simsimd_dot_bf16x32_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_serial(simsimd_dot_bf16x32_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
    state->sums[2] = 0;
    state->sums[3] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_serial(simsimd_dot_bf16x32_state_serial_t *state, simsimd_b512_vec_t a,
                                                        simsimd_b512_vec_t b) {
    simsimd_f32_t sum0 = state->sums[0], sum1 = state->sums[1], sum2 = state->sums[2], sum3 = state->sums[3];
    for (simsimd_size_t i = 0; i < 32; i += 4) {
        simsimd_f32_t a0, a1, a2, a3, b0, b1, b2, b3;
        simsimd_bf16_to_f32(a.bf16s + i + 0, &a0);
        simsimd_bf16_to_f32(a.bf16s + i + 1, &a1);
        simsimd_bf16_to_f32(a.bf16s + i + 2, &a2);
        simsimd_bf16_to_f32(a.bf16s + i + 3, &a3);
        simsimd_bf16_to_f32(b.bf16s + i + 0, &b0);
        simsimd_bf16_to_f32(b.bf16s + i + 1, &b1);
        simsimd_bf16_to_f32(b.bf16s + i + 2, &b2);
        simsimd_bf16_to_f32(b.bf16s + i + 3, &b3);
        sum0 += a0 * b0;
        sum1 += a1 * b1;
        sum2 += a2 * b2;
        sum3 += a3 * b3;
    }
    state->sums[0] = sum0, state->sums[1] = sum1, state->sums[2] = sum2, state->sums[3] = sum3;
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_serial(                                      //
    simsimd_dot_bf16x32_state_serial_t const *s0, simsimd_dot_bf16x32_state_serial_t const *s1, //
    simsimd_dot_bf16x32_state_serial_t const *s2, simsimd_dot_bf16x32_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = s0->sums[0] + s0->sums[1] + s0->sums[2] + s0->sums[3];
    results[1] = s1->sums[0] + s1->sums[1] + s1->sums[2] + s1->sums[3];
    results[2] = s2->sums[0] + s2->sums[1] + s2->sums[2] + s2->sums[3];
    results[3] = s3->sums[0] + s3->sums[1] + s3->sums[2] + s3->sums[3];
}

/**
 *  @brief Running state for 512-bit dot accumulation over i8 scalars.
 */
typedef struct simsimd_dot_i8x64_state_serial_t {
    simsimd_i64_t sums[2];
} simsimd_dot_i8x64_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_serial(simsimd_dot_i8x64_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_serial(simsimd_dot_i8x64_state_serial_t *state, simsimd_b512_vec_t a,
                                                      simsimd_b512_vec_t b) {
    simsimd_i64_t sum0 = state->sums[0];
    simsimd_i64_t sum1 = state->sums[1];

    sum0 += (simsimd_i16_t)a.i8s[0] * (simsimd_i16_t)b.i8s[0],
        sum1 += (simsimd_i16_t)a.i8s[1] * (simsimd_i16_t)b.i8s[1];
    sum0 += (simsimd_i16_t)a.i8s[2] * (simsimd_i16_t)b.i8s[2],
        sum1 += (simsimd_i16_t)a.i8s[3] * (simsimd_i16_t)b.i8s[3];
    sum0 += (simsimd_i16_t)a.i8s[4] * (simsimd_i16_t)b.i8s[4],
        sum1 += (simsimd_i16_t)a.i8s[5] * (simsimd_i16_t)b.i8s[5];
    sum0 += (simsimd_i16_t)a.i8s[6] * (simsimd_i16_t)b.i8s[6],
        sum1 += (simsimd_i16_t)a.i8s[7] * (simsimd_i16_t)b.i8s[7];
    sum0 += (simsimd_i16_t)a.i8s[8] * (simsimd_i16_t)b.i8s[8],
        sum1 += (simsimd_i16_t)a.i8s[9] * (simsimd_i16_t)b.i8s[9];
    sum0 += (simsimd_i16_t)a.i8s[10] * (simsimd_i16_t)b.i8s[10],
        sum1 += (simsimd_i16_t)a.i8s[11] * (simsimd_i16_t)b.i8s[11];
    sum0 += (simsimd_i16_t)a.i8s[12] * (simsimd_i16_t)b.i8s[12],
        sum1 += (simsimd_i16_t)a.i8s[13] * (simsimd_i16_t)b.i8s[13];
    sum0 += (simsimd_i16_t)a.i8s[14] * (simsimd_i16_t)b.i8s[14],
        sum1 += (simsimd_i16_t)a.i8s[15] * (simsimd_i16_t)b.i8s[15];
    sum0 += (simsimd_i16_t)a.i8s[16] * (simsimd_i16_t)b.i8s[16],
        sum1 += (simsimd_i16_t)a.i8s[17] * (simsimd_i16_t)b.i8s[17];
    sum0 += (simsimd_i16_t)a.i8s[18] * (simsimd_i16_t)b.i8s[18],
        sum1 += (simsimd_i16_t)a.i8s[19] * (simsimd_i16_t)b.i8s[19];
    sum0 += (simsimd_i16_t)a.i8s[20] * (simsimd_i16_t)b.i8s[20],
        sum1 += (simsimd_i16_t)a.i8s[21] * (simsimd_i16_t)b.i8s[21];
    sum0 += (simsimd_i16_t)a.i8s[22] * (simsimd_i16_t)b.i8s[22],
        sum1 += (simsimd_i16_t)a.i8s[23] * (simsimd_i16_t)b.i8s[23];
    sum0 += (simsimd_i16_t)a.i8s[24] * (simsimd_i16_t)b.i8s[24],
        sum1 += (simsimd_i16_t)a.i8s[25] * (simsimd_i16_t)b.i8s[25];
    sum0 += (simsimd_i16_t)a.i8s[26] * (simsimd_i16_t)b.i8s[26],
        sum1 += (simsimd_i16_t)a.i8s[27] * (simsimd_i16_t)b.i8s[27];
    sum0 += (simsimd_i16_t)a.i8s[28] * (simsimd_i16_t)b.i8s[28],
        sum1 += (simsimd_i16_t)a.i8s[29] * (simsimd_i16_t)b.i8s[29];
    sum0 += (simsimd_i16_t)a.i8s[30] * (simsimd_i16_t)b.i8s[30],
        sum1 += (simsimd_i16_t)a.i8s[31] * (simsimd_i16_t)b.i8s[31];
    sum0 += (simsimd_i16_t)a.i8s[32] * (simsimd_i16_t)b.i8s[32],
        sum1 += (simsimd_i16_t)a.i8s[33] * (simsimd_i16_t)b.i8s[33];
    sum0 += (simsimd_i16_t)a.i8s[34] * (simsimd_i16_t)b.i8s[34],
        sum1 += (simsimd_i16_t)a.i8s[35] * (simsimd_i16_t)b.i8s[35];
    sum0 += (simsimd_i16_t)a.i8s[36] * (simsimd_i16_t)b.i8s[36],
        sum1 += (simsimd_i16_t)a.i8s[37] * (simsimd_i16_t)b.i8s[37];
    sum0 += (simsimd_i16_t)a.i8s[38] * (simsimd_i16_t)b.i8s[38],
        sum1 += (simsimd_i16_t)a.i8s[39] * (simsimd_i16_t)b.i8s[39];
    sum0 += (simsimd_i16_t)a.i8s[40] * (simsimd_i16_t)b.i8s[40],
        sum1 += (simsimd_i16_t)a.i8s[41] * (simsimd_i16_t)b.i8s[41];
    sum0 += (simsimd_i16_t)a.i8s[42] * (simsimd_i16_t)b.i8s[42],
        sum1 += (simsimd_i16_t)a.i8s[43] * (simsimd_i16_t)b.i8s[43];
    sum0 += (simsimd_i16_t)a.i8s[44] * (simsimd_i16_t)b.i8s[44],
        sum1 += (simsimd_i16_t)a.i8s[45] * (simsimd_i16_t)b.i8s[45];
    sum0 += (simsimd_i16_t)a.i8s[46] * (simsimd_i16_t)b.i8s[46],
        sum1 += (simsimd_i16_t)a.i8s[47] * (simsimd_i16_t)b.i8s[47];
    sum0 += (simsimd_i16_t)a.i8s[48] * (simsimd_i16_t)b.i8s[48],
        sum1 += (simsimd_i16_t)a.i8s[49] * (simsimd_i16_t)b.i8s[49];
    sum0 += (simsimd_i16_t)a.i8s[50] * (simsimd_i16_t)b.i8s[50],
        sum1 += (simsimd_i16_t)a.i8s[51] * (simsimd_i16_t)b.i8s[51];
    sum0 += (simsimd_i16_t)a.i8s[52] * (simsimd_i16_t)b.i8s[52],
        sum1 += (simsimd_i16_t)a.i8s[53] * (simsimd_i16_t)b.i8s[53];
    sum0 += (simsimd_i16_t)a.i8s[54] * (simsimd_i16_t)b.i8s[54],
        sum1 += (simsimd_i16_t)a.i8s[55] * (simsimd_i16_t)b.i8s[55];
    sum0 += (simsimd_i16_t)a.i8s[56] * (simsimd_i16_t)b.i8s[56],
        sum1 += (simsimd_i16_t)a.i8s[57] * (simsimd_i16_t)b.i8s[57];
    sum0 += (simsimd_i16_t)a.i8s[58] * (simsimd_i16_t)b.i8s[58],
        sum1 += (simsimd_i16_t)a.i8s[59] * (simsimd_i16_t)b.i8s[59];
    sum0 += (simsimd_i16_t)a.i8s[60] * (simsimd_i16_t)b.i8s[60],
        sum1 += (simsimd_i16_t)a.i8s[61] * (simsimd_i16_t)b.i8s[61];
    sum0 += (simsimd_i16_t)a.i8s[62] * (simsimd_i16_t)b.i8s[62],
        sum1 += (simsimd_i16_t)a.i8s[63] * (simsimd_i16_t)b.i8s[63];

    state->sums[0] = sum0, state->sums[1] = sum1;
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_serial(                                    //
    simsimd_dot_i8x64_state_serial_t const *s0, simsimd_dot_i8x64_state_serial_t const *s1, //
    simsimd_dot_i8x64_state_serial_t const *s2, simsimd_dot_i8x64_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = (simsimd_f32_t)(s0->sums[0] + s0->sums[1]);
    results[1] = (simsimd_f32_t)(s1->sums[0] + s1->sums[1]);
    results[2] = (simsimd_f32_t)(s2->sums[0] + s2->sums[1]);
    results[3] = (simsimd_f32_t)(s3->sums[0] + s3->sums[1]);
}

/**
 *  @brief Running state for 512-bit dot accumulation over u8 scalars.
 */
typedef struct simsimd_dot_u8x64_state_serial_t {
    simsimd_u64_t sums[2];
} simsimd_dot_u8x64_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_serial(simsimd_dot_u8x64_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_serial(simsimd_dot_u8x64_state_serial_t *state, simsimd_b512_vec_t a,
                                                      simsimd_b512_vec_t b) {
    simsimd_u64_t sum0 = state->sums[0];
    simsimd_u64_t sum1 = state->sums[1];

    sum0 += (simsimd_u16_t)a.u8s[0] * (simsimd_u16_t)b.u8s[0],
        sum1 += (simsimd_u16_t)a.u8s[1] * (simsimd_u16_t)b.u8s[1];
    sum0 += (simsimd_u16_t)a.u8s[2] * (simsimd_u16_t)b.u8s[2],
        sum1 += (simsimd_u16_t)a.u8s[3] * (simsimd_u16_t)b.u8s[3];
    sum0 += (simsimd_u16_t)a.u8s[4] * (simsimd_u16_t)b.u8s[4],
        sum1 += (simsimd_u16_t)a.u8s[5] * (simsimd_u16_t)b.u8s[5];
    sum0 += (simsimd_u16_t)a.u8s[6] * (simsimd_u16_t)b.u8s[6],
        sum1 += (simsimd_u16_t)a.u8s[7] * (simsimd_u16_t)b.u8s[7];
    sum0 += (simsimd_u16_t)a.u8s[8] * (simsimd_u16_t)b.u8s[8],
        sum1 += (simsimd_u16_t)a.u8s[9] * (simsimd_u16_t)b.u8s[9];
    sum0 += (simsimd_u16_t)a.u8s[10] * (simsimd_u16_t)b.u8s[10],
        sum1 += (simsimd_u16_t)a.u8s[11] * (simsimd_u16_t)b.u8s[11];
    sum0 += (simsimd_u16_t)a.u8s[12] * (simsimd_u16_t)b.u8s[12],
        sum1 += (simsimd_u16_t)a.u8s[13] * (simsimd_u16_t)b.u8s[13];
    sum0 += (simsimd_u16_t)a.u8s[14] * (simsimd_u16_t)b.u8s[14],
        sum1 += (simsimd_u16_t)a.u8s[15] * (simsimd_u16_t)b.u8s[15];
    sum0 += (simsimd_u16_t)a.u8s[16] * (simsimd_u16_t)b.u8s[16],
        sum1 += (simsimd_u16_t)a.u8s[17] * (simsimd_u16_t)b.u8s[17];
    sum0 += (simsimd_u16_t)a.u8s[18] * (simsimd_u16_t)b.u8s[18],
        sum1 += (simsimd_u16_t)a.u8s[19] * (simsimd_u16_t)b.u8s[19];
    sum0 += (simsimd_u16_t)a.u8s[20] * (simsimd_u16_t)b.u8s[20],
        sum1 += (simsimd_u16_t)a.u8s[21] * (simsimd_u16_t)b.u8s[21];
    sum0 += (simsimd_u16_t)a.u8s[22] * (simsimd_u16_t)b.u8s[22],
        sum1 += (simsimd_u16_t)a.u8s[23] * (simsimd_u16_t)b.u8s[23];
    sum0 += (simsimd_u16_t)a.u8s[24] * (simsimd_u16_t)b.u8s[24],
        sum1 += (simsimd_u16_t)a.u8s[25] * (simsimd_u16_t)b.u8s[25];
    sum0 += (simsimd_u16_t)a.u8s[26] * (simsimd_u16_t)b.u8s[26],
        sum1 += (simsimd_u16_t)a.u8s[27] * (simsimd_u16_t)b.u8s[27];
    sum0 += (simsimd_u16_t)a.u8s[28] * (simsimd_u16_t)b.u8s[28],
        sum1 += (simsimd_u16_t)a.u8s[29] * (simsimd_u16_t)b.u8s[29];
    sum0 += (simsimd_u16_t)a.u8s[30] * (simsimd_u16_t)b.u8s[30],
        sum1 += (simsimd_u16_t)a.u8s[31] * (simsimd_u16_t)b.u8s[31];
    sum0 += (simsimd_u16_t)a.u8s[32] * (simsimd_u16_t)b.u8s[32],
        sum1 += (simsimd_u16_t)a.u8s[33] * (simsimd_u16_t)b.u8s[33];
    sum0 += (simsimd_u16_t)a.u8s[34] * (simsimd_u16_t)b.u8s[34],
        sum1 += (simsimd_u16_t)a.u8s[35] * (simsimd_u16_t)b.u8s[35];
    sum0 += (simsimd_u16_t)a.u8s[36] * (simsimd_u16_t)b.u8s[36],
        sum1 += (simsimd_u16_t)a.u8s[37] * (simsimd_u16_t)b.u8s[37];
    sum0 += (simsimd_u16_t)a.u8s[38] * (simsimd_u16_t)b.u8s[38],
        sum1 += (simsimd_u16_t)a.u8s[39] * (simsimd_u16_t)b.u8s[39];
    sum0 += (simsimd_u16_t)a.u8s[40] * (simsimd_u16_t)b.u8s[40],
        sum1 += (simsimd_u16_t)a.u8s[41] * (simsimd_u16_t)b.u8s[41];
    sum0 += (simsimd_u16_t)a.u8s[42] * (simsimd_u16_t)b.u8s[42],
        sum1 += (simsimd_u16_t)a.u8s[43] * (simsimd_u16_t)b.u8s[43];
    sum0 += (simsimd_u16_t)a.u8s[44] * (simsimd_u16_t)b.u8s[44],
        sum1 += (simsimd_u16_t)a.u8s[45] * (simsimd_u16_t)b.u8s[45];
    sum0 += (simsimd_u16_t)a.u8s[46] * (simsimd_u16_t)b.u8s[46],
        sum1 += (simsimd_u16_t)a.u8s[47] * (simsimd_u16_t)b.u8s[47];
    sum0 += (simsimd_u16_t)a.u8s[48] * (simsimd_u16_t)b.u8s[48],
        sum1 += (simsimd_u16_t)a.u8s[49] * (simsimd_u16_t)b.u8s[49];
    sum0 += (simsimd_u16_t)a.u8s[50] * (simsimd_u16_t)b.u8s[50],
        sum1 += (simsimd_u16_t)a.u8s[51] * (simsimd_u16_t)b.u8s[51];
    sum0 += (simsimd_u16_t)a.u8s[52] * (simsimd_u16_t)b.u8s[52],
        sum1 += (simsimd_u16_t)a.u8s[53] * (simsimd_u16_t)b.u8s[53];
    sum0 += (simsimd_u16_t)a.u8s[54] * (simsimd_u16_t)b.u8s[54],
        sum1 += (simsimd_u16_t)a.u8s[55] * (simsimd_u16_t)b.u8s[55];
    sum0 += (simsimd_u16_t)a.u8s[56] * (simsimd_u16_t)b.u8s[56],
        sum1 += (simsimd_u16_t)a.u8s[57] * (simsimd_u16_t)b.u8s[57];
    sum0 += (simsimd_u16_t)a.u8s[58] * (simsimd_u16_t)b.u8s[58],
        sum1 += (simsimd_u16_t)a.u8s[59] * (simsimd_u16_t)b.u8s[59];
    sum0 += (simsimd_u16_t)a.u8s[60] * (simsimd_u16_t)b.u8s[60],
        sum1 += (simsimd_u16_t)a.u8s[61] * (simsimd_u16_t)b.u8s[61];
    sum0 += (simsimd_u16_t)a.u8s[62] * (simsimd_u16_t)b.u8s[62],
        sum1 += (simsimd_u16_t)a.u8s[63] * (simsimd_u16_t)b.u8s[63];

    state->sums[0] = sum0, state->sums[1] = sum1;
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_serial(                                    //
    simsimd_dot_u8x64_state_serial_t const *s0, simsimd_dot_u8x64_state_serial_t const *s1, //
    simsimd_dot_u8x64_state_serial_t const *s2, simsimd_dot_u8x64_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = (simsimd_f32_t)(s0->sums[0] + s0->sums[1]);
    results[1] = (simsimd_f32_t)(s1->sums[0] + s1->sums[1]);
    results[2] = (simsimd_f32_t)(s2->sums[0] + s2->sums[1]);
    results[3] = (simsimd_f32_t)(s3->sums[0] + s3->sums[1]);
}

/**
 *  @brief Running state for 512-bit dot accumulation over e4m3 scalars.
 */
typedef struct simsimd_dot_e4m3x64_state_serial_t {
    simsimd_f32_t sums[4];
} simsimd_dot_e4m3x64_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_serial(simsimd_dot_e4m3x64_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
    state->sums[2] = 0;
    state->sums[3] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_serial(simsimd_dot_e4m3x64_state_serial_t *state, simsimd_b512_vec_t a,
                                                        simsimd_b512_vec_t b) {
    simsimd_f32_t sum0 = state->sums[0];
    simsimd_f32_t sum1 = state->sums[1];
    simsimd_f32_t sum2 = state->sums[2];
    simsimd_f32_t sum3 = state->sums[3];
    simsimd_f32_t ai0, ai1, ai2, ai3;
    simsimd_f32_t bi0, bi1, bi2, bi3;
    for (simsimd_size_t i = 0; i != 64; i += 4) {
        simsimd_e4m3_to_f32(a.e4m3s + i, &ai0);
        simsimd_e4m3_to_f32(b.e4m3s + i, &bi0);
        simsimd_e4m3_to_f32(a.e4m3s + i + 1, &ai1);
        simsimd_e4m3_to_f32(b.e4m3s + i + 1, &bi1);
        simsimd_e4m3_to_f32(a.e4m3s + i + 2, &ai2);
        simsimd_e4m3_to_f32(b.e4m3s + i + 2, &bi2);
        simsimd_e4m3_to_f32(a.e4m3s + i + 3, &ai3);
        simsimd_e4m3_to_f32(b.e4m3s + i + 3, &bi3);
        sum0 += ai0 * bi0;
        sum1 += ai1 * bi1;
        sum2 += ai2 * bi2;
        sum3 += ai3 * bi3;
    }

    state->sums[0] = sum0, state->sums[1] = sum1, state->sums[2] = sum2, state->sums[3] = sum3;
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_serial(                                      //
    simsimd_dot_e4m3x64_state_serial_t const *s0, simsimd_dot_e4m3x64_state_serial_t const *s1, //
    simsimd_dot_e4m3x64_state_serial_t const *s2, simsimd_dot_e4m3x64_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = s0->sums[0] + s0->sums[1] + s0->sums[2] + s0->sums[3];
    results[1] = s1->sums[0] + s1->sums[1] + s1->sums[2] + s1->sums[3];
    results[2] = s2->sums[0] + s2->sums[1] + s2->sums[2] + s2->sums[3];
    results[3] = s3->sums[0] + s3->sums[1] + s3->sums[2] + s3->sums[3];
}

/**
 *  @brief Running state for 512-bit dot accumulation over e5m2 scalars.
 */
typedef struct simsimd_dot_e5m2x64_state_serial_t {
    simsimd_f32_t sums[4];
} simsimd_dot_e5m2x64_state_serial_t;

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_serial(simsimd_dot_e5m2x64_state_serial_t *state) {
    state->sums[0] = 0;
    state->sums[1] = 0;
    state->sums[2] = 0;
    state->sums[3] = 0;
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_serial(simsimd_dot_e5m2x64_state_serial_t *state, simsimd_b512_vec_t a,
                                                        simsimd_b512_vec_t b) {
    simsimd_f32_t sum0 = state->sums[0];
    simsimd_f32_t sum1 = state->sums[1];
    simsimd_f32_t sum2 = state->sums[2];
    simsimd_f32_t sum3 = state->sums[3];
    simsimd_f32_t ai0, ai1, ai2, ai3;
    simsimd_f32_t bi0, bi1, bi2, bi3;
    for (simsimd_size_t i = 0; i != 64; i += 4) {
        simsimd_e5m2_to_f32(a.e5m2s + i, &ai0);
        simsimd_e5m2_to_f32(b.e5m2s + i, &bi0);
        simsimd_e5m2_to_f32(a.e5m2s + i + 1, &ai1);
        simsimd_e5m2_to_f32(b.e5m2s + i + 1, &bi1);
        simsimd_e5m2_to_f32(a.e5m2s + i + 2, &ai2);
        simsimd_e5m2_to_f32(b.e5m2s + i + 2, &bi2);
        simsimd_e5m2_to_f32(a.e5m2s + i + 3, &ai3);
        simsimd_e5m2_to_f32(b.e5m2s + i + 3, &bi3);
        sum0 += ai0 * bi0;
        sum1 += ai1 * bi1;
        sum2 += ai2 * bi2;
        sum3 += ai3 * bi3;
    }

    state->sums[0] = sum0, state->sums[1] = sum1, state->sums[2] = sum2, state->sums[3] = sum3;
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_serial(                                      //
    simsimd_dot_e5m2x64_state_serial_t const *s0, simsimd_dot_e5m2x64_state_serial_t const *s1, //
    simsimd_dot_e5m2x64_state_serial_t const *s2, simsimd_dot_e5m2x64_state_serial_t const *s3, //
    simsimd_f32_t *results) {
    results[0] = s0->sums[0] + s0->sums[1] + s0->sums[2] + s0->sums[3];
    results[1] = s1->sums[0] + s1->sums[1] + s1->sums[2] + s1->sums[3];
    results[2] = s2->sums[0] + s2->sums[1] + s2->sums[2] + s2->sums[3];
    results[3] = s3->sums[0] + s3->sums[1] + s3->sums[2] + s3->sums[3];
}

#if _SIMSIMD_TARGET_ARM
#if SIMSIMD_TARGET_NEON
#pragma GCC push_options
#pragma GCC target("arch=armv8-a+simd")
#pragma clang attribute push(__attribute__((target("arch=armv8-a+simd"))), apply_to = function)

SIMSIMD_INTERNAL float32x4_t _simsimd_partial_load_f32x4_neon(simsimd_f32_t const *x, simsimd_size_t n) {
    union {
        float32x4_t vec;
        simsimd_f32_t scalars[4];
    } result;
    simsimd_size_t i = 0;
    for (; i < n; ++i) result.scalars[i] = x[i];
    for (; i < 4; ++i) result.scalars[i] = 0;
    return result.vec;
}

SIMSIMD_PUBLIC void simsimd_dot_f32_neon(simsimd_f32_t const *a_scalars, simsimd_f32_t const *b_scalars,
                                         simsimd_size_t count_scalars, simsimd_distance_t *result) {
    float32x4_t ab_vec = vdupq_n_f32(0);
    simsimd_size_t idx_scalars = 0;
    for (; idx_scalars + 4 <= count_scalars; idx_scalars += 4) {
        float32x4_t a_vec = vld1q_f32(a_scalars + idx_scalars);
        float32x4_t b_vec = vld1q_f32(b_scalars + idx_scalars);
        ab_vec = vfmaq_f32(ab_vec, a_vec, b_vec);
    }
    simsimd_f32_t ab = vaddvq_f32(ab_vec);
    for (; idx_scalars < count_scalars; ++idx_scalars) ab += a_scalars[idx_scalars] * b_scalars[idx_scalars];
    *result = ab;
}

SIMSIMD_PUBLIC void simsimd_dot_f32c_neon(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                          simsimd_size_t count_pairs, simsimd_distance_t *results) {
    float32x4_t ab_real_vec = vdupq_n_f32(0);
    float32x4_t ab_imag_vec = vdupq_n_f32(0);
    simsimd_size_t idx_pairs = 0;
    for (; idx_pairs + 4 <= count_pairs; idx_pairs += 4) {
        // Unpack the input arrays into real and imaginary parts:
        float32x4x2_t a_vec = vld2q_f32((simsimd_f32_t const *)(a_pairs + idx_pairs));
        float32x4x2_t b_vec = vld2q_f32((simsimd_f32_t const *)(b_pairs + idx_pairs));
        float32x4_t a_real_vec = a_vec.val[0];
        float32x4_t a_imag_vec = a_vec.val[1];
        float32x4_t b_real_vec = b_vec.val[0];
        float32x4_t b_imag_vec = b_vec.val[1];

        // Compute the dot product:
        ab_real_vec = vfmaq_f32(ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = vfmsq_f32(ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_imag_vec, b_real_vec);
    }

    // Reduce horizontal sums:
    simsimd_f32_t ab_real = vaddvq_f32(ab_real_vec);
    simsimd_f32_t ab_imag = vaddvq_f32(ab_imag_vec);

    // Handle the tail:
    for (; idx_pairs != count_pairs; ++idx_pairs) {
        simsimd_f32c_t a_pair = a_pairs[idx_pairs], b_pair = b_pairs[idx_pairs];
        simsimd_f32_t ar = a_pair.real, ai = a_pair.imag, br = b_pair.real, bi = b_pair.imag;
        ab_real += ar * br - ai * bi;
        ab_imag += ar * bi + ai * br;
    }
    results[0] = ab_real;
    results[1] = ab_imag;
}

SIMSIMD_PUBLIC void simsimd_vdot_f32c_neon(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                           simsimd_size_t count_pairs, simsimd_distance_t *results) {
    float32x4_t ab_real_vec = vdupq_n_f32(0);
    float32x4_t ab_imag_vec = vdupq_n_f32(0);
    simsimd_size_t idx_pairs = 0;
    for (; idx_pairs + 4 <= count_pairs; idx_pairs += 4) {
        // Unpack the input arrays into real and imaginary parts:
        float32x4x2_t a_vec = vld2q_f32((simsimd_f32_t const *)(a_pairs + idx_pairs));
        float32x4x2_t b_vec = vld2q_f32((simsimd_f32_t const *)(b_pairs + idx_pairs));
        float32x4_t a_real_vec = a_vec.val[0];
        float32x4_t a_imag_vec = a_vec.val[1];
        float32x4_t b_real_vec = b_vec.val[0];
        float32x4_t b_imag_vec = b_vec.val[1];

        // Compute the dot product:
        ab_real_vec = vfmaq_f32(ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = vfmaq_f32(ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = vfmsq_f32(ab_imag_vec, a_imag_vec, b_real_vec);
    }

    // Reduce horizontal sums:
    simsimd_f32_t ab_real = vaddvq_f32(ab_real_vec);
    simsimd_f32_t ab_imag = vaddvq_f32(ab_imag_vec);

    // Handle the tail:
    for (; idx_pairs != count_pairs; ++idx_pairs) {
        simsimd_f32c_t a_pair = a_pairs[idx_pairs], b_pair = b_pairs[idx_pairs];
        simsimd_f32_t ar = a_pair.real, ai = a_pair.imag, br = b_pair.real, bi = b_pair.imag;
        ab_real += ar * br + ai * bi;
        ab_imag += ar * bi - ai * br;
    }
    results[0] = ab_real;
    results[1] = ab_imag;
}

/**
 *  @brief Running state for 512-bit dot accumulation over f32 scalars on NEON.
 */
typedef struct simsimd_dot_f32x16_state_neon_t {
    float32x4_t sum;
} simsimd_dot_f32x16_state_neon_t;

SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_neon(simsimd_dot_f32x16_state_neon_t *state) {
    state->sum = vdupq_n_f32(0);
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_neon(simsimd_dot_f32x16_state_neon_t *state, simsimd_b512_vec_t a,
                                                     simsimd_b512_vec_t b) {
    float32x4_t sum = state->sum;
    sum = vfmaq_f32(sum, vreinterpretq_f32_u32(a.u32x4s[0]), vreinterpretq_f32_u32(b.u32x4s[0]));
    sum = vfmaq_f32(sum, vreinterpretq_f32_u32(a.u32x4s[1]), vreinterpretq_f32_u32(b.u32x4s[1]));
    sum = vfmaq_f32(sum, vreinterpretq_f32_u32(a.u32x4s[2]), vreinterpretq_f32_u32(b.u32x4s[2]));
    sum = vfmaq_f32(sum, vreinterpretq_f32_u32(a.u32x4s[3]), vreinterpretq_f32_u32(b.u32x4s[3]));
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_neon(                                   //
    simsimd_dot_f32x16_state_neon_t const *s0, simsimd_dot_f32x16_state_neon_t const *s1, //
    simsimd_dot_f32x16_state_neon_t const *s2, simsimd_dot_f32x16_state_neon_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for f32 on NEON (4 elements  1 scalar each)
    // Use vaddvq_f32 for each state (horizontal add)
    results[0] = vaddvq_f32(s0->sum);
    results[1] = vaddvq_f32(s1->sum);
    results[2] = vaddvq_f32(s2->sum);
    results[3] = vaddvq_f32(s3->sum);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_NEON

#if SIMSIMD_TARGET_NEON_I8
#pragma GCC push_options
#pragma GCC target("arch=armv8.2-a+dotprod")
#pragma clang attribute push(__attribute__((target("arch=armv8.2-a+dotprod"))), apply_to = function)

SIMSIMD_PUBLIC void simsimd_dot_i8_neon(simsimd_i8_t const *a_scalars, simsimd_i8_t const *b_scalars,
                                        simsimd_size_t count_scalars, simsimd_distance_t *result) {

    int32x4_t ab_vec = vdupq_n_s32(0);
    simsimd_size_t idx_scalars = 0;

    // If the 128-bit `vdot_s32` intrinsic is unavailable, we can use the 64-bit `vdot_s32`.
    // for (simsimd_size_t idx_scalars = 0; idx_scalars != n; idx_scalars += 8) {
    //     int16x8_t a_vec = vmovl_s8(vld1_s8(a_scalars + idx_scalars));
    //     int16x8_t b_vec = vmovl_s8(vld1_s8(b_scalars + idx_scalars));
    //     int16x8_t ab_part_vec = vmulq_s16(a_vec, b_vec);
    //     ab_vec = vaddq_s32(ab_vec, vaddq_s32(vmovl_s16(vget_high_s16(ab_part_vec)), //
    //                                          vmovl_s16(vget_low_s16(ab_part_vec))));
    // }
    for (; idx_scalars + 16 <= count_scalars; idx_scalars += 16) {
        int8x16_t a_vec = vld1q_s8(a_scalars + idx_scalars);
        int8x16_t b_vec = vld1q_s8(b_scalars + idx_scalars);
        ab_vec = vdotq_s32(ab_vec, a_vec, b_vec);
    }

    // Take care of the tail:
    simsimd_i32_t ab = vaddvq_s32(ab_vec);
    for (; idx_scalars < count_scalars; ++idx_scalars) {
        simsimd_i32_t ai = a_scalars[idx_scalars], bi = b_scalars[idx_scalars];
        ab += ai * bi;
    }

    *result = ab;
}

SIMSIMD_PUBLIC void simsimd_dot_u8_neon(simsimd_u8_t const *a_scalars, simsimd_u8_t const *b_scalars,
                                        simsimd_size_t count_scalars, simsimd_distance_t *result) {

    uint32x4_t ab_vec = vdupq_n_u32(0);
    simsimd_size_t idx_scalars = 0;
    for (; idx_scalars + 16 <= count_scalars; idx_scalars += 16) {
        uint8x16_t a_vec = vld1q_u8(a_scalars + idx_scalars);
        uint8x16_t b_vec = vld1q_u8(b_scalars + idx_scalars);
        ab_vec = vdotq_u32(ab_vec, a_vec, b_vec);
    }

    // Take care of the tail:
    simsimd_u32_t ab = vaddvq_u32(ab_vec);
    for (; idx_scalars < count_scalars; ++idx_scalars) {
        simsimd_u32_t ai = a_scalars[idx_scalars], bi = b_scalars[idx_scalars];
        ab += ai * bi;
    }

    *result = ab;
}

/**
 *  @brief Running state for 512-bit dot accumulation over i8 scalars on NEON.
 */
typedef struct simsimd_dot_i8x64_state_neon_t {
    int32x4_t sum;
} simsimd_dot_i8x64_state_neon_t;

SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_neon(simsimd_dot_i8x64_state_neon_t *state) {
    state->sum = vdupq_n_s32(0);
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_neon(simsimd_dot_i8x64_state_neon_t *state, simsimd_b512_vec_t a,
                                                    simsimd_b512_vec_t b) {
    int32x4_t sum = state->sum;
    sum = vdotq_s32(sum, vld1q_s8(a.i8s + 0), vld1q_s8(b.i8s + 0));
    sum = vdotq_s32(sum, vld1q_s8(a.i8s + 16), vld1q_s8(b.i8s + 16));
    sum = vdotq_s32(sum, vld1q_s8(a.i8s + 32), vld1q_s8(b.i8s + 32));
    sum = vdotq_s32(sum, vld1q_s8(a.i8s + 48), vld1q_s8(b.i8s + 48));
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_neon(                                  //
    simsimd_dot_i8x64_state_neon_t const *s0, simsimd_dot_i8x64_state_neon_t const *s1, //
    simsimd_dot_i8x64_state_neon_t const *s2, simsimd_dot_i8x64_state_neon_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for i32 on NEON
    results[0] = (simsimd_f32_t)vaddvq_s32(s0->sum);
    results[1] = (simsimd_f32_t)vaddvq_s32(s1->sum);
    results[2] = (simsimd_f32_t)vaddvq_s32(s2->sum);
    results[3] = (simsimd_f32_t)vaddvq_s32(s3->sum);
}

/**
 *  @brief Running state for 512-bit dot accumulation over u8 scalars on NEON.
 */
typedef struct simsimd_dot_u8x64_state_neon_t {
    uint32x4_t sum;
} simsimd_dot_u8x64_state_neon_t;

SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_neon(simsimd_dot_u8x64_state_neon_t *state) {
    state->sum = vdupq_n_u32(0);
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_neon(simsimd_dot_u8x64_state_neon_t *state, simsimd_b512_vec_t a,
                                                    simsimd_b512_vec_t b) {
    uint32x4_t sum = state->sum;
    sum = vdotq_u32(sum, vld1q_u8(a.u8s + 0), vld1q_u8(b.u8s + 0));
    sum = vdotq_u32(sum, vld1q_u8(a.u8s + 16), vld1q_u8(b.u8s + 16));
    sum = vdotq_u32(sum, vld1q_u8(a.u8s + 32), vld1q_u8(b.u8s + 32));
    sum = vdotq_u32(sum, vld1q_u8(a.u8s + 48), vld1q_u8(b.u8s + 48));
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_neon(                                  //
    simsimd_dot_u8x64_state_neon_t const *s0, simsimd_dot_u8x64_state_neon_t const *s1, //
    simsimd_dot_u8x64_state_neon_t const *s2, simsimd_dot_u8x64_state_neon_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for u32 on NEON
    results[0] = (simsimd_f32_t)vaddvq_u32(s0->sum);
    results[1] = (simsimd_f32_t)vaddvq_u32(s1->sum);
    results[2] = (simsimd_f32_t)vaddvq_u32(s2->sum);
    results[3] = (simsimd_f32_t)vaddvq_u32(s3->sum);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_NEON_I8

#if SIMSIMD_TARGET_NEON_F16
#pragma GCC push_options
#pragma GCC target("arch=armv8.2-a+simd+fp16")
#pragma clang attribute push(__attribute__((target("arch=armv8.2-a+simd+fp16"))), apply_to = function)

SIMSIMD_INTERNAL float16x4_t _simsimd_partial_load_f16x4_neon(simsimd_f16_t const *x, simsimd_size_t n) {
    // In case the software emulation for `f16` scalars is enabled, the `simsimd_f16_to_f32`
    // function will run. It is extremely slow, so even for the tail, let's combine serial
    // loads and stores with vectorized math.
    union {
        float16x4_t vec;
        simsimd_f16_t scalars[4];
    } result;
    simsimd_size_t i = 0;
    for (; i < n; ++i) result.scalars[i] = x[i];
    for (; i < 4; ++i) result.scalars[i] = 0;
    return result.vec;
}

SIMSIMD_PUBLIC void simsimd_dot_f16_neon(simsimd_f16_t const *a_scalars, simsimd_f16_t const *b_scalars,
                                         simsimd_size_t count_scalars, simsimd_distance_t *result) {
    float32x4_t a_vec, b_vec;
    float32x4_t ab_vec = vdupq_n_f32(0);
    simsimd_size_t i = 0;

simsimd_dot_f16_neon_cycle:
    if (count_scalars < 4) {
        a_vec = vcvt_f32_f16(_simsimd_partial_load_f16x4_neon(a_scalars, count_scalars));
        b_vec = vcvt_f32_f16(_simsimd_partial_load_f16x4_neon(b_scalars, count_scalars));
        count_scalars = 0;
    }
    else {
        a_vec = vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)a_scalars));
        b_vec = vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)b_scalars));
        a_scalars += 4, b_scalars += 4, count_scalars -= 4;
    }
    ab_vec = vfmaq_f32(ab_vec, a_vec, b_vec);
    if (count_scalars) goto simsimd_dot_f16_neon_cycle;
    *result = vaddvq_f32(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f16c_neon(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                          simsimd_size_t count_pairs, simsimd_distance_t *results) {

    // A nicer approach is to use `f16` arithmetic for the dot product, but that requires
    // FMLA extensions available on Arm v8.3 and later. That we can also process 16 entries
    // at once. That's how the original implementation worked, but compiling it was a nightmare :)
    float32x4_t ab_real_vec = vdupq_n_f32(0);
    float32x4_t ab_imag_vec = vdupq_n_f32(0);

    while (count_pairs >= 4) {
        // Unpack the input arrays into real and imaginary parts.
        // MSVC sadly doesn't recognize the `vld2_f16`, so we load the  data as signed
        // integers of the same size and reinterpret with `vreinterpret_f16_s16` afterwards.
        int16x4x2_t a_vec = vld2_s16((short *)a_pairs);
        int16x4x2_t b_vec = vld2_s16((short *)b_pairs);
        float32x4_t a_real_vec = vcvt_f32_f16(vreinterpret_f16_s16(a_vec.val[0]));
        float32x4_t a_imag_vec = vcvt_f32_f16(vreinterpret_f16_s16(a_vec.val[1]));
        float32x4_t b_real_vec = vcvt_f32_f16(vreinterpret_f16_s16(b_vec.val[0]));
        float32x4_t b_imag_vec = vcvt_f32_f16(vreinterpret_f16_s16(b_vec.val[1]));

        // Compute the dot product:
        ab_real_vec = vfmaq_f32(ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = vfmsq_f32(ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_imag_vec, b_real_vec);

        count_pairs -= 4, a_pairs += 4, b_pairs += 4;
    }

    // Reduce horizontal sums and aggregate with the tail:
    simsimd_dot_f16c_serial(a_pairs, b_pairs, count_pairs, results);
    results[0] += vaddvq_f32(ab_real_vec);
    results[1] += vaddvq_f32(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f16c_neon(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                           simsimd_size_t count_pairs, simsimd_distance_t *results) {

    // A nicer approach is to use `f16` arithmetic for the dot product, but that requires
    // FMLA extensions available on Arm v8.3 and later. That we can also process 16 entries
    // at once. That's how the original implementation worked, but compiling it was a nightmare :)
    float32x4_t ab_real_vec = vdupq_n_f32(0);
    float32x4_t ab_imag_vec = vdupq_n_f32(0);

    while (count_pairs >= 4) {
        // Unpack the input arrays into real and imaginary parts.
        // MSVC sadly doesn't recognize the `vld2_f16`, so we load the  data as signed
        // integers of the same size and reinterpret with `vreinterpret_f16_s16` afterwards.
        int16x4x2_t a_vec = vld2_s16((short *)a_pairs);
        int16x4x2_t b_vec = vld2_s16((short *)b_pairs);
        float32x4_t a_real_vec = vcvt_f32_f16(vreinterpret_f16_s16(a_vec.val[0]));
        float32x4_t a_imag_vec = vcvt_f32_f16(vreinterpret_f16_s16(a_vec.val[1]));
        float32x4_t b_real_vec = vcvt_f32_f16(vreinterpret_f16_s16(b_vec.val[0]));
        float32x4_t b_imag_vec = vcvt_f32_f16(vreinterpret_f16_s16(b_vec.val[1]));

        // Compute the dot product:
        ab_real_vec = vfmaq_f32(ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = vfmaq_f32(ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = vfmsq_f32(ab_imag_vec, a_imag_vec, b_real_vec);

        count_pairs -= 4, a_pairs += 4, b_pairs += 4;
    }

    // Reduce horizontal sums and aggregate with the tail:
    simsimd_vdot_f16c_serial(a_pairs, b_pairs, count_pairs, results);
    results[0] += vaddvq_f32(ab_real_vec);
    results[1] += vaddvq_f32(ab_imag_vec);
}

/**
 *  @brief Running state for 512-bit dot accumulation over f16 scalars on NEON.
 */
typedef struct simsimd_dot_f16x32_state_neon_t {
    float32x4_t sum;
} simsimd_dot_f16x32_state_neon_t;

SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_neon(simsimd_dot_f16x32_state_neon_t *state) {
    state->sum = vdupq_n_f32(0);
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_neon(simsimd_dot_f16x32_state_neon_t *state, simsimd_b512_vec_t a,
                                                     simsimd_b512_vec_t b) {
    float32x4_t sum = state->sum;
    simsimd_f16_t const *a_scalars = a.f16s;
    simsimd_f16_t const *b_scalars = b.f16s;
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 0))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 0))));
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 4))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 4))));
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 8))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 8))));
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 12))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 12))));
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 16))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 16))));
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 20))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 20))));
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 24))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 24))));
    sum = vfmaq_f32(sum, vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(a_scalars + 28))),
                    vcvt_f32_f16(vld1_f16((simsimd_f16_for_arm_simd_t const *)(b_scalars + 28))));
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_neon(                                   //
    simsimd_dot_f16x32_state_neon_t const *s0, simsimd_dot_f16x32_state_neon_t const *s1, //
    simsimd_dot_f16x32_state_neon_t const *s2, simsimd_dot_f16x32_state_neon_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for f32 on NEON
    results[0] = vaddvq_f32(s0->sum);
    results[1] = vaddvq_f32(s1->sum);
    results[2] = vaddvq_f32(s2->sum);
    results[3] = vaddvq_f32(s3->sum);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_NEON_F16

#if SIMSIMD_TARGET_NEON_BF16
#pragma GCC push_options
#pragma GCC target("arch=armv8.6-a+simd+bf16")
#pragma clang attribute push(__attribute__((target("arch=armv8.6-a+simd+bf16"))), apply_to = function)

SIMSIMD_INTERNAL bfloat16x8_t _simsimd_partial_load_bf16x8_neon(simsimd_bf16_t const *x, simsimd_size_t n) {
    union {
        bfloat16x8_t vec;
        simsimd_bf16_t scalars[8];
    } result;
    simsimd_size_t i = 0;
    for (; i < n; ++i) result.scalars[i] = x[i];
    for (; i < 8; ++i) result.scalars[i] = 0;
    return result.vec;
}

SIMSIMD_PUBLIC void simsimd_dot_bf16_neon(simsimd_bf16_t const *a_scalars, simsimd_bf16_t const *b_scalars,
                                          simsimd_size_t count_scalars, simsimd_distance_t *result) {
    bfloat16x8_t a_vec, b_vec;
    float32x4_t ab_vec = vdupq_n_f32(0);

simsimd_dot_bf16_neon_cycle:
    if (count_scalars < 8) {
        a_vec = _simsimd_partial_load_bf16x8_neon(a_scalars, count_scalars);
        b_vec = _simsimd_partial_load_bf16x8_neon(b_scalars, count_scalars);
        count_scalars = 0;
    }
    else {
        a_vec = vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)a_scalars);
        b_vec = vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)b_scalars);
        a_scalars += 8, b_scalars += 8, count_scalars -= 8;
    }
    ab_vec = vbfdotq_f32(ab_vec, a_vec, b_vec);
    if (count_scalars) goto simsimd_dot_bf16_neon_cycle;

    *result = vaddvq_f32(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_bf16c_neon(simsimd_bf16c_t const *a_pairs, simsimd_bf16c_t const *b_pairs,
                                           simsimd_size_t count_pairs, simsimd_distance_t *results) {

    // A nicer approach is to use `bf16` arithmetic for the dot product, but that requires
    // FMLA extensions available on Arm v8.3 and later. That we can also process 16 entries
    // at once. That's how the original implementation worked, but compiling it was a nightmare :)
    float32x4_t ab_real_vec = vdupq_n_f32(0);
    float32x4_t ab_imag_vec = vdupq_n_f32(0);

    while (count_pairs >= 4) {
        // Unpack the input arrays into real and imaginary parts.
        // MSVC sadly doesn't recognize the `vld2_bf16`, so we load the  data as signed
        // integers of the same size and reinterpret with `vreinterpret_bf16_s16` afterwards.
        int16x4x2_t a_vec = vld2_s16((short const *)a_pairs);
        int16x4x2_t b_vec = vld2_s16((short const *)b_pairs);
        float32x4_t a_real_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(a_vec.val[0]));
        float32x4_t a_imag_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(a_vec.val[1]));
        float32x4_t b_real_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(b_vec.val[0]));
        float32x4_t b_imag_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(b_vec.val[1]));

        // Compute the dot product:
        ab_real_vec = vfmaq_f32(ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = vfmsq_f32(ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_imag_vec, b_real_vec);

        count_pairs -= 4, a_pairs += 4, b_pairs += 4;
    }

    // Reduce horizontal sums and aggregate with the tail:
    simsimd_dot_bf16c_serial(a_pairs, b_pairs, count_pairs, results);
    results[0] += vaddvq_f32(ab_real_vec);
    results[1] += vaddvq_f32(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_bf16c_neon(simsimd_bf16c_t const *a_pairs, simsimd_bf16c_t const *b_pairs,
                                            simsimd_size_t count_pairs, simsimd_distance_t *results) {

    // A nicer approach is to use `bf16` arithmetic for the dot product, but that requires
    // FMLA extensions available on Arm v8.3 and later. That we can also process 16 entries
    // at once. That's how the original implementation worked, but compiling it was a nightmare :)
    float32x4_t ab_real_vec = vdupq_n_f32(0);
    float32x4_t ab_imag_vec = vdupq_n_f32(0);

    while (count_pairs >= 4) {
        // Unpack the input arrays into real and imaginary parts.
        // MSVC sadly doesn't recognize the `vld2_bf16`, so we load the  data as signed
        // integers of the same size and reinterpret with `vreinterpret_bf16_s16` afterwards.
        int16x4x2_t a_vec = vld2_s16((short const *)a_pairs);
        int16x4x2_t b_vec = vld2_s16((short const *)b_pairs);
        float32x4_t a_real_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(a_vec.val[0]));
        float32x4_t a_imag_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(a_vec.val[1]));
        float32x4_t b_real_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(b_vec.val[0]));
        float32x4_t b_imag_vec = vcvt_f32_bf16(vreinterpret_bf16_s16(b_vec.val[1]));

        // Compute the dot product:
        ab_real_vec = vfmaq_f32(ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = vfmaq_f32(ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = vfmaq_f32(ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = vfmsq_f32(ab_imag_vec, a_imag_vec, b_real_vec);

        count_pairs -= 4, a_pairs += 4, b_pairs += 4;
    }

    // Reduce horizontal sums and aggregate with the tail:
    simsimd_vdot_bf16c_serial(a_pairs, b_pairs, count_pairs, results);
    results[0] += vaddvq_f32(ab_real_vec);
    results[1] += vaddvq_f32(ab_imag_vec);
}

/**
 *  @brief Running state for 512-bit dot accumulation over bf16 scalars on NEON.
 */
typedef struct simsimd_dot_bf16x32_state_neon_t {
    float32x4_t sum;
} simsimd_dot_bf16x32_state_neon_t;

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_neon(simsimd_dot_bf16x32_state_neon_t *state) {
    state->sum = vdupq_n_f32(0);
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_neon(simsimd_dot_bf16x32_state_neon_t *state, simsimd_b512_vec_t a,
                                                      simsimd_b512_vec_t b) {
    float32x4_t sum = state->sum;
    simsimd_bf16_t const *a_scalars = a.bf16s;
    simsimd_bf16_t const *b_scalars = b.bf16s;
    sum = vbfdotq_f32(sum, vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(a_scalars + 0)),
                      vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(b_scalars + 0)));
    sum = vbfdotq_f32(sum, vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(a_scalars + 8)),
                      vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(b_scalars + 8)));
    sum = vbfdotq_f32(sum, vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(a_scalars + 16)),
                      vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(b_scalars + 16)));
    sum = vbfdotq_f32(sum, vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(a_scalars + 24)),
                      vld1q_bf16((simsimd_bf16_for_arm_simd_t const *)(b_scalars + 24)));
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_neon(                                    //
    simsimd_dot_bf16x32_state_neon_t const *s0, simsimd_dot_bf16x32_state_neon_t const *s1, //
    simsimd_dot_bf16x32_state_neon_t const *s2, simsimd_dot_bf16x32_state_neon_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for f32 on NEON
    results[0] = vaddvq_f32(s0->sum);
    results[1] = vaddvq_f32(s1->sum);
    results[2] = vaddvq_f32(s2->sum);
    results[3] = vaddvq_f32(s3->sum);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_NEON_BF16

#if SIMSIMD_TARGET_SVE

#pragma GCC push_options
#pragma GCC target("arch=armv8.2-a+sve")
#pragma clang attribute push(__attribute__((target("arch=armv8.2-a+sve"))), apply_to = function)

SIMSIMD_PUBLIC void simsimd_dot_f32_sve(simsimd_f32_t const *a_scalars, simsimd_f32_t const *b_scalars,
                                        simsimd_size_t count_scalars, simsimd_distance_t *result) {
    simsimd_size_t idx_scalars = 0;
    svfloat32_t ab_vec = svdup_f32(0.f);
    do {
        svbool_t pg_vec = svwhilelt_b32((unsigned int)idx_scalars, (unsigned int)count_scalars);
        svfloat32_t a_vec = svld1_f32(pg_vec, a_scalars + idx_scalars);
        svfloat32_t b_vec = svld1_f32(pg_vec, b_scalars + idx_scalars);
        ab_vec = svmla_f32_x(pg_vec, ab_vec, a_vec, b_vec);
        idx_scalars += svcntw();
    } while (idx_scalars < count_scalars);
    *result = svaddv_f32(svptrue_b32(), ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f32c_sve(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                         simsimd_size_t count_pairs, simsimd_distance_t *results) {
    simsimd_size_t idx_pairs = 0;
    svfloat32_t ab_real_vec = svdup_f32(0.f);
    svfloat32_t ab_imag_vec = svdup_f32(0.f);
    do {
        svbool_t pg_vec = svwhilelt_b32((unsigned int)idx_pairs, (unsigned int)count_pairs);
        svfloat32x2_t a_vec = svld2_f32(pg_vec, (simsimd_f32_t const *)(a_pairs + idx_pairs));
        svfloat32x2_t b_vec = svld2_f32(pg_vec, (simsimd_f32_t const *)(b_pairs + idx_pairs));
        svfloat32_t a_real_vec = svget2_f32(a_vec, 0);
        svfloat32_t a_imag_vec = svget2_f32(a_vec, 1);
        svfloat32_t b_real_vec = svget2_f32(b_vec, 0);
        svfloat32_t b_imag_vec = svget2_f32(b_vec, 1);
        ab_real_vec = svmla_f32_x(pg_vec, ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = svmls_f32_x(pg_vec, ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = svmla_f32_x(pg_vec, ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = svmla_f32_x(pg_vec, ab_imag_vec, a_imag_vec, b_real_vec);
        idx_pairs += svcntw();
    } while (idx_pairs < count_pairs);
    results[0] = svaddv_f32(svptrue_b32(), ab_real_vec);
    results[1] = svaddv_f32(svptrue_b32(), ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f32c_sve(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                          simsimd_size_t count_pairs, simsimd_distance_t *results) {
    simsimd_size_t idx_pairs = 0;
    svfloat32_t ab_real_vec = svdup_f32(0.f);
    svfloat32_t ab_imag_vec = svdup_f32(0.f);
    do {
        svbool_t pg_vec = svwhilelt_b32((unsigned int)idx_pairs, (unsigned int)count_pairs);
        svfloat32x2_t a_vec = svld2_f32(pg_vec, (simsimd_f32_t const *)(a_pairs + idx_pairs));
        svfloat32x2_t b_vec = svld2_f32(pg_vec, (simsimd_f32_t const *)(b_pairs + idx_pairs));
        svfloat32_t a_real_vec = svget2_f32(a_vec, 0);
        svfloat32_t a_imag_vec = svget2_f32(a_vec, 1);
        svfloat32_t b_real_vec = svget2_f32(b_vec, 0);
        svfloat32_t b_imag_vec = svget2_f32(b_vec, 1);
        ab_real_vec = svmla_f32_x(pg_vec, ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = svmla_f32_x(pg_vec, ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = svmla_f32_x(pg_vec, ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = svmls_f32_x(pg_vec, ab_imag_vec, a_imag_vec, b_real_vec);
        idx_pairs += svcntw();
    } while (idx_pairs < count_pairs);
    results[0] = svaddv_f32(svptrue_b32(), ab_real_vec);
    results[1] = svaddv_f32(svptrue_b32(), ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f64_sve(simsimd_f64_t const *a_scalars, simsimd_f64_t const *b_scalars,
                                        simsimd_size_t count_scalars, simsimd_distance_t *result) {
    simsimd_size_t idx_scalars = 0;
    svfloat64_t ab_vec = svdup_f64(0.);
    do {
        svbool_t pg_vec = svwhilelt_b64((unsigned int)idx_scalars, (unsigned int)count_scalars);
        svfloat64_t a_vec = svld1_f64(pg_vec, a_scalars + idx_scalars);
        svfloat64_t b_vec = svld1_f64(pg_vec, b_scalars + idx_scalars);
        ab_vec = svmla_f64_x(pg_vec, ab_vec, a_vec, b_vec);
        idx_scalars += svcntd();
    } while (idx_scalars < count_scalars);
    *result = svaddv_f64(svptrue_b32(), ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f64c_sve(simsimd_f64c_t const *a_pairs, simsimd_f64c_t const *b_pairs,
                                         simsimd_size_t count_pairs, simsimd_distance_t *results) {
    simsimd_size_t idx_pairs = 0;
    svfloat64_t ab_real_vec = svdup_f64(0.);
    svfloat64_t ab_imag_vec = svdup_f64(0.);
    do {
        svbool_t pg_vec = svwhilelt_b64((unsigned int)idx_pairs, (unsigned int)count_pairs);
        svfloat64x2_t a_vec = svld2_f64(pg_vec, (simsimd_f64_t const *)(a_pairs + idx_pairs));
        svfloat64x2_t b_vec = svld2_f64(pg_vec, (simsimd_f64_t const *)(b_pairs + idx_pairs));
        svfloat64_t a_real_vec = svget2_f64(a_vec, 0);
        svfloat64_t a_imag_vec = svget2_f64(a_vec, 1);
        svfloat64_t b_real_vec = svget2_f64(b_vec, 0);
        svfloat64_t b_imag_vec = svget2_f64(b_vec, 1);
        ab_real_vec = svmla_f64_x(pg_vec, ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = svmls_f64_x(pg_vec, ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = svmla_f64_x(pg_vec, ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = svmla_f64_x(pg_vec, ab_imag_vec, a_imag_vec, b_real_vec);
        idx_pairs += svcntd();
    } while (idx_pairs < count_pairs);
    results[0] = svaddv_f64(svptrue_b64(), ab_real_vec);
    results[1] = svaddv_f64(svptrue_b64(), ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f64c_sve(simsimd_f64c_t const *a_pairs, simsimd_f64c_t const *b_pairs,
                                          simsimd_size_t count_pairs, simsimd_distance_t *results) {
    simsimd_size_t idx_pairs = 0;
    svfloat64_t ab_real_vec = svdup_f64(0.);
    svfloat64_t ab_imag_vec = svdup_f64(0.);
    do {
        svbool_t pg_vec = svwhilelt_b64((unsigned int)idx_pairs, (unsigned int)count_pairs);
        svfloat64x2_t a_vec = svld2_f64(pg_vec, (simsimd_f64_t const *)(a_pairs + idx_pairs));
        svfloat64x2_t b_vec = svld2_f64(pg_vec, (simsimd_f64_t const *)(b_pairs + idx_pairs));
        svfloat64_t a_real_vec = svget2_f64(a_vec, 0);
        svfloat64_t a_imag_vec = svget2_f64(a_vec, 1);
        svfloat64_t b_real_vec = svget2_f64(b_vec, 0);
        svfloat64_t b_imag_vec = svget2_f64(b_vec, 1);
        ab_real_vec = svmla_f64_x(pg_vec, ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = svmla_f64_x(pg_vec, ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = svmla_f64_x(pg_vec, ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = svmls_f64_x(pg_vec, ab_imag_vec, a_imag_vec, b_real_vec);
        idx_pairs += svcntd();
    } while (idx_pairs < count_pairs);
    results[0] = svaddv_f64(svptrue_b64(), ab_real_vec);
    results[1] = svaddv_f64(svptrue_b64(), ab_imag_vec);
}

#pragma clang attribute pop
#pragma GCC pop_options

#pragma GCC push_options
#pragma GCC target("arch=armv8.2-a+sve+fp16")
#pragma clang attribute push(__attribute__((target("arch=armv8.2-a+sve+fp16"))), apply_to = function)

SIMSIMD_PUBLIC void simsimd_dot_f16_sve(simsimd_f16_t const *a_scalars, simsimd_f16_t const *b_scalars,
                                        simsimd_size_t count_scalars, simsimd_distance_t *result) {
    simsimd_size_t idx_scalars = 0;
    svfloat16_t ab_vec = svdup_f16(0);
    do {
        svbool_t pg_vec = svwhilelt_b16((unsigned int)idx_scalars, (unsigned int)count_scalars);
        svfloat16_t a_vec = svld1_f16(pg_vec, (simsimd_f16_for_arm_simd_t const *)(a_scalars + idx_scalars));
        svfloat16_t b_vec = svld1_f16(pg_vec, (simsimd_f16_for_arm_simd_t const *)(b_scalars + idx_scalars));
        ab_vec = svmla_f16_x(pg_vec, ab_vec, a_vec, b_vec);
        idx_scalars += svcnth();
    } while (idx_scalars < count_scalars);
    simsimd_f16_for_arm_simd_t ab = svaddv_f16(svptrue_b16(), ab_vec);
    *result = ab;
}

SIMSIMD_PUBLIC void simsimd_dot_f16c_sve(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                         simsimd_size_t count_pairs, simsimd_distance_t *results) {
    simsimd_size_t idx_pairs = 0;
    svfloat16_t ab_real_vec = svdup_f16(0);
    svfloat16_t ab_imag_vec = svdup_f16(0);
    do {
        svbool_t pg_vec = svwhilelt_b32((unsigned int)idx_pairs, (unsigned int)count_pairs);
        svfloat16x2_t a_vec = svld2_f16(pg_vec, (simsimd_f16_for_arm_simd_t const *)(a_pairs + idx_pairs));
        svfloat16x2_t b_vec = svld2_f16(pg_vec, (simsimd_f16_for_arm_simd_t const *)(b_pairs + idx_pairs));
        svfloat16_t a_real_vec = svget2_f16(a_vec, 0);
        svfloat16_t a_imag_vec = svget2_f16(a_vec, 1);
        svfloat16_t b_real_vec = svget2_f16(b_vec, 0);
        svfloat16_t b_imag_vec = svget2_f16(b_vec, 1);
        ab_real_vec = svmla_f16_x(pg_vec, ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = svmls_f16_x(pg_vec, ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = svmla_f16_x(pg_vec, ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = svmla_f16_x(pg_vec, ab_imag_vec, a_imag_vec, b_real_vec);
        idx_pairs += svcnth();
    } while (idx_pairs < count_pairs);
    results[0] = svaddv_f16(svptrue_b16(), ab_real_vec);
    results[1] = svaddv_f16(svptrue_b16(), ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f16c_sve(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                          simsimd_size_t count_pairs, simsimd_distance_t *results) {
    simsimd_size_t idx_pairs = 0;
    svfloat16_t ab_real_vec = svdup_f16(0);
    svfloat16_t ab_imag_vec = svdup_f16(0);
    do {
        svbool_t pg_vec = svwhilelt_b32((unsigned int)idx_pairs, (unsigned int)count_pairs);
        svfloat16x2_t a_vec = svld2_f16(pg_vec, (simsimd_f16_for_arm_simd_t const *)(a_pairs + idx_pairs));
        svfloat16x2_t b_vec = svld2_f16(pg_vec, (simsimd_f16_for_arm_simd_t const *)(b_pairs + idx_pairs));
        svfloat16_t a_real_vec = svget2_f16(a_vec, 0);
        svfloat16_t a_imag_vec = svget2_f16(a_vec, 1);
        svfloat16_t b_real_vec = svget2_f16(b_vec, 0);
        svfloat16_t b_imag_vec = svget2_f16(b_vec, 1);
        ab_real_vec = svmla_f16_x(pg_vec, ab_real_vec, a_real_vec, b_real_vec);
        ab_real_vec = svmla_f16_x(pg_vec, ab_real_vec, a_imag_vec, b_imag_vec);
        ab_imag_vec = svmla_f16_x(pg_vec, ab_imag_vec, a_real_vec, b_imag_vec);
        ab_imag_vec = svmls_f16_x(pg_vec, ab_imag_vec, a_imag_vec, b_real_vec);
        idx_pairs += svcnth();
    } while (idx_pairs < count_pairs);
    results[0] = svaddv_f16(svptrue_b16(), ab_real_vec);
    results[1] = svaddv_f16(svptrue_b16(), ab_imag_vec);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_SVE
#endif // _SIMSIMD_TARGET_ARM

#if _SIMSIMD_TARGET_X86
#if SIMSIMD_TARGET_HASWELL
#pragma GCC push_options
#pragma GCC target("avx2", "f16c", "fma")
#pragma clang attribute push(__attribute__((target("avx2,f16c,fma"))), apply_to = function)

SIMSIMD_INTERNAL simsimd_f64_t _simsimd_reduce_f64x4_haswell(__m256d vec) {
    // Reduce the double-precision vector to a scalar
    // Horizontal add the first and second double-precision values, and third and fourth
    __m128d vec_low = _mm256_castpd256_pd128(vec);
    __m128d vec_high = _mm256_extractf128_pd(vec, 1);
    __m128d vec128 = _mm_add_pd(vec_low, vec_high);

    // Horizontal add again to accumulate all four values into one
    vec128 = _mm_hadd_pd(vec128, vec128);

    // Convert the final sum to a scalar double-precision value and return
    return _mm_cvtsd_f64(vec128);
}

SIMSIMD_INTERNAL simsimd_f64_t _simsimd_reduce_f32x8_haswell(__m256 vec) {
    // Convert the lower and higher 128-bit lanes of the input vector to double precision
    __m128 low_f32 = _mm256_castps256_ps128(vec);
    __m128 high_f32 = _mm256_extractf128_ps(vec, 1);

    // Convert single-precision (float) vectors to double-precision (double) vectors
    __m256d low_f64 = _mm256_cvtps_pd(low_f32);
    __m256d high_f64 = _mm256_cvtps_pd(high_f32);

    // Perform the addition in double-precision
    __m256d sum = _mm256_add_pd(low_f64, high_f64);
    return _simsimd_reduce_f64x4_haswell(sum);
}

SIMSIMD_INTERNAL simsimd_i32_t _simsimd_reduce_i32x8_haswell(__m256i vec) {
    __m128i low = _mm256_castsi256_si128(vec);
    __m128i high = _mm256_extracti128_si256(vec, 1);
    __m128i sum = _mm_add_epi32(low, high);
    sum = _mm_hadd_epi32(sum, sum);
    sum = _mm_hadd_epi32(sum, sum);
    return _mm_cvtsi128_si32(sum);
}

SIMSIMD_PUBLIC void simsimd_dot_f32_haswell(simsimd_f32_t const *a_scalars, simsimd_f32_t const *b_scalars,
                                            simsimd_size_t count_scalars, simsimd_distance_t *results) {

    __m256 ab_vec = _mm256_setzero_ps();
    simsimd_size_t idx_scalars = 0;
    for (; idx_scalars + 8 <= count_scalars; idx_scalars += 8) {
        __m256 a_vec = _mm256_loadu_ps(a_scalars + idx_scalars);
        __m256 b_vec = _mm256_loadu_ps(b_scalars + idx_scalars);
        ab_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_vec);
    }
    simsimd_f64_t ab = _simsimd_reduce_f32x8_haswell(ab_vec);
    for (; idx_scalars < count_scalars; ++idx_scalars) ab += a_scalars[idx_scalars] * b_scalars[idx_scalars];
    *results = ab;
}

SIMSIMD_PUBLIC void simsimd_dot_f32c_haswell(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                             simsimd_size_t count_pairs, simsimd_distance_t *results) {

    // The naive approach would be to use FMA and FMS instructions on different parts of the vectors.
    // Prior to that we would need to shuffle the input vectors to separate real and imaginary parts.
    // Both operations are quite expensive, and the resulting kernel would run at 2.5 GB/s.
    // __m128 ab_real_vec = _mm_setzero_ps();
    // __m128 ab_imag_vec = _mm_setzero_ps();
    // __m256i permute_vec = _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0);
    // simsimd_size_t idx_pairs = 0;
    // for (; idx_pairs + 4 <= count_pairs; idx_pairs += 4) {
    //     __m256 a_vec = _mm256_loadu_ps((simsimd_f32_t const *)(a_pairs + idx_pairs));
    //     __m256 b_vec = _mm256_loadu_ps((simsimd_f32_t const *)(b_pairs + idx_pairs));
    //     __m256 a_shuffled = _mm256_permutevar8x32_ps(a_vec, permute_vec);
    //     __m256 b_shuffled = _mm256_permutevar8x32_ps(b_vec, permute_vec);
    //     __m128 a_real_vec = _mm256_extractf128_ps(a_shuffled, 0);
    //     __m128 a_imag_vec = _mm256_extractf128_ps(a_shuffled, 1);
    //     __m128 b_real_vec = _mm256_extractf128_ps(b_shuffled, 0);
    //     __m128 b_imag_vec = _mm256_extractf128_ps(b_shuffled, 1);
    //     ab_real_vec = _mm_fmadd_ps(a_real_vec, b_real_vec, ab_real_vec);
    //     ab_real_vec = _mm_fnmadd_ps(a_imag_vec, b_imag_vec, ab_real_vec);
    //     ab_imag_vec = _mm_fmadd_ps(a_real_vec, b_imag_vec, ab_imag_vec);
    //     ab_imag_vec = _mm_fmadd_ps(a_imag_vec, b_real_vec, ab_imag_vec);
    // }
    //
    // Instead, we take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors. Moreover, `XOR` can be placed after the primary loop.
    // Both operations are quite cheap, and the throughput doubles from 2.5 GB/s to 5 GB/s.
    __m256 ab_real_vec = _mm256_setzero_ps();
    __m256 ab_imag_vec = _mm256_setzero_ps();
    __m256i sign_flip_vec = _mm256_set1_epi64x(0x8000000000000000);
    __m256i swap_adjacent_vec = _mm256_set_epi8( //
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4,                              // Points to the second f32 in 128-bit lane
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4                               // Points to the second f32 in 128-bit lane
    );

    simsimd_size_t idx_pairs = 0;
    for (; idx_pairs + 4 <= count_pairs; idx_pairs += 4) {
        __m256 a_vec = _mm256_loadu_ps((simsimd_f32_t const *)(a_pairs + idx_pairs));
        __m256 b_vec = _mm256_loadu_ps((simsimd_f32_t const *)(b_pairs + idx_pairs));
        __m256 b_swapped_vec = _mm256_castsi256_ps(_mm256_shuffle_epi8(_mm256_castps_si256(b_vec), swap_adjacent_vec));
        ab_real_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_real_vec);
        ab_imag_vec = _mm256_fmadd_ps(a_vec, b_swapped_vec, ab_imag_vec);
    }

    // Flip the sign bit in every second scalar before accumulation:
    ab_real_vec = _mm256_castsi256_ps(_mm256_xor_si256(_mm256_castps_si256(ab_real_vec), sign_flip_vec));

    // Reduce horizontal sums:
    simsimd_distance_t ab_real = _simsimd_reduce_f32x8_haswell(ab_real_vec);
    simsimd_distance_t ab_imag = _simsimd_reduce_f32x8_haswell(ab_imag_vec);

    // Handle the tail:
    for (; idx_pairs != count_pairs; ++idx_pairs) {
        simsimd_f32c_t a_pair = a_pairs[idx_pairs], b_pair = b_pairs[idx_pairs];
        simsimd_f32_t ar = a_pair.real, ai = a_pair.imag, br = b_pair.real, bi = b_pair.imag;
        ab_real += ar * br - ai * bi;
        ab_imag += ar * bi + ai * br;
    }
    results[0] = ab_real;
    results[1] = ab_imag;
}

SIMSIMD_PUBLIC void simsimd_vdot_f32c_haswell(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                              simsimd_size_t count_pairs, simsimd_distance_t *results) {

    __m256 ab_real_vec = _mm256_setzero_ps();
    __m256 ab_imag_vec = _mm256_setzero_ps();
    __m256i sign_flip_vec = _mm256_set1_epi64x(0x8000000000000000);
    __m256i swap_adjacent_vec = _mm256_set_epi8( //
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4,                              // Points to the second f32 in 128-bit lane
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4                               // Points to the second f32 in 128-bit lane
    );

    simsimd_size_t idx_pairs = 0;
    for (; idx_pairs + 4 <= count_pairs; idx_pairs += 4) {
        __m256 a_vec = _mm256_loadu_ps((simsimd_f32_t const *)(a_pairs + idx_pairs));
        __m256 b_vec = _mm256_loadu_ps((simsimd_f32_t const *)(b_pairs + idx_pairs));
        ab_real_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_real_vec);
        b_vec = _mm256_castsi256_ps(_mm256_shuffle_epi8(_mm256_castps_si256(b_vec), swap_adjacent_vec));
        ab_imag_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_imag_vec);
    }

    // Flip the sign bit in every second scalar before accumulation:
    ab_imag_vec = _mm256_castsi256_ps(_mm256_xor_si256(_mm256_castps_si256(ab_imag_vec), sign_flip_vec));

    // Reduce horizontal sums:
    simsimd_distance_t ab_real = _simsimd_reduce_f32x8_haswell(ab_real_vec);
    simsimd_distance_t ab_imag = _simsimd_reduce_f32x8_haswell(ab_imag_vec);

    // Handle the tail:
    for (; idx_pairs != count_pairs; ++idx_pairs) {
        simsimd_f32c_t a_pair = a_pairs[idx_pairs], b_pair = b_pairs[idx_pairs];
        simsimd_f32_t ar = a_pair.real, ai = a_pair.imag, br = b_pair.real, bi = b_pair.imag;
        ab_real += ar * br + ai * bi;
        ab_imag += ar * bi - ai * br;
    }
    results[0] = ab_real;
    results[1] = ab_imag;
}

SIMSIMD_INTERNAL __m256 _simsimd_partial_load_f16x8_haswell(simsimd_f16_t const *a, simsimd_size_t n) {
    // In case the software emulation for `f16` scalars is enabled, the `simsimd_f16_to_f32`
    // function will run. It is extremely slow, so even for the tail, let's combine serial
    // loads and stores with vectorized math.
    union {
        __m128i vec;
        simsimd_f16_t scalars[8];
    } result;
    simsimd_size_t i = 0;
    for (; i < n; ++i) result.scalars[i] = a[i];
    for (; i < 8; ++i) result.scalars[i] = 0;
    return _mm256_cvtph_ps(result.vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f16_haswell(simsimd_f16_t const *a_scalars, simsimd_f16_t const *b_scalars,
                                            simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m256 a_vec, b_vec;
    __m256 ab_vec = _mm256_setzero_ps();

simsimd_dot_f16_haswell_cycle:
    if (count_scalars < 8) {
        a_vec = _simsimd_partial_load_f16x8_haswell(a_scalars, count_scalars);
        b_vec = _simsimd_partial_load_f16x8_haswell(b_scalars, count_scalars);
        count_scalars = 0;
    }
    else {
        a_vec = _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)a_scalars));
        b_vec = _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)b_scalars));
        count_scalars -= 8, a_scalars += 8, b_scalars += 8;
    }
    // We can silence the NaNs using blends:
    //
    //     __m256 a_is_nan = _mm256_cmp_ps(a_vec, a_vec, _CMP_UNORD_Q);
    //     __m256 b_is_nan = _mm256_cmp_ps(b_vec, b_vec, _CMP_UNORD_Q);
    //     ab_vec = _mm256_blendv_ps(_mm256_fmadd_ps(a_vec, b_vec, ab_vec), ab_vec, _mm256_or_ps(a_is_nan, b_is_nan));
    //
    ab_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_vec);
    if (count_scalars) goto simsimd_dot_f16_haswell_cycle;

    *result = _simsimd_reduce_f32x8_haswell(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f16c_haswell(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                             simsimd_size_t count_pairs, simsimd_distance_t *results) {

    // Ideally the implementation would load 256 bits worth of vector data at a time,
    // shuffle those within a register, split in halfs, and only then upcast.
    // That way, we are stepping through 32x 16-bit vector components at a time, or 16 dimensions.
    // Sadly, shuffling 16-bit entries in a YMM register is hard to implement efficiently.
    //
    // Simpler approach is to load 128 bits at a time, upcast, and then shuffle.
    // This mostly replicates the `simsimd_dot_f32c_haswell`.
    __m256 ab_real_vec = _mm256_setzero_ps();
    __m256 ab_imag_vec = _mm256_setzero_ps();
    __m256i sign_flip_vec = _mm256_set1_epi64x(0x8000000000000000);
    __m256i swap_adjacent_vec = _mm256_set_epi8( //
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4,                              // Points to the second f32 in 128-bit lane
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4                               // Points to the second f32 in 128-bit lane
    );

    while (count_pairs >= 4) {
        __m256 a_vec = _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)a_pairs));
        __m256 b_vec = _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)b_pairs));
        __m256 b_swapped_vec = _mm256_castsi256_ps(_mm256_shuffle_epi8(_mm256_castps_si256(b_vec), swap_adjacent_vec));
        ab_real_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_real_vec);
        ab_imag_vec = _mm256_fmadd_ps(a_vec, b_swapped_vec, ab_imag_vec);
        count_pairs -= 4, a_pairs += 4, b_pairs += 4;
    }

    // Flip the sign bit in every second scalar before accumulation:
    ab_real_vec = _mm256_castsi256_ps(_mm256_xor_si256(_mm256_castps_si256(ab_real_vec), sign_flip_vec));

    // Reduce horizontal sums and aggregate with the tail:
    simsimd_dot_f16c_serial(a_pairs, b_pairs, count_pairs, results);
    results[0] += _simsimd_reduce_f32x8_haswell(ab_real_vec);
    results[1] += _simsimd_reduce_f32x8_haswell(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f16c_haswell(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                              simsimd_size_t count_pairs, simsimd_distance_t *results) {

    // Ideally the implementation would load 256 bits worth of vector data at a time,
    // shuffle those within a register, split in halfs, and only then upcast.
    // That way, we are stepping through 32x 16-bit vector components at a time, or 16 dimensions.
    // Sadly, shuffling 16-bit entries in a YMM register is hard to implement efficiently.
    //
    // Simpler approach is to load 128 bits at a time, upcast, and then shuffle.
    // This mostly replicates the `simsimd_vdot_f32c_haswell`.
    __m256 ab_real_vec = _mm256_setzero_ps();
    __m256 ab_imag_vec = _mm256_setzero_ps();
    __m256i sign_flip_vec = _mm256_set1_epi64x(0x8000000000000000);
    __m256i swap_adjacent_vec = _mm256_set_epi8( //
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4,                              // Points to the second f32 in 128-bit lane
        11, 10, 9, 8,                            // Points to the third f32 in 128-bit lane
        15, 14, 13, 12,                          // Points to the fourth f32 in 128-bit lane
        3, 2, 1, 0,                              // Points to the first f32 in 128-bit lane
        7, 6, 5, 4                               // Points to the second f32 in 128-bit lane
    );

    while (count_pairs >= 4) {
        __m256 a_vec = _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)a_pairs));
        __m256 b_vec = _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)b_pairs));
        ab_real_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_real_vec);
        b_vec = _mm256_castsi256_ps(_mm256_shuffle_epi8(_mm256_castps_si256(b_vec), swap_adjacent_vec));
        ab_imag_vec = _mm256_fmadd_ps(a_vec, b_vec, ab_imag_vec);
        count_pairs -= 4, a_pairs += 4, b_pairs += 4;
    }

    // Flip the sign bit in every second scalar before accumulation:
    ab_imag_vec = _mm256_castsi256_ps(_mm256_xor_si256(_mm256_castps_si256(ab_imag_vec), sign_flip_vec));

    // Reduce horizontal sums and aggregate with the tail:
    simsimd_dot_f16c_serial(a_pairs, b_pairs, count_pairs, results);
    results[0] += _simsimd_reduce_f32x8_haswell(ab_real_vec);
    results[1] += _simsimd_reduce_f32x8_haswell(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_i8_haswell(simsimd_i8_t const *a_scalars, simsimd_i8_t const *b_scalars,
                                           simsimd_size_t count_scalars, simsimd_distance_t *result) {

    __m256i ab_i32_low_vec = _mm256_setzero_si256();
    __m256i ab_i32_high_vec = _mm256_setzero_si256();

    // AVX2 has no instructions for 8-bit signed integer dot-products,
    // but it has a weird instruction for mixed signed-unsigned 8-bit dot-product.
    // So we need to normalize the first vector to its absolute value,
    // and shift the product sign into the second vector.
    //
    //      __m256i a_i8_abs_vec = _mm256_abs_epi8(a_i8_vec);
    //      __m256i b_i8_flipped_vec = _mm256_sign_epi8(b_i8_vec, a_i8_vec);
    //      __m256i ab_i16_vec = _mm256_maddubs_epi16(a_i8_abs_vec, b_i8_flipped_vec);
    //
    // The problem with this approach, however, is the `-128` value in the second vector.
    // Flipping its sign will do nothing, and the result will be incorrect.
    // This can easily lead to noticeable numerical errors in the final result.
    simsimd_size_t idx_scalars = 0;
    for (; idx_scalars + 32 <= count_scalars; idx_scalars += 32) {
        __m256i a_i8_vec = _mm256_lddqu_si256((__m256i const *)(a_scalars + idx_scalars));
        __m256i b_i8_vec = _mm256_lddqu_si256((__m256i const *)(b_scalars + idx_scalars));

        // Upcast `int8` to `int16`
        __m256i a_i16_low_vec = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a_i8_vec, 0));
        __m256i a_i16_high_vec = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a_i8_vec, 1));
        __m256i b_i16_low_vec = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b_i8_vec, 0));
        __m256i b_i16_high_vec = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b_i8_vec, 1));

        // Multiply and accumulate at `int16` level, accumulate at `int32` level
        ab_i32_low_vec = _mm256_add_epi32(ab_i32_low_vec, _mm256_madd_epi16(a_i16_low_vec, b_i16_low_vec));
        ab_i32_high_vec = _mm256_add_epi32(ab_i32_high_vec, _mm256_madd_epi16(a_i16_high_vec, b_i16_high_vec));
    }

    // Horizontal sum across the 256-bit register
    int ab = _simsimd_reduce_i32x8_haswell(_mm256_add_epi32(ab_i32_low_vec, ab_i32_high_vec));

    // Take care of the tail:
    for (; idx_scalars < count_scalars; ++idx_scalars) ab += (int)(a_scalars[idx_scalars]) * b_scalars[idx_scalars];
    *result = ab;
}

SIMSIMD_PUBLIC void simsimd_dot_u8_haswell(simsimd_u8_t const *a_scalars, simsimd_u8_t const *b_scalars,
                                           simsimd_size_t count_scalars, simsimd_distance_t *result) {

    __m256i ab_i32_low_vec = _mm256_setzero_si256();
    __m256i ab_i32_high_vec = _mm256_setzero_si256();
    __m256i const zeros_vec = _mm256_setzero_si256();

    // AVX2 has no instructions for unsigned 8-bit integer dot-products,
    // but it has a weird instruction for mixed signed-unsigned 8-bit dot-product.
    simsimd_size_t idx_scalars = 0;
    for (; idx_scalars + 32 <= count_scalars; idx_scalars += 32) {
        __m256i a_u8_vec = _mm256_lddqu_si256((__m256i const *)(a_scalars + idx_scalars));
        __m256i b_u8_vec = _mm256_lddqu_si256((__m256i const *)(b_scalars + idx_scalars));

        // Upcast `uint8` to `int16`. Unlike the signed version, we can use the unpacking
        // instructions instead of extracts, as they are much faster and more efficient.
        __m256i a_i16_low_vec = _mm256_unpacklo_epi8(a_u8_vec, zeros_vec);
        __m256i a_i16_high_vec = _mm256_unpackhi_epi8(a_u8_vec, zeros_vec);
        __m256i b_i16_low_vec = _mm256_unpacklo_epi8(b_u8_vec, zeros_vec);
        __m256i b_i16_high_vec = _mm256_unpackhi_epi8(b_u8_vec, zeros_vec);

        // Multiply and accumulate at `int16` level, accumulate at `int32` level
        ab_i32_low_vec = _mm256_add_epi32(ab_i32_low_vec, _mm256_madd_epi16(a_i16_low_vec, b_i16_low_vec));
        ab_i32_high_vec = _mm256_add_epi32(ab_i32_high_vec, _mm256_madd_epi16(a_i16_high_vec, b_i16_high_vec));
    }

    // Horizontal sum across the 256-bit register
    int ab = _simsimd_reduce_i32x8_haswell(_mm256_add_epi32(ab_i32_low_vec, ab_i32_high_vec));

    // Take care of the tail:
    for (; idx_scalars < count_scalars; ++idx_scalars) ab += (int)(a_scalars[idx_scalars]) * b_scalars[idx_scalars];
    *result = ab;
}

SIMSIMD_INTERNAL __m256 _simsimd_bf16x8_to_f32x8_haswell(__m128i x) {
    // Upcasting from `bf16` to `f32` is done by shifting the `bf16` values by 16 bits to the left, like:
    return _mm256_castsi256_ps(_mm256_slli_epi32(_mm256_cvtepu16_epi32(x), 16));
}

SIMSIMD_INTERNAL __m128i _simsimd_f32x8_to_bf16x8_haswell(__m256 x) {
    // Pack the 32-bit integers into 16-bit integers.
    // This is less trivial than unpacking: https://stackoverflow.com/a/77781241/2766161
    // The best approach is to shuffle within lanes first: https://stackoverflow.com/a/49723746/2766161
    // Our shuffling mask will drop the low 2-bytes from every 4-byte word.
    __m256i trunc_elements = _mm256_shuffle_epi8(                       //
        _mm256_castps_si256(x),                                         //
        _mm256_set_epi8(                                                //
            -1, -1, -1, -1, -1, -1, -1, -1, 15, 14, 11, 10, 7, 6, 3, 2, //
            -1, -1, -1, -1, -1, -1, -1, -1, 15, 14, 11, 10, 7, 6, 3, 2  //
            ));
    __m256i ordered = _mm256_permute4x64_epi64(trunc_elements, 0x58);
    __m128i result = _mm256_castsi256_si128(ordered);
    return result;
}

SIMSIMD_INTERNAL __m128i _simsimd_partial_load_bf16x8_haswell(simsimd_bf16_t const *a, simsimd_size_t n) {
    // In case the software emulation for `bf16` scalars is enabled, the `simsimd_bf16_to_f32`
    // function will run. It is extremely slow, so even for the tail, let's combine serial
    // loads and stores with vectorized math.
    union {
        __m128i vec;
        simsimd_bf16_t scalars[8];
    } result;
    simsimd_size_t i = 0;
    for (; i < n; ++i) result.scalars[i] = a[i];
    for (; i < 8; ++i) result.scalars[i] = 0;
    return result.vec;
}

SIMSIMD_PUBLIC void simsimd_dot_bf16_haswell(simsimd_bf16_t const *a_scalars, simsimd_bf16_t const *b_scalars,
                                             simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m128i a_vec, b_vec;
    __m256 ab_vec = _mm256_setzero_ps();

simsimd_dot_bf16_haswell_cycle:
    if (count_scalars < 8) {
        a_vec = _simsimd_partial_load_bf16x8_haswell(a_scalars, count_scalars);
        b_vec = _simsimd_partial_load_bf16x8_haswell(b_scalars, count_scalars);
        count_scalars = 0;
    }
    else {
        a_vec = _mm_lddqu_si128((__m128i const *)a_scalars);
        b_vec = _mm_lddqu_si128((__m128i const *)b_scalars);
        a_scalars += 8, b_scalars += 8, count_scalars -= 8;
    }
    ab_vec = _mm256_fmadd_ps(_simsimd_bf16x8_to_f32x8_haswell(a_vec), _simsimd_bf16x8_to_f32x8_haswell(b_vec), ab_vec);
    if (count_scalars) goto simsimd_dot_bf16_haswell_cycle;

    *result = _simsimd_reduce_f32x8_haswell(ab_vec);
}

/*  Convert 8x E4M3 values to 8x F32 values using AVX2 bit manipulation.
 *
 *  E4M3 format: S EEEE MMM (bias=7, range: 2^-6 to 448)
 *  F32 format:  S EEEEEEEE MMMMMMMMMMMMMMMMMMMMMMM (bias=127)
 *  Conversion:  sign<<31, (exp+120)<<23, mant<<20
 */
SIMSIMD_INTERNAL __m256 _simsimd_e4m3x8_to_f32x8_haswell(__m128i fp8) {
    // Only use lower 64 bits (8 bytes)
    __m256i v = _mm256_cvtepu8_epi32(fp8);
    __m256i sign = _mm256_slli_epi32(_mm256_and_si256(_mm256_srli_epi32(v, 7), _mm256_set1_epi32(1)), 31);
    __m256i exp = _mm256_and_si256(_mm256_srli_epi32(v, 3), _mm256_set1_epi32(0x0F));
    __m256i mant = _mm256_and_si256(v, _mm256_set1_epi32(0x07));
    // Build F32: (exp + 120) << 23, mant << 20
    __m256i f32_exp = _mm256_slli_epi32(_mm256_add_epi32(exp, _mm256_set1_epi32(120)), 23);
    __m256i f32_mant = _mm256_slli_epi32(mant, 20);
    __m256i f32_bits = _mm256_or_si256(sign, _mm256_or_si256(f32_exp, f32_mant));
    // Handle exp=0: zero out the entire value (flush denormals to zero)
    // AVX2 doesn't have masked move, so use blend with comparison
    __m256i zero_mask = _mm256_cmpeq_epi32(exp, _mm256_setzero_si256());
    f32_bits = _mm256_andnot_si256(zero_mask, f32_bits);
    return _mm256_castsi256_ps(f32_bits);
}

/*  Convert 8x E5M2 values to 8x F32 values using AVX2 bit manipulation.
 *
 *  E5M2 format: S EEEEE MM (bias=15, range: 2^-14 to 57344)
 *  F32 format:  S EEEEEEEE MMMMMMMMMMMMMMMMMMMMMMM (bias=127)
 *  Conversion:  sign<<31, (exp+112)<<23, mant<<21
 */
SIMSIMD_INTERNAL __m256 _simsimd_e5m2x8_to_f32x8_haswell(__m128i fp8) {
    // Only use lower 64 bits (8 bytes)
    __m256i v = _mm256_cvtepu8_epi32(fp8);
    __m256i sign = _mm256_slli_epi32(_mm256_and_si256(_mm256_srli_epi32(v, 7), _mm256_set1_epi32(1)), 31);
    __m256i exp = _mm256_and_si256(_mm256_srli_epi32(v, 2), _mm256_set1_epi32(0x1F));
    __m256i mant = _mm256_and_si256(v, _mm256_set1_epi32(0x03));
    // Build F32: (exp + 112) << 23, mant << 21
    __m256i f32_exp = _mm256_slli_epi32(_mm256_add_epi32(exp, _mm256_set1_epi32(112)), 23);
    __m256i f32_mant = _mm256_slli_epi32(mant, 21);
    __m256i f32_bits = _mm256_or_si256(sign, _mm256_or_si256(f32_exp, f32_mant));
    // Handle exp=0: zero out the entire value (flush denormals to zero)
    __m256i zero_mask = _mm256_cmpeq_epi32(exp, _mm256_setzero_si256());
    f32_bits = _mm256_andnot_si256(zero_mask, f32_bits);
    return _mm256_castsi256_ps(f32_bits);
}

SIMSIMD_INTERNAL __m128i _simsimd_partial_load_e4m3x8_haswell(simsimd_e4m3_t const *a, simsimd_size_t n) {
    union {
        __m128i vec;
        simsimd_u8_t scalars[16];
    } result;
    result.vec = _mm_setzero_si128();
    simsimd_size_t i = 0;
    for (; i < n; ++i) result.scalars[i] = a[i];
    return result.vec;
}

SIMSIMD_INTERNAL __m128i _simsimd_partial_load_e5m2x8_haswell(simsimd_e5m2_t const *a, simsimd_size_t n) {
    union {
        __m128i vec;
        simsimd_u8_t scalars[16];
    } result;
    result.vec = _mm_setzero_si128();
    simsimd_size_t i = 0;
    for (; i < n; ++i) result.scalars[i] = a[i];
    return result.vec;
}

SIMSIMD_PUBLIC void simsimd_dot_e4m3_haswell(simsimd_e4m3_t const *a_scalars, simsimd_e4m3_t const *b_scalars,
                                             simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m128i a_vec, b_vec;
    __m256 ab_vec = _mm256_setzero_ps();

simsimd_dot_e4m3_haswell_cycle:
    if (count_scalars < 8) {
        a_vec = _simsimd_partial_load_e4m3x8_haswell(a_scalars, count_scalars);
        b_vec = _simsimd_partial_load_e4m3x8_haswell(b_scalars, count_scalars);
        count_scalars = 0;
    }
    else {
        a_vec = _mm_loadl_epi64((__m128i const *)a_scalars);
        b_vec = _mm_loadl_epi64((__m128i const *)b_scalars);
        a_scalars += 8, b_scalars += 8, count_scalars -= 8;
    }
    ab_vec = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(a_vec), _simsimd_e4m3x8_to_f32x8_haswell(b_vec), ab_vec);
    if (count_scalars) goto simsimd_dot_e4m3_haswell_cycle;

    *result = _simsimd_reduce_f32x8_haswell(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_e5m2_haswell(simsimd_e5m2_t const *a_scalars, simsimd_e5m2_t const *b_scalars,
                                             simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m128i a_vec, b_vec;
    __m256 ab_vec = _mm256_setzero_ps();

simsimd_dot_e5m2_haswell_cycle:
    if (count_scalars < 8) {
        a_vec = _simsimd_partial_load_e5m2x8_haswell(a_scalars, count_scalars);
        b_vec = _simsimd_partial_load_e5m2x8_haswell(b_scalars, count_scalars);
        count_scalars = 0;
    }
    else {
        a_vec = _mm_loadl_epi64((__m128i const *)a_scalars);
        b_vec = _mm_loadl_epi64((__m128i const *)b_scalars);
        a_scalars += 8, b_scalars += 8, count_scalars -= 8;
    }
    ab_vec = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(a_vec), _simsimd_e5m2x8_to_f32x8_haswell(b_vec), ab_vec);
    if (count_scalars) goto simsimd_dot_e5m2_haswell_cycle;

    *result = _simsimd_reduce_f32x8_haswell(ab_vec);
}

/**
 *  @brief Running state for 16-element dot accumulation over f32 scalars on Haswell.
 */
typedef struct simsimd_dot_f32x16_state_haswell_t {
    __m256 sum;
} simsimd_dot_f32x16_state_haswell_t;

SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_haswell(simsimd_dot_f32x16_state_haswell_t *state) {
    state->sum = _mm256_setzero_ps();
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_haswell(simsimd_dot_f32x16_state_haswell_t *state, simsimd_b512_vec_t a,
                                                        simsimd_b512_vec_t b) {
    __m256 sum = state->sum;
    __m256 a0 = _mm256_loadu_ps(a.f32s + 0);
    __m256 b0 = _mm256_loadu_ps(b.f32s + 0);
    __m256 a1 = _mm256_loadu_ps(a.f32s + 8);
    __m256 b1 = _mm256_loadu_ps(b.f32s + 8);
    sum = _mm256_fmadd_ps(a0, b0, sum);
    sum = _mm256_fmadd_ps(a1, b1, sum);
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_haswell(                                      //
    simsimd_dot_f32x16_state_haswell_t const *s0, simsimd_dot_f32x16_state_haswell_t const *s1, //
    simsimd_dot_f32x16_state_haswell_t const *s2, simsimd_dot_f32x16_state_haswell_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for f32 in AVX2 (8 elements  1 scalar each)
    // Step 1: 84 for all 4 states (extract high 128-bit half and add to low half)
    __m128 a0 = _mm_add_ps(_mm256_castps256_ps128(s0->sum), _mm256_extractf128_ps(s0->sum, 1));
    __m128 a1 = _mm_add_ps(_mm256_castps256_ps128(s1->sum), _mm256_extractf128_ps(s1->sum, 1));
    __m128 a2 = _mm_add_ps(_mm256_castps256_ps128(s2->sum), _mm256_extractf128_ps(s2->sum, 1));
    __m128 a3 = _mm_add_ps(_mm256_castps256_ps128(s3->sum), _mm256_extractf128_ps(s3->sum, 1));
    // Step 2: Transpose 44 matrix of partial sums
    __m128 t01_lo = _mm_unpacklo_ps(a0, a1);     // s0[0], s1[0], s0[1], s1[1]
    __m128 t23_lo = _mm_unpacklo_ps(a2, a3);     // s2[0], s3[0], s2[1], s3[1]
    __m128 t01_hi = _mm_unpackhi_ps(a0, a1);     // s0[2], s1[2], s0[3], s1[3]
    __m128 t23_hi = _mm_unpackhi_ps(a2, a3);     // s2[2], s3[2], s2[3], s3[3]
    __m128 row0 = _mm_movelh_ps(t01_lo, t23_lo); // s0[0], s1[0], s2[0], s3[0]
    __m128 row1 = _mm_movehl_ps(t23_lo, t01_lo); // s0[1], s1[1], s2[1], s3[1]
    __m128 row2 = _mm_movelh_ps(t01_hi, t23_hi); // s0[2], s1[2], s2[2], s3[2]
    __m128 row3 = _mm_movehl_ps(t23_hi, t01_hi); // s0[3], s1[3], s2[3], s3[3]
    // Step 3: Vertical sum - each lane becomes the final result for one state
    __m128 sum = _mm_add_ps(_mm_add_ps(row0, row1), _mm_add_ps(row2, row3));
    _mm_storeu_ps(results, sum);
}

/**
 *  @brief Running state for 32-element dot accumulation over f16 scalars on Haswell.
 */
typedef struct simsimd_dot_f16x32_state_haswell_t {
    __m256 sum;
} simsimd_dot_f16x32_state_haswell_t;

SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_haswell(simsimd_dot_f16x32_state_haswell_t *state) {
    state->sum = _mm256_setzero_ps();
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_haswell(simsimd_dot_f16x32_state_haswell_t *state, simsimd_b512_vec_t a,
                                                        simsimd_b512_vec_t b) {
    __m256 sum = state->sum;
    sum = _mm256_fmadd_ps(_mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(a.f16s + 0))),
                          _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(b.f16s + 0))), sum);
    sum = _mm256_fmadd_ps(_mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(a.f16s + 8))),
                          _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(b.f16s + 8))), sum);
    sum = _mm256_fmadd_ps(_mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(a.f16s + 16))),
                          _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(b.f16s + 16))), sum);
    sum = _mm256_fmadd_ps(_mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(a.f16s + 24))),
                          _mm256_cvtph_ps(_mm_lddqu_si128((__m128i const *)(b.f16s + 24))), sum);
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_haswell(                                      //
    simsimd_dot_f16x32_state_haswell_t const *s0, simsimd_dot_f16x32_state_haswell_t const *s1, //
    simsimd_dot_f16x32_state_haswell_t const *s2, simsimd_dot_f16x32_state_haswell_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f32x16 (both contain just __m256 sum)
    simsimd_dot_f32x16_finalize_haswell(                                                                //
        (simsimd_dot_f32x16_state_haswell_t const *)s0, (simsimd_dot_f32x16_state_haswell_t const *)s1, //
        (simsimd_dot_f32x16_state_haswell_t const *)s2, (simsimd_dot_f32x16_state_haswell_t const *)s3, results);
}

/**
 *  @brief Running state for 32-element dot accumulation over bf16 scalars on Haswell.
 */
typedef struct simsimd_dot_bf16x32_state_haswell_t {
    __m256 sum;
} simsimd_dot_bf16x32_state_haswell_t;

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_haswell(simsimd_dot_bf16x32_state_haswell_t *state) {
    state->sum = _mm256_setzero_ps();
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_haswell(simsimd_dot_bf16x32_state_haswell_t *state,
                                                         simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m256 sum = state->sum;
    sum = _mm256_fmadd_ps(_simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(a.bf16s + 0))),
                          _simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(b.bf16s + 0))), sum);
    sum = _mm256_fmadd_ps(_simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(a.bf16s + 8))),
                          _simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(b.bf16s + 8))), sum);
    sum = _mm256_fmadd_ps(_simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(a.bf16s + 16))),
                          _simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(b.bf16s + 16))), sum);
    sum = _mm256_fmadd_ps(_simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(a.bf16s + 24))),
                          _simsimd_bf16x8_to_f32x8_haswell(_mm_lddqu_si128((__m128i const *)(b.bf16s + 24))), sum);
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_haswell(                                       //
    simsimd_dot_bf16x32_state_haswell_t const *s0, simsimd_dot_bf16x32_state_haswell_t const *s1, //
    simsimd_dot_bf16x32_state_haswell_t const *s2, simsimd_dot_bf16x32_state_haswell_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f32x16 (both contain just __m256 sum)
    simsimd_dot_f32x16_finalize_haswell(                                                                //
        (simsimd_dot_f32x16_state_haswell_t const *)s0, (simsimd_dot_f32x16_state_haswell_t const *)s1, //
        (simsimd_dot_f32x16_state_haswell_t const *)s2, (simsimd_dot_f32x16_state_haswell_t const *)s3, results);
}

/**
 *  @brief Running state for 64-element dot accumulation over e4m3 scalars on Haswell.
 */
typedef struct simsimd_dot_e4m3x64_state_haswell_t {
    __m256 sum;
} simsimd_dot_e4m3x64_state_haswell_t;

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_haswell(simsimd_dot_e4m3x64_state_haswell_t *state) {
    state->sum = _mm256_setzero_ps();
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_haswell(simsimd_dot_e4m3x64_state_haswell_t *state,
                                                         simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m256 sum = state->sum;
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 0))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 0))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 8))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 8))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 16))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 16))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 24))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 24))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 32))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 32))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 40))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 40))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 48))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 48))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e4m3s + 56))),
                          _simsimd_e4m3x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e4m3s + 56))), sum);
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_haswell(                                       //
    simsimd_dot_e4m3x64_state_haswell_t const *s0, simsimd_dot_e4m3x64_state_haswell_t const *s1, //
    simsimd_dot_e4m3x64_state_haswell_t const *s2, simsimd_dot_e4m3x64_state_haswell_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f32x16 (both contain just __m256 sum)
    simsimd_dot_f32x16_finalize_haswell(                                                                //
        (simsimd_dot_f32x16_state_haswell_t const *)s0, (simsimd_dot_f32x16_state_haswell_t const *)s1, //
        (simsimd_dot_f32x16_state_haswell_t const *)s2, (simsimd_dot_f32x16_state_haswell_t const *)s3, results);
}

/**
 *  @brief Running state for 64-element dot accumulation over e5m2 scalars on Haswell.
 */
typedef struct simsimd_dot_e5m2x64_state_haswell_t {
    __m256 sum;
} simsimd_dot_e5m2x64_state_haswell_t;

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_haswell(simsimd_dot_e5m2x64_state_haswell_t *state) {
    state->sum = _mm256_setzero_ps();
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_haswell(simsimd_dot_e5m2x64_state_haswell_t *state,
                                                         simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m256 sum = state->sum;
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 0))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 0))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 8))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 8))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 16))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 16))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 24))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 24))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 32))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 32))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 40))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 40))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 48))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 48))), sum);
    sum = _mm256_fmadd_ps(_simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(a.e5m2s + 56))),
                          _simsimd_e5m2x8_to_f32x8_haswell(_mm_loadl_epi64((__m128i const *)(b.e5m2s + 56))), sum);
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_haswell(                                       //
    simsimd_dot_e5m2x64_state_haswell_t const *s0, simsimd_dot_e5m2x64_state_haswell_t const *s1, //
    simsimd_dot_e5m2x64_state_haswell_t const *s2, simsimd_dot_e5m2x64_state_haswell_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f32x16 (both contain just __m256 sum)
    simsimd_dot_f32x16_finalize_haswell(                                                                //
        (simsimd_dot_f32x16_state_haswell_t const *)s0, (simsimd_dot_f32x16_state_haswell_t const *)s1, //
        (simsimd_dot_f32x16_state_haswell_t const *)s2, (simsimd_dot_f32x16_state_haswell_t const *)s3, results);
}

/**
 *  @brief Running state for 64-element dot accumulation over i8 scalars on Haswell.
 */
typedef struct simsimd_dot_i8x64_state_haswell_t {
    __m256i sum_low;
    __m256i sum_high;
} simsimd_dot_i8x64_state_haswell_t;

SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_haswell(simsimd_dot_i8x64_state_haswell_t *state) {
    state->sum_low = _mm256_setzero_si256();
    state->sum_high = _mm256_setzero_si256();
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_haswell(simsimd_dot_i8x64_state_haswell_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    __m256i sum_low = state->sum_low;
    __m256i sum_high = state->sum_high;

    __m256i a_i8_vec = _mm256_lddqu_si256((__m256i const *)(a.i8s + 0));
    __m256i b_i8_vec = _mm256_lddqu_si256((__m256i const *)(b.i8s + 0));
    __m256i a_i16_low = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a_i8_vec, 0));
    __m256i a_i16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a_i8_vec, 1));
    __m256i b_i16_low = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b_i8_vec, 0));
    __m256i b_i16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b_i8_vec, 1));
    sum_low = _mm256_add_epi32(sum_low, _mm256_madd_epi16(a_i16_low, b_i16_low));
    sum_high = _mm256_add_epi32(sum_high, _mm256_madd_epi16(a_i16_high, b_i16_high));

    a_i8_vec = _mm256_lddqu_si256((__m256i const *)(a.i8s + 32));
    b_i8_vec = _mm256_lddqu_si256((__m256i const *)(b.i8s + 32));
    a_i16_low = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a_i8_vec, 0));
    a_i16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(a_i8_vec, 1));
    b_i16_low = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b_i8_vec, 0));
    b_i16_high = _mm256_cvtepi8_epi16(_mm256_extracti128_si256(b_i8_vec, 1));
    sum_low = _mm256_add_epi32(sum_low, _mm256_madd_epi16(a_i16_low, b_i16_low));
    sum_high = _mm256_add_epi32(sum_high, _mm256_madd_epi16(a_i16_high, b_i16_high));

    state->sum_low = sum_low;
    state->sum_high = sum_high;
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_haswell(                                     //
    simsimd_dot_i8x64_state_haswell_t const *s0, simsimd_dot_i8x64_state_haswell_t const *s1, //
    simsimd_dot_i8x64_state_haswell_t const *s2, simsimd_dot_i8x64_state_haswell_t const *s3, //
    simsimd_f32_t *results) {
    // First, combine the low and high accumulators for each state
    __m256i sum0 = _mm256_add_epi32(s0->sum_low, s0->sum_high);
    __m256i sum1 = _mm256_add_epi32(s1->sum_low, s1->sum_high);
    __m256i sum2 = _mm256_add_epi32(s2->sum_low, s2->sum_high);
    __m256i sum3 = _mm256_add_epi32(s3->sum_low, s3->sum_high);
    // ILP-optimized 4-way horizontal reduction for i32 in AVX2
    // Step 1: 84 for all 4 states
    __m128i a0 = _mm_add_epi32(_mm256_castsi256_si128(sum0), _mm256_extracti128_si256(sum0, 1));
    __m128i a1 = _mm_add_epi32(_mm256_castsi256_si128(sum1), _mm256_extracti128_si256(sum1, 1));
    __m128i a2 = _mm_add_epi32(_mm256_castsi256_si128(sum2), _mm256_extracti128_si256(sum2, 1));
    __m128i a3 = _mm_add_epi32(_mm256_castsi256_si128(sum3), _mm256_extracti128_si256(sum3, 1));
    // Step 2: Transpose 44 matrix
    __m128i t01_lo = _mm_unpacklo_epi32(a0, a1);
    __m128i t23_lo = _mm_unpacklo_epi32(a2, a3);
    __m128i t01_hi = _mm_unpackhi_epi32(a0, a1);
    __m128i t23_hi = _mm_unpackhi_epi32(a2, a3);
    __m128i row0 = _mm_unpacklo_epi64(t01_lo, t23_lo);
    __m128i row1 = _mm_unpackhi_epi64(t01_lo, t23_lo);
    __m128i row2 = _mm_unpacklo_epi64(t01_hi, t23_hi);
    __m128i row3 = _mm_unpackhi_epi64(t01_hi, t23_hi);
    // Step 3: Vertical sum and convert to f32
    __m128i sum_i32 = _mm_add_epi32(_mm_add_epi32(row0, row1), _mm_add_epi32(row2, row3));
    _mm_storeu_ps(results, _mm_cvtepi32_ps(sum_i32));
}

/**
 *  @brief Running state for 64-element dot accumulation over u8 scalars on Haswell.
 */
typedef struct simsimd_dot_u8x64_state_haswell_t {
    __m256i sum_low;
    __m256i sum_high;
} simsimd_dot_u8x64_state_haswell_t;

SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_haswell(simsimd_dot_u8x64_state_haswell_t *state) {
    state->sum_low = _mm256_setzero_si256();
    state->sum_high = _mm256_setzero_si256();
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_haswell(simsimd_dot_u8x64_state_haswell_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    __m256i sum_low = state->sum_low;
    __m256i sum_high = state->sum_high;
    __m256i const zeros_vec = _mm256_setzero_si256();

    __m256i a_u8_vec = _mm256_lddqu_si256((__m256i const *)(a.u8s + 0));
    __m256i b_u8_vec = _mm256_lddqu_si256((__m256i const *)(b.u8s + 0));
    __m256i a_i16_low = _mm256_unpacklo_epi8(a_u8_vec, zeros_vec);
    __m256i a_i16_high = _mm256_unpackhi_epi8(a_u8_vec, zeros_vec);
    __m256i b_i16_low = _mm256_unpacklo_epi8(b_u8_vec, zeros_vec);
    __m256i b_i16_high = _mm256_unpackhi_epi8(b_u8_vec, zeros_vec);
    sum_low = _mm256_add_epi32(sum_low, _mm256_madd_epi16(a_i16_low, b_i16_low));
    sum_high = _mm256_add_epi32(sum_high, _mm256_madd_epi16(a_i16_high, b_i16_high));

    a_u8_vec = _mm256_lddqu_si256((__m256i const *)(a.u8s + 32));
    b_u8_vec = _mm256_lddqu_si256((__m256i const *)(b.u8s + 32));
    a_i16_low = _mm256_unpacklo_epi8(a_u8_vec, zeros_vec);
    a_i16_high = _mm256_unpackhi_epi8(a_u8_vec, zeros_vec);
    b_i16_low = _mm256_unpacklo_epi8(b_u8_vec, zeros_vec);
    b_i16_high = _mm256_unpackhi_epi8(b_u8_vec, zeros_vec);
    sum_low = _mm256_add_epi32(sum_low, _mm256_madd_epi16(a_i16_low, b_i16_low));
    sum_high = _mm256_add_epi32(sum_high, _mm256_madd_epi16(a_i16_high, b_i16_high));

    state->sum_low = sum_low;
    state->sum_high = sum_high;
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_haswell(                                     //
    simsimd_dot_u8x64_state_haswell_t const *s0, simsimd_dot_u8x64_state_haswell_t const *s1, //
    simsimd_dot_u8x64_state_haswell_t const *s2, simsimd_dot_u8x64_state_haswell_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with i8x64 (both contain sum_low and sum_high)
    simsimd_dot_i8x64_finalize_haswell(                                                               //
        (simsimd_dot_i8x64_state_haswell_t const *)s0, (simsimd_dot_i8x64_state_haswell_t const *)s1, //
        (simsimd_dot_i8x64_state_haswell_t const *)s2, (simsimd_dot_i8x64_state_haswell_t const *)s3, results);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_HASWELL

#if SIMSIMD_TARGET_SKYLAKE
#pragma GCC push_options
#pragma GCC target("avx2", "avx512f", "avx512vl", "avx512bw", "bmi2")
#pragma clang attribute push(__attribute__((target("avx2,avx512f,avx512vl,avx512bw,bmi2"))), apply_to = function)

SIMSIMD_INTERNAL simsimd_f64_t _simsimd_reduce_f32x16_skylake(__m512 a) {
    __m512 x = _mm512_add_ps(a, _mm512_shuffle_f32x4(a, a, _MM_SHUFFLE(0, 0, 3, 2)));
    __m128 r = _mm512_castps512_ps128(_mm512_add_ps(x, _mm512_shuffle_f32x4(x, x, _MM_SHUFFLE(0, 0, 0, 1))));
    r = _mm_hadd_ps(r, r);
    return _mm_cvtss_f32(_mm_hadd_ps(r, r));
}

/** @brief Native F32 horizontal reduction without F64 conversion, for high-performance matmul. */
SIMSIMD_INTERNAL simsimd_f32_t _simsimd_reduce_f32x16_to_f32_skylake(__m512 a) {
    __m512 x = _mm512_add_ps(a, _mm512_shuffle_f32x4(a, a, _MM_SHUFFLE(0, 0, 3, 2)));
    __m128 r = _mm512_castps512_ps128(_mm512_add_ps(x, _mm512_shuffle_f32x4(x, x, _MM_SHUFFLE(0, 0, 0, 1))));
    r = _mm_hadd_ps(r, r);
    return _mm_cvtss_f32(_mm_hadd_ps(r, r));
}

SIMSIMD_INTERNAL __m512 _simsimd_bf16x16_to_f32x16_skylake(__m256i a) {
    // Upcasting from `bf16` to `f32` is done by shifting the `bf16` values by 16 bits to the left, like:
    return _mm512_castsi512_ps(_mm512_slli_epi32(_mm512_cvtepu16_epi32(a), 16));
}

SIMSIMD_INTERNAL __m256i _simsimd_f32x16_to_bf16x16_skylake(__m512 a) {
    // Add 2^15 and right shift 16 to do round-nearest
    __m512i x = _mm512_srli_epi32(_mm512_add_epi32(_mm512_castps_si512(a), _mm512_set1_epi32(1 << 15)), 16);
    return _mm512_cvtepi32_epi16(x);
}

SIMSIMD_PUBLIC void simsimd_dot_f32_skylake(simsimd_f32_t const *a_scalars, simsimd_f32_t const *b_scalars,
                                            simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m512 a_vec, b_vec;
    __m512 ab_vec = _mm512_setzero();

simsimd_dot_f32_skylake_cycle:
    if (count_scalars < 16) {
        __mmask16 mask = (__mmask16)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_vec = _mm512_maskz_loadu_ps(mask, a_scalars);
        b_vec = _mm512_maskz_loadu_ps(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_vec = _mm512_loadu_ps(a_scalars);
        b_vec = _mm512_loadu_ps(b_scalars);
        a_scalars += 16, b_scalars += 16, count_scalars -= 16;
    }
    ab_vec = _mm512_fmadd_ps(a_vec, b_vec, ab_vec);
    if (count_scalars) goto simsimd_dot_f32_skylake_cycle;

    *result = _simsimd_reduce_f32x16_skylake(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f64_skylake(simsimd_f64_t const *a_scalars, simsimd_f64_t const *b_scalars,
                                            simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m512d a_vec, b_vec;
    __m512d ab_vec = _mm512_setzero_pd();

simsimd_dot_f64_skylake_cycle:
    if (count_scalars < 8) {
        __mmask8 mask = (__mmask8)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_vec = _mm512_maskz_loadu_pd(mask, a_scalars);
        b_vec = _mm512_maskz_loadu_pd(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_vec = _mm512_loadu_pd(a_scalars);
        b_vec = _mm512_loadu_pd(b_scalars);
        a_scalars += 8, b_scalars += 8, count_scalars -= 8;
    }
    ab_vec = _mm512_fmadd_pd(a_vec, b_vec, ab_vec);
    if (count_scalars) goto simsimd_dot_f64_skylake_cycle;

    *result = _mm512_reduce_add_pd(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f32c_skylake(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                             simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512 a_vec, b_vec;
    __m512 ab_real_vec = _mm512_setzero();
    __m512 ab_imag_vec = _mm512_setzero();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set1_epi64(0x8000000000000000);
simsimd_dot_f32c_skylake_cycle:
    if (count_pairs < 8) {
        __mmask16 mask = (__mmask16)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_ps(mask, a_pairs);
        b_vec = _mm512_maskz_loadu_ps(mask, b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_ps(a_pairs);
        b_vec = _mm512_loadu_ps(b_pairs);
        a_pairs += 8, b_pairs += 8, count_pairs -= 8;
    }
    ab_real_vec = _mm512_fmadd_ps(b_vec, a_vec, ab_real_vec);
    b_vec = _mm512_permute_ps(b_vec, 0xB1); //? Swap adjacent entries within each pair
    ab_imag_vec = _mm512_fmadd_ps(b_vec, a_vec, ab_imag_vec);
    if (count_pairs) goto simsimd_dot_f32c_skylake_cycle;

    // Flip the sign bit in every second scalar before accumulation:
    ab_real_vec = _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(ab_real_vec), sign_flip_vec));

    // Reduce horizontal sums:
    results[0] = _simsimd_reduce_f32x16_skylake(ab_real_vec);
    results[1] = _simsimd_reduce_f32x16_skylake(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f32c_skylake(simsimd_f32c_t const *a_pairs, simsimd_f32c_t const *b_pairs,
                                              simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512 a_vec, b_vec;
    __m512 ab_real_vec = _mm512_setzero();
    __m512 ab_imag_vec = _mm512_setzero();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set1_epi64(0x8000000000000000);
    __m512i const swap_adjacent_vec = _mm512_set_epi8(                  //
        59, 58, 57, 56, 63, 62, 61, 60, 51, 50, 49, 48, 55, 54, 53, 52, // 4th 128-bit lane
        43, 42, 41, 40, 47, 46, 45, 44, 35, 34, 33, 32, 39, 38, 37, 36, // 3rd 128-bit lane
        27, 26, 25, 24, 31, 30, 29, 28, 19, 18, 17, 16, 23, 22, 21, 20, // 2nd 128-bit lane
        11, 10, 9, 8, 15, 14, 13, 12, 3, 2, 1, 0, 7, 6, 5, 4            // 1st 128-bit lane
    );
simsimd_vdot_f32c_skylake_cycle:
    if (count_pairs < 8) {
        __mmask16 mask = (__mmask16)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_ps(mask, (simsimd_f32_t const *)a_pairs);
        b_vec = _mm512_maskz_loadu_ps(mask, (simsimd_f32_t const *)b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_ps((simsimd_f32_t const *)a_pairs);
        b_vec = _mm512_loadu_ps((simsimd_f32_t const *)b_pairs);
        a_pairs += 8, b_pairs += 8, count_pairs -= 8;
    }
    ab_real_vec = _mm512_fmadd_ps(a_vec, b_vec, ab_real_vec);
    b_vec = _mm512_permute_ps(b_vec, 0xB1); //? Swap adjacent entries within each pair
    ab_imag_vec = _mm512_fmadd_ps(a_vec, b_vec, ab_imag_vec);
    if (count_pairs) goto simsimd_vdot_f32c_skylake_cycle;

    // Flip the sign bit in every second scalar before accumulation:
    ab_imag_vec = _mm512_castsi512_ps(_mm512_xor_si512(_mm512_castps_si512(ab_imag_vec), sign_flip_vec));

    // Reduce horizontal sums:
    results[0] = _simsimd_reduce_f32x16_skylake(ab_real_vec);
    results[1] = _simsimd_reduce_f32x16_skylake(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f64c_skylake(simsimd_f64c_t const *a_pairs, simsimd_f64c_t const *b_pairs,
                                             simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512d a_vec, b_vec;
    __m512d ab_real_vec = _mm512_setzero_pd();
    __m512d ab_imag_vec = _mm512_setzero_pd();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set_epi64(                                     //
        0x8000000000000000, 0x0000000000000000, 0x8000000000000000, 0x0000000000000000, //
        0x8000000000000000, 0x0000000000000000, 0x8000000000000000, 0x0000000000000000  //
    );
simsimd_dot_f64c_skylake_cycle:
    if (count_pairs < 4) {
        __mmask8 mask = (__mmask8)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_pd(mask, a_pairs);
        b_vec = _mm512_maskz_loadu_pd(mask, b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_pd(a_pairs);
        b_vec = _mm512_loadu_pd(b_pairs);
        a_pairs += 4, b_pairs += 4, count_pairs -= 4;
    }
    ab_real_vec = _mm512_fmadd_pd(b_vec, a_vec, ab_real_vec);
    b_vec = _mm512_permute_pd(b_vec, 0x55); //? Same as 0b01010101.
    ab_imag_vec = _mm512_fmadd_pd(b_vec, a_vec, ab_imag_vec);
    if (count_pairs) goto simsimd_dot_f64c_skylake_cycle;

    // Flip the sign bit in every second scalar before accumulation:
    ab_real_vec = _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(ab_real_vec), sign_flip_vec));

    // Reduce horizontal sums:
    results[0] = _mm512_reduce_add_pd(ab_real_vec);
    results[1] = _mm512_reduce_add_pd(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f64c_skylake(simsimd_f64c_t const *a_pairs, simsimd_f64c_t const *b_pairs,
                                              simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512d a_vec, b_vec;
    __m512d ab_real_vec = _mm512_setzero_pd();
    __m512d ab_imag_vec = _mm512_setzero_pd();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set_epi64(                                     //
        0x8000000000000000, 0x0000000000000000, 0x8000000000000000, 0x0000000000000000, //
        0x8000000000000000, 0x0000000000000000, 0x8000000000000000, 0x0000000000000000  //
    );
simsimd_vdot_f64c_skylake_cycle:
    if (count_pairs < 4) {
        __mmask8 mask = (__mmask8)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_pd(mask, (simsimd_f32_t const *)a_pairs);
        b_vec = _mm512_maskz_loadu_pd(mask, (simsimd_f32_t const *)b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_pd((simsimd_f32_t const *)a_pairs);
        b_vec = _mm512_loadu_pd((simsimd_f32_t const *)b_pairs);
        a_pairs += 4, b_pairs += 4, count_pairs -= 4;
    }
    ab_real_vec = _mm512_fmadd_pd(a_vec, b_vec, ab_real_vec);
    b_vec = _mm512_permute_pd(b_vec, 0x55); //? Same as 0b01010101.
    ab_imag_vec = _mm512_fmadd_pd(a_vec, b_vec, ab_imag_vec);
    if (count_pairs) goto simsimd_vdot_f64c_skylake_cycle;

    // Flip the sign bit in every second scalar before accumulation:
    ab_imag_vec = _mm512_castsi512_pd(_mm512_xor_si512(_mm512_castpd_si512(ab_imag_vec), sign_flip_vec));

    // Reduce horizontal sums:
    results[0] = _mm512_reduce_add_pd(ab_real_vec);
    results[1] = _mm512_reduce_add_pd(ab_imag_vec);
}

/*  Convert 16x E4M3 values to 16x F32 values using bit manipulation.
 *  This works on Skylake-X and later (AVX-512F only, no BF16/FP16 required).
 *
 *  E4M3 format: S EEEE MMM (bias=7, range: 2^-6 to 448)
 *  F32 format:  S EEEEEEEE MMMMMMMMMMMMMMMMMMMMMMM (bias=127)
 *  Conversion:  sign<<31, (exp+120)<<23, mant<<20
 */
SIMSIMD_INTERNAL __m512 _simsimd_e4m3x16_to_f32x16_skylake(__m128i fp8) {
    __m512i v = _mm512_cvtepu8_epi32(fp8);
    __m512i sign = _mm512_slli_epi32(_mm512_and_si512(_mm512_srli_epi32(v, 7), _mm512_set1_epi32(1)), 31);
    __m512i exp = _mm512_and_si512(_mm512_srli_epi32(v, 3), _mm512_set1_epi32(0x0F));
    __m512i mant = _mm512_and_si512(v, _mm512_set1_epi32(0x07));
    // Build F32: (exp + 120) << 23, mant << 20
    __m512i f32_exp = _mm512_slli_epi32(_mm512_add_epi32(exp, _mm512_set1_epi32(120)), 23);
    __m512i f32_mant = _mm512_slli_epi32(mant, 20);
    __m512i f32_bits = _mm512_or_si512(sign, _mm512_or_si512(f32_exp, f32_mant));
    // DAZ: use TEST to check if exp bits (bits 6-3) are nonzero - single instruction!
    __mmask16 has_exp = _mm512_test_epi32_mask(v, _mm512_set1_epi32(0x78));
    f32_bits = _mm512_maskz_mov_epi32(has_exp, f32_bits);
    return _mm512_castsi512_ps(f32_bits);
}

/*  Convert 16x E5M2 values to 16x F32 values using bit manipulation.
 *  This works on Skylake-X and later (AVX-512F only, no BF16/FP16 required).
 *
 *  E5M2 format: S EEEEE MM (bias=15, range: 2^-14 to 57344)
 *  F32 format:  S EEEEEEEE MMMMMMMMMMMMMMMMMMMMMMM (bias=127)
 *  Conversion:  sign<<31, (exp+112)<<23, mant<<21
 */
SIMSIMD_INTERNAL __m512 _simsimd_e5m2x16_to_f32x16_skylake(__m128i fp8) {
    __m512i v = _mm512_cvtepu8_epi32(fp8);
    __m512i sign = _mm512_slli_epi32(_mm512_and_si512(_mm512_srli_epi32(v, 7), _mm512_set1_epi32(1)), 31);
    __m512i exp = _mm512_and_si512(_mm512_srli_epi32(v, 2), _mm512_set1_epi32(0x1F));
    __m512i mant = _mm512_and_si512(v, _mm512_set1_epi32(0x03));
    // Build F32: (exp + 112) << 23, mant << 21
    __m512i f32_exp = _mm512_slli_epi32(_mm512_add_epi32(exp, _mm512_set1_epi32(112)), 23);
    __m512i f32_mant = _mm512_slli_epi32(mant, 21);
    __m512i f32_bits = _mm512_or_si512(sign, _mm512_or_si512(f32_exp, f32_mant));
    // DAZ: use TEST to check if exp bits (bits 6-2) are nonzero - single instruction!
    __mmask16 has_exp = _mm512_test_epi32_mask(v, _mm512_set1_epi32(0x7C));
    f32_bits = _mm512_maskz_mov_epi32(has_exp, f32_bits);
    return _mm512_castsi512_ps(f32_bits);
}

SIMSIMD_PUBLIC void simsimd_dot_e4m3_skylake(simsimd_e4m3_t const *a_scalars, simsimd_e4m3_t const *b_scalars,
                                             simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m128i a_i8_vec, b_i8_vec;
    __m512 ab_vec = _mm512_setzero_ps();

simsimd_dot_e4m3_skylake_cycle:
    if (count_scalars < 16) {
        __mmask16 mask = (__mmask16)_bzhi_u32(0xFFFF, count_scalars);
        a_i8_vec = _mm_maskz_loadu_epi8(mask, a_scalars);
        b_i8_vec = _mm_maskz_loadu_epi8(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i8_vec = _mm_loadu_si128((__m128i const *)a_scalars);
        b_i8_vec = _mm_loadu_si128((__m128i const *)b_scalars);
        a_scalars += 16, b_scalars += 16, count_scalars -= 16;
    }
    __m512 a_f32 = _simsimd_e4m3x16_to_f32x16_skylake(a_i8_vec);
    __m512 b_f32 = _simsimd_e4m3x16_to_f32x16_skylake(b_i8_vec);
    ab_vec = _mm512_fmadd_ps(a_f32, b_f32, ab_vec);
    if (count_scalars) goto simsimd_dot_e4m3_skylake_cycle;

    *result = _simsimd_reduce_f32x16_skylake(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_e5m2_skylake(simsimd_e5m2_t const *a_scalars, simsimd_e5m2_t const *b_scalars,
                                             simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m128i a_i8_vec, b_i8_vec;
    __m512 ab_vec = _mm512_setzero_ps();

simsimd_dot_e5m2_skylake_cycle:
    if (count_scalars < 16) {
        __mmask16 mask = (__mmask16)_bzhi_u32(0xFFFF, count_scalars);
        a_i8_vec = _mm_maskz_loadu_epi8(mask, a_scalars);
        b_i8_vec = _mm_maskz_loadu_epi8(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i8_vec = _mm_loadu_si128((__m128i const *)a_scalars);
        b_i8_vec = _mm_loadu_si128((__m128i const *)b_scalars);
        a_scalars += 16, b_scalars += 16, count_scalars -= 16;
    }
    __m512 a_f32 = _simsimd_e5m2x16_to_f32x16_skylake(a_i8_vec);
    __m512 b_f32 = _simsimd_e5m2x16_to_f32x16_skylake(b_i8_vec);
    ab_vec = _mm512_fmadd_ps(a_f32, b_f32, ab_vec);
    if (count_scalars) goto simsimd_dot_e5m2_skylake_cycle;

    *result = _simsimd_reduce_f32x16_skylake(ab_vec);
}

/**
 *  @brief Running state for 8-element dot accumulation over f64 scalars on Skylake.
 */
typedef struct simsimd_dot_f64x8_state_skylake_t {
    __m512d sum;
} simsimd_dot_f64x8_state_skylake_t;

SIMSIMD_INTERNAL void simsimd_dot_f64x8_init_skylake(simsimd_dot_f64x8_state_skylake_t *state) {
    state->sum = _mm512_setzero_pd();
}

SIMSIMD_INTERNAL void simsimd_dot_f64x8_update_skylake(simsimd_dot_f64x8_state_skylake_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    state->sum = _mm512_fmadd_pd(a.zmm_pd, b.zmm_pd, state->sum);
}

SIMSIMD_INTERNAL void simsimd_dot_f64x8_finalize_skylake(                                     //
    simsimd_dot_f64x8_state_skylake_t const *s0, simsimd_dot_f64x8_state_skylake_t const *s1, //
    simsimd_dot_f64x8_state_skylake_t const *s2, simsimd_dot_f64x8_state_skylake_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for f64
    // Step 1: 84 for all 4 states (extract high 256-bit half and add to low half)
    __m256d a0 = _mm256_add_pd(_mm512_castpd512_pd256(s0->sum), _mm512_extractf64x4_pd(s0->sum, 1));
    __m256d a1 = _mm256_add_pd(_mm512_castpd512_pd256(s1->sum), _mm512_extractf64x4_pd(s1->sum, 1));
    __m256d a2 = _mm256_add_pd(_mm512_castpd512_pd256(s2->sum), _mm512_extractf64x4_pd(s2->sum, 1));
    __m256d a3 = _mm256_add_pd(_mm512_castpd512_pd256(s3->sum), _mm512_extractf64x4_pd(s3->sum, 1));
    // Step 2: 42 for all 4 states (extract high 128-bit half and add to low half)
    __m128d b0 = _mm_add_pd(_mm256_castpd256_pd128(a0), _mm256_extractf128_pd(a0, 1));
    __m128d b1 = _mm_add_pd(_mm256_castpd256_pd128(a1), _mm256_extractf128_pd(a1, 1));
    __m128d b2 = _mm_add_pd(_mm256_castpd256_pd128(a2), _mm256_extractf128_pd(a2, 1));
    __m128d b3 = _mm_add_pd(_mm256_castpd256_pd128(a3), _mm256_extractf128_pd(a3, 1));
    // Step 3: 21 for each state and combine into 4-element result
    // Each __m128d has [low, high], need to add them to get final scalar
    __m128d sum01 = _mm_add_pd(_mm_unpacklo_pd(b0, b1), _mm_unpackhi_pd(b0, b1)); // [s0_result, s1_result]
    __m128d sum23 = _mm_add_pd(_mm_unpacklo_pd(b2, b3), _mm_unpackhi_pd(b2, b3)); // [s2_result, s3_result]
    // Convert to f32 and store
    __m128 result_f32 = _mm_movelh_ps(_mm_cvtpd_ps(sum01), _mm_cvtpd_ps(sum23));
    _mm_storeu_ps(results, result_f32);
}

/**
 *  @brief Running state for 16-element dot accumulation over f32 scalars on Skylake.
 */
typedef struct simsimd_dot_f32x16_state_skylake_t {
    __m512 sum;
} simsimd_dot_f32x16_state_skylake_t;

SIMSIMD_INTERNAL void simsimd_dot_f32x16_init_skylake(simsimd_dot_f32x16_state_skylake_t *state) {
    state->sum = _mm512_setzero();
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_update_skylake(simsimd_dot_f32x16_state_skylake_t *state, simsimd_b512_vec_t a,
                                                        simsimd_b512_vec_t b) {
    // Use pre-loaded zmm_ps directly (avoids redundant loads when used with GEMM macro)
    state->sum = _mm512_fmadd_ps(a.zmm_ps, b.zmm_ps, state->sum);
}

SIMSIMD_INTERNAL void simsimd_dot_f32x16_finalize_skylake(                                      //
    simsimd_dot_f32x16_state_skylake_t const *s0, simsimd_dot_f32x16_state_skylake_t const *s1, //
    simsimd_dot_f32x16_state_skylake_t const *s2, simsimd_dot_f32x16_state_skylake_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction
    // Step 1: 168 for all 4 states (extract high 256-bit half and add to low half)
    __m256 a0 = _mm256_add_ps(_mm512_castps512_ps256(s0->sum), _mm512_extractf32x8_ps(s0->sum, 1));
    __m256 a1 = _mm256_add_ps(_mm512_castps512_ps256(s1->sum), _mm512_extractf32x8_ps(s1->sum, 1));
    __m256 a2 = _mm256_add_ps(_mm512_castps512_ps256(s2->sum), _mm512_extractf32x8_ps(s2->sum, 1));
    __m256 a3 = _mm256_add_ps(_mm512_castps512_ps256(s3->sum), _mm512_extractf32x8_ps(s3->sum, 1));
    // Step 2: 84 for all 4 states (extract high 128-bit half and add to low half)
    __m128 b0 = _mm_add_ps(_mm256_castps256_ps128(a0), _mm256_extractf128_ps(a0, 1));
    __m128 b1 = _mm_add_ps(_mm256_castps256_ps128(a1), _mm256_extractf128_ps(a1, 1));
    __m128 b2 = _mm_add_ps(_mm256_castps256_ps128(a2), _mm256_extractf128_ps(a2, 1));
    __m128 b3 = _mm_add_ps(_mm256_castps256_ps128(a3), _mm256_extractf128_ps(a3, 1));
    // Step 3: Transpose 44 matrix of partial sums - now each row has one element from each state
    __m128 t01_lo = _mm_unpacklo_ps(b0, b1);     // s0[0], s1[0], s0[1], s1[1]
    __m128 t23_lo = _mm_unpacklo_ps(b2, b3);     // s2[0], s3[0], s2[1], s3[1]
    __m128 t01_hi = _mm_unpackhi_ps(b0, b1);     // s0[2], s1[2], s0[3], s1[3]
    __m128 t23_hi = _mm_unpackhi_ps(b2, b3);     // s2[2], s3[2], s2[3], s3[3]
    __m128 row0 = _mm_movelh_ps(t01_lo, t23_lo); // s0[0], s1[0], s2[0], s3[0]
    __m128 row1 = _mm_movehl_ps(t23_lo, t01_lo); // s0[1], s1[1], s2[1], s3[1]
    __m128 row2 = _mm_movelh_ps(t01_hi, t23_hi); // s0[2], s1[2], s2[2], s3[2]
    __m128 row3 = _mm_movehl_ps(t23_hi, t01_hi); // s0[3], s1[3], s2[3], s3[3]
    // Step 4: Vertical sum - each lane becomes the final result for one state
    __m128 sum = _mm_add_ps(_mm_add_ps(row0, row1), _mm_add_ps(row2, row3));
    _mm_storeu_ps(results, sum);
}

/**
 *  @brief Running state for 64-element dot accumulation over e4m3 scalars on Skylake.
 */
typedef struct simsimd_dot_e4m3x64_state_skylake_t {
    __m512 sum;
} simsimd_dot_e4m3x64_state_skylake_t;

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_skylake(simsimd_dot_e4m3x64_state_skylake_t *state) {
    state->sum = _mm512_setzero();
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_skylake(simsimd_dot_e4m3x64_state_skylake_t *state,
                                                         simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m512 sum = state->sum;
    __m128i a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e4m3s + 0));
    __m128i b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e4m3s + 0));
    sum = _mm512_fmadd_ps(_simsimd_e4m3x16_to_f32x16_skylake(a_i8_vec), _simsimd_e4m3x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e4m3s + 16));
    b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e4m3s + 16));
    sum = _mm512_fmadd_ps(_simsimd_e4m3x16_to_f32x16_skylake(a_i8_vec), _simsimd_e4m3x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e4m3s + 32));
    b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e4m3s + 32));
    sum = _mm512_fmadd_ps(_simsimd_e4m3x16_to_f32x16_skylake(a_i8_vec), _simsimd_e4m3x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e4m3s + 48));
    b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e4m3s + 48));
    sum = _mm512_fmadd_ps(_simsimd_e4m3x16_to_f32x16_skylake(a_i8_vec), _simsimd_e4m3x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_skylake(                                       //
    simsimd_dot_e4m3x64_state_skylake_t const *s0, simsimd_dot_e4m3x64_state_skylake_t const *s1, //
    simsimd_dot_e4m3x64_state_skylake_t const *s2, simsimd_dot_e4m3x64_state_skylake_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f32x16 (both contain just __m512 sum)
    simsimd_dot_f32x16_finalize_skylake(                                                                //
        (simsimd_dot_f32x16_state_skylake_t const *)s0, (simsimd_dot_f32x16_state_skylake_t const *)s1, //
        (simsimd_dot_f32x16_state_skylake_t const *)s2, (simsimd_dot_f32x16_state_skylake_t const *)s3, results);
}

/**
 *  @brief Running state for 64-element dot accumulation over e5m2 scalars on Skylake.
 */
typedef struct simsimd_dot_e5m2x64_state_skylake_t {
    __m512 sum;
} simsimd_dot_e5m2x64_state_skylake_t;

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_skylake(simsimd_dot_e5m2x64_state_skylake_t *state) {
    state->sum = _mm512_setzero();
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_skylake(simsimd_dot_e5m2x64_state_skylake_t *state,
                                                         simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m512 sum = state->sum;
    __m128i a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e5m2s + 0));
    __m128i b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e5m2s + 0));
    sum = _mm512_fmadd_ps(_simsimd_e5m2x16_to_f32x16_skylake(a_i8_vec), _simsimd_e5m2x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e5m2s + 16));
    b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e5m2s + 16));
    sum = _mm512_fmadd_ps(_simsimd_e5m2x16_to_f32x16_skylake(a_i8_vec), _simsimd_e5m2x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e5m2s + 32));
    b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e5m2s + 32));
    sum = _mm512_fmadd_ps(_simsimd_e5m2x16_to_f32x16_skylake(a_i8_vec), _simsimd_e5m2x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    a_i8_vec = _mm_loadu_si128((__m128i const *)(a.e5m2s + 48));
    b_i8_vec = _mm_loadu_si128((__m128i const *)(b.e5m2s + 48));
    sum = _mm512_fmadd_ps(_simsimd_e5m2x16_to_f32x16_skylake(a_i8_vec), _simsimd_e5m2x16_to_f32x16_skylake(b_i8_vec),
                          sum);
    state->sum = sum;
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_skylake(                                       //
    simsimd_dot_e5m2x64_state_skylake_t const *s0, simsimd_dot_e5m2x64_state_skylake_t const *s1, //
    simsimd_dot_e5m2x64_state_skylake_t const *s2, simsimd_dot_e5m2x64_state_skylake_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f32x16 (both contain just __m512 sum)
    simsimd_dot_f32x16_finalize_skylake(                                                                //
        (simsimd_dot_f32x16_state_skylake_t const *)s0, (simsimd_dot_f32x16_state_skylake_t const *)s1, //
        (simsimd_dot_f32x16_state_skylake_t const *)s2, (simsimd_dot_f32x16_state_skylake_t const *)s3, results);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_SKYLAKE

#if SIMSIMD_TARGET_GENOA
#pragma GCC push_options
#pragma GCC target("avx2", "avx512f", "avx512vl", "bmi2", "avx512bw", "avx512bf16")
#pragma clang attribute push(__attribute__((target("avx2,avx512f,avx512vl,bmi2,avx512bw,avx512bf16"))), \
                             apply_to = function)

SIMSIMD_PUBLIC void simsimd_dot_bf16_genoa(simsimd_bf16_t const *a_scalars, simsimd_bf16_t const *b_scalars,
                                           simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m512i a_i16_vec, b_i16_vec;
    __m512 ab_vec = _mm512_setzero_ps();

simsimd_dot_bf16_genoa_cycle:
    if (count_scalars < 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_i16_vec = _mm512_maskz_loadu_epi16(mask, a_scalars);
        b_i16_vec = _mm512_maskz_loadu_epi16(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i16_vec = _mm512_loadu_epi16(a_scalars);
        b_i16_vec = _mm512_loadu_epi16(b_scalars);
        a_scalars += 32, b_scalars += 32, count_scalars -= 32;
    }
    ab_vec = _mm512_dpbf16_ps(ab_vec, (__m512bh)(a_i16_vec), (__m512bh)(b_i16_vec));
    if (count_scalars) goto simsimd_dot_bf16_genoa_cycle;

    *result = _simsimd_reduce_f32x16_skylake(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_bf16c_genoa(simsimd_bf16c_t const *a_pairs, simsimd_bf16c_t const *b_pairs,
                                            simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512i a_vec, b_vec;
    __m512 ab_real_vec = _mm512_setzero_ps();
    __m512 ab_imag_vec = _mm512_setzero_ps();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set1_epi32(0x80000000);
    __m512i const swap_adjacent_vec = _mm512_set_epi8(                  //
        61, 60, 63, 62, 57, 56, 59, 58, 53, 52, 55, 54, 49, 48, 51, 50, // 4th 128-bit lane
        45, 44, 47, 46, 41, 40, 43, 42, 37, 36, 39, 38, 33, 32, 35, 34, // 3rd 128-bit lane
        29, 28, 31, 30, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, // 2nd 128-bit lane
        13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2            // 1st 128-bit lane
    );

simsimd_dot_bf16c_genoa_cycle:
    if (count_pairs < 16) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_epi16(mask, (simsimd_i16_t const *)a_pairs);
        b_vec = _mm512_maskz_loadu_epi16(mask, (simsimd_i16_t const *)b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_epi16((simsimd_i16_t const *)a_pairs);
        b_vec = _mm512_loadu_epi16((simsimd_i16_t const *)b_pairs);
        a_pairs += 16, b_pairs += 16, count_pairs -= 16;
    }
    ab_real_vec = _mm512_dpbf16_ps(ab_real_vec, (__m512bh)(_mm512_xor_si512(b_vec, sign_flip_vec)), (__m512bh)(a_vec));
    ab_imag_vec = _mm512_dpbf16_ps(ab_imag_vec, (__m512bh)(_mm512_shuffle_epi8(b_vec, swap_adjacent_vec)),
                                   (__m512bh)(a_vec));
    if (count_pairs) goto simsimd_dot_bf16c_genoa_cycle;

    // Reduce horizontal sums:
    results[0] = _simsimd_reduce_f32x16_skylake(ab_real_vec);
    results[1] = _simsimd_reduce_f32x16_skylake(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_bf16c_genoa(simsimd_bf16c_t const *a_pairs, simsimd_bf16c_t const *b_pairs,
                                             simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512i a_vec, b_vec;
    __m512 ab_real_vec = _mm512_setzero_ps();
    __m512 ab_imag_vec = _mm512_setzero_ps();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set1_epi32(0x80000000);
    __m512i const swap_adjacent_vec = _mm512_set_epi8(                  //
        61, 60, 63, 62, 57, 56, 59, 58, 53, 52, 55, 54, 49, 48, 51, 50, // 4th 128-bit lane
        45, 44, 47, 46, 41, 40, 43, 42, 37, 36, 39, 38, 33, 32, 35, 34, // 3rd 128-bit lane
        29, 28, 31, 30, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, // 2nd 128-bit lane
        13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2            // 1st 128-bit lane
    );

simsimd_dot_bf16c_genoa_cycle:
    if (count_pairs < 16) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_epi16(mask, (simsimd_i16_t const *)a_pairs);
        b_vec = _mm512_maskz_loadu_epi16(mask, (simsimd_i16_t const *)b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_epi16((simsimd_i16_t const *)a_pairs);
        b_vec = _mm512_loadu_epi16((simsimd_i16_t const *)b_pairs);
        a_pairs += 16, b_pairs += 16, count_pairs -= 16;
    }
    ab_real_vec = _mm512_dpbf16_ps(ab_real_vec, (__m512bh)(a_vec), (__m512bh)(b_vec));
    a_vec = _mm512_xor_si512(a_vec, sign_flip_vec);
    b_vec = _mm512_shuffle_epi8(b_vec, swap_adjacent_vec);
    ab_imag_vec = _mm512_dpbf16_ps(ab_imag_vec, (__m512bh)(a_vec), (__m512bh)(b_vec));
    if (count_pairs) goto simsimd_dot_bf16c_genoa_cycle;

    // Reduce horizontal sums:
    results[0] = _simsimd_reduce_f32x16_skylake(ab_real_vec);
    results[1] = _simsimd_reduce_f32x16_skylake(ab_imag_vec);
}

/**
 *  @brief Convert 32x E4M3 values to 32x BF16 values.
 *
 *  Uses optimized path with fused exp+mant extraction.
 *  Denormals (exp=0, mant!=0) are flushed to zero (DAZ behavior).
 *
 *  E4M3 format: S EEEE MMM (bias=7, range: 2^-6 to 448)
 *  BF16 format: S EEEEEEEE MMMMMMM (bias=127)
 *  Conversion: sign<<8, (exp+120)<<7, mant<<4
 */
SIMSIMD_INTERNAL __m512i _simsimd_e4m3_to_bf16_genoa(__m256i fp8) {
    __m512i v = _mm512_cvtepu8_epi16(fp8);
    // Sign: shift bit 7 to bit 15
    __m512i sign = _mm512_and_si512(_mm512_slli_epi16(v, 8), _mm512_set1_epi16((short)0x8000));
    // Lower 7 bits contain exp (4) and mant (3): shift left 4 and add bias
    __m512i low7 = _mm512_and_si512(v, _mm512_set1_epi16(0x7F));
    __m512i exp_mant = _mm512_add_epi16(_mm512_slli_epi16(low7, 4), _mm512_set1_epi16(0x3C00)); // 120<<7 = 0x3C00
    // DAZ: use TEST to check if exp bits (bits 6-3) are nonzero - single instruction!
    __mmask32 has_exp = _mm512_test_epi16_mask(v, _mm512_set1_epi16(0x78));
    __m512i masked_exp_mant = _mm512_maskz_mov_epi16(has_exp, exp_mant);
    return _mm512_or_si512(sign, masked_exp_mant);
}

/**
 *  @brief Convert 32x E5M2 values to 32x BF16 values.
 *
 *  Uses optimized path with fused exp+mant extraction.
 *  Denormals (exp=0, mant!=0) are flushed to zero (DAZ behavior).
 *
 *  E5M2 format: S EEEEE MM (bias=15, range: 2^-14 to 57344)
 *  BF16 format: S EEEEEEEE MMMMMMM (bias=127)
 *  Conversion: sign<<8, (exp+112)<<7, mant<<5
 */
SIMSIMD_INTERNAL __m512i _simsimd_e5m2_to_bf16_genoa(__m256i fp8) {
    __m512i v = _mm512_cvtepu8_epi16(fp8);
    __m512i sign = _mm512_and_si512(_mm512_slli_epi16(v, 8), _mm512_set1_epi16((short)0x8000));
    // Lower 7 bits: exp(5) + mant(2), shift left 5 and add bias
    __m512i low7 = _mm512_and_si512(v, _mm512_set1_epi16(0x7F));
    __m512i exp_mant = _mm512_add_epi16(_mm512_slli_epi16(low7, 5), _mm512_set1_epi16(0x3800)); // 112<<7 = 0x3800
    // DAZ: use TEST to check if exp bits (bits 6-2) are nonzero - single instruction!
    __mmask32 has_exp = _mm512_test_epi16_mask(v, _mm512_set1_epi16(0x7C));
    __m512i masked_exp_mant = _mm512_maskz_mov_epi16(has_exp, exp_mant);
    return _mm512_or_si512(sign, masked_exp_mant);
}

SIMSIMD_PUBLIC void simsimd_dot_e4m3_genoa(simsimd_e4m3_t const *a_scalars, simsimd_e4m3_t const *b_scalars,
                                           simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m256i a_i8_vec, b_i8_vec;
    __m512 ab_vec = _mm512_setzero_ps();

simsimd_dot_e4m3_genoa_cycle:
    if (count_scalars < 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_i8_vec = _mm256_maskz_loadu_epi8(mask, a_scalars);
        b_i8_vec = _mm256_maskz_loadu_epi8(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i8_vec = _mm256_loadu_epi8(a_scalars);
        b_i8_vec = _mm256_loadu_epi8(b_scalars);
        a_scalars += 32, b_scalars += 32, count_scalars -= 32;
    }
    // Convert E4M3 to BF16 and compute dot product
    __m512i a_bf16 = _simsimd_e4m3_to_bf16_genoa(a_i8_vec);
    __m512i b_bf16 = _simsimd_e4m3_to_bf16_genoa(b_i8_vec);
    ab_vec = _mm512_dpbf16_ps(ab_vec, (__m512bh)(a_bf16), (__m512bh)(b_bf16));
    if (count_scalars) goto simsimd_dot_e4m3_genoa_cycle;

    *result = _simsimd_reduce_f32x16_skylake(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_e5m2_genoa(simsimd_e5m2_t const *a_scalars, simsimd_e5m2_t const *b_scalars,
                                           simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m256i a_i8_vec, b_i8_vec;
    __m512 ab_vec = _mm512_setzero_ps();

simsimd_dot_e5m2_genoa_cycle:
    if (count_scalars < 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_i8_vec = _mm256_maskz_loadu_epi8(mask, a_scalars);
        b_i8_vec = _mm256_maskz_loadu_epi8(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i8_vec = _mm256_loadu_epi8(a_scalars);
        b_i8_vec = _mm256_loadu_epi8(b_scalars);
        a_scalars += 32, b_scalars += 32, count_scalars -= 32;
    }
    // Convert E5M2 to BF16 and compute dot product
    __m512i a_bf16 = _simsimd_e5m2_to_bf16_genoa(a_i8_vec);
    __m512i b_bf16 = _simsimd_e5m2_to_bf16_genoa(b_i8_vec);
    ab_vec = _mm512_dpbf16_ps(ab_vec, (__m512bh)(a_bf16), (__m512bh)(b_bf16));
    if (count_scalars) goto simsimd_dot_e5m2_genoa_cycle;

    *result = _simsimd_reduce_f32x16_skylake(ab_vec);
}

/**
 *  @brief Running state for 32-element dot accumulation over bf16 scalars on Genoa.
 */
typedef struct simsimd_dot_bf16x32_state_genoa_t {
    __m512 sum;
} simsimd_dot_bf16x32_state_genoa_t;

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_init_genoa(simsimd_dot_bf16x32_state_genoa_t *state) {
    state->sum = _mm512_setzero();
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_update_genoa(simsimd_dot_bf16x32_state_genoa_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    // Use pre-loaded zmm directly (avoids redundant loads when used with GEMM macro)
    state->sum = _mm512_dpbf16_ps(state->sum, (__m512bh)(a.zmm), (__m512bh)(b.zmm));
}

SIMSIMD_INTERNAL void simsimd_dot_bf16x32_finalize_genoa(                                     //
    simsimd_dot_bf16x32_state_genoa_t const *s0, simsimd_dot_bf16x32_state_genoa_t const *s1, //
    simsimd_dot_bf16x32_state_genoa_t const *s2, simsimd_dot_bf16x32_state_genoa_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction (same logic as Skylake f32x16)
    __m256 a0 = _mm256_add_ps(_mm512_castps512_ps256(s0->sum), _mm512_extractf32x8_ps(s0->sum, 1));
    __m256 a1 = _mm256_add_ps(_mm512_castps512_ps256(s1->sum), _mm512_extractf32x8_ps(s1->sum, 1));
    __m256 a2 = _mm256_add_ps(_mm512_castps512_ps256(s2->sum), _mm512_extractf32x8_ps(s2->sum, 1));
    __m256 a3 = _mm256_add_ps(_mm512_castps512_ps256(s3->sum), _mm512_extractf32x8_ps(s3->sum, 1));
    __m128 b0 = _mm_add_ps(_mm256_castps256_ps128(a0), _mm256_extractf128_ps(a0, 1));
    __m128 b1 = _mm_add_ps(_mm256_castps256_ps128(a1), _mm256_extractf128_ps(a1, 1));
    __m128 b2 = _mm_add_ps(_mm256_castps256_ps128(a2), _mm256_extractf128_ps(a2, 1));
    __m128 b3 = _mm_add_ps(_mm256_castps256_ps128(a3), _mm256_extractf128_ps(a3, 1));
    __m128 t01_lo = _mm_unpacklo_ps(b0, b1), t23_lo = _mm_unpacklo_ps(b2, b3);
    __m128 t01_hi = _mm_unpackhi_ps(b0, b1), t23_hi = _mm_unpackhi_ps(b2, b3);
    __m128 row0 = _mm_movelh_ps(t01_lo, t23_lo), row1 = _mm_movehl_ps(t23_lo, t01_lo);
    __m128 row2 = _mm_movelh_ps(t01_hi, t23_hi), row3 = _mm_movehl_ps(t23_hi, t01_hi);
    _mm_storeu_ps(results, _mm_add_ps(_mm_add_ps(row0, row1), _mm_add_ps(row2, row3)));
}

/**
 *  @brief Running state for 64-element dot accumulation over e4m3 scalars on Genoa.
 */
typedef struct simsimd_dot_e4m3x64_state_genoa_t {
    __m512 sum;
} simsimd_dot_e4m3x64_state_genoa_t;

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_genoa(simsimd_dot_e4m3x64_state_genoa_t *state) {
    state->sum = _mm512_setzero();
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_genoa(simsimd_dot_e4m3x64_state_genoa_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    __m512 sum = state->sum;
    __m256i a_i8_vec = _mm256_loadu_epi8(a.e4m3s + 0);
    __m256i b_i8_vec = _mm256_loadu_epi8(b.e4m3s + 0);
    __m512i a_bf16 = _simsimd_e4m3_to_bf16_genoa(a_i8_vec);
    __m512i b_bf16 = _simsimd_e4m3_to_bf16_genoa(b_i8_vec);
    sum = _mm512_dpbf16_ps(sum, (__m512bh)(a_bf16), (__m512bh)(b_bf16));
    a_i8_vec = _mm256_loadu_epi8(a.e4m3s + 32);
    b_i8_vec = _mm256_loadu_epi8(b.e4m3s + 32);
    a_bf16 = _simsimd_e4m3_to_bf16_genoa(a_i8_vec);
    b_bf16 = _simsimd_e4m3_to_bf16_genoa(b_i8_vec);
    state->sum = _mm512_dpbf16_ps(sum, (__m512bh)(a_bf16), (__m512bh)(b_bf16));
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_genoa(                                     //
    simsimd_dot_e4m3x64_state_genoa_t const *s0, simsimd_dot_e4m3x64_state_genoa_t const *s1, //
    simsimd_dot_e4m3x64_state_genoa_t const *s2, simsimd_dot_e4m3x64_state_genoa_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction (same logic as Skylake f32x16)
    __m256 a0 = _mm256_add_ps(_mm512_castps512_ps256(s0->sum), _mm512_extractf32x8_ps(s0->sum, 1));
    __m256 a1 = _mm256_add_ps(_mm512_castps512_ps256(s1->sum), _mm512_extractf32x8_ps(s1->sum, 1));
    __m256 a2 = _mm256_add_ps(_mm512_castps512_ps256(s2->sum), _mm512_extractf32x8_ps(s2->sum, 1));
    __m256 a3 = _mm256_add_ps(_mm512_castps512_ps256(s3->sum), _mm512_extractf32x8_ps(s3->sum, 1));
    __m128 b0 = _mm_add_ps(_mm256_castps256_ps128(a0), _mm256_extractf128_ps(a0, 1));
    __m128 b1 = _mm_add_ps(_mm256_castps256_ps128(a1), _mm256_extractf128_ps(a1, 1));
    __m128 b2 = _mm_add_ps(_mm256_castps256_ps128(a2), _mm256_extractf128_ps(a2, 1));
    __m128 b3 = _mm_add_ps(_mm256_castps256_ps128(a3), _mm256_extractf128_ps(a3, 1));
    __m128 t01_lo = _mm_unpacklo_ps(b0, b1), t23_lo = _mm_unpacklo_ps(b2, b3);
    __m128 t01_hi = _mm_unpackhi_ps(b0, b1), t23_hi = _mm_unpackhi_ps(b2, b3);
    __m128 row0 = _mm_movelh_ps(t01_lo, t23_lo), row1 = _mm_movehl_ps(t23_lo, t01_lo);
    __m128 row2 = _mm_movelh_ps(t01_hi, t23_hi), row3 = _mm_movehl_ps(t23_hi, t01_hi);
    _mm_storeu_ps(results, _mm_add_ps(_mm_add_ps(row0, row1), _mm_add_ps(row2, row3)));
}

/**
 *  @brief Running state for 64-element dot accumulation over e5m2 scalars on Genoa.
 */
typedef struct simsimd_dot_e5m2x64_state_genoa_t {
    __m512 sum;
} simsimd_dot_e5m2x64_state_genoa_t;

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_genoa(simsimd_dot_e5m2x64_state_genoa_t *state) {
    state->sum = _mm512_setzero();
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_genoa(simsimd_dot_e5m2x64_state_genoa_t *state, simsimd_b512_vec_t a,
                                                       simsimd_b512_vec_t b) {
    __m512 sum = state->sum;
    __m256i a_i8_vec = _mm256_loadu_epi8(a.e5m2s + 0);
    __m256i b_i8_vec = _mm256_loadu_epi8(b.e5m2s + 0);
    __m512i a_bf16 = _simsimd_e5m2_to_bf16_genoa(a_i8_vec);
    __m512i b_bf16 = _simsimd_e5m2_to_bf16_genoa(b_i8_vec);
    sum = _mm512_dpbf16_ps(sum, (__m512bh)(a_bf16), (__m512bh)(b_bf16));
    a_i8_vec = _mm256_loadu_epi8(a.e5m2s + 32);
    b_i8_vec = _mm256_loadu_epi8(b.e5m2s + 32);
    a_bf16 = _simsimd_e5m2_to_bf16_genoa(a_i8_vec);
    b_bf16 = _simsimd_e5m2_to_bf16_genoa(b_i8_vec);
    state->sum = _mm512_dpbf16_ps(sum, (__m512bh)(a_bf16), (__m512bh)(b_bf16));
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_genoa(                                     //
    simsimd_dot_e5m2x64_state_genoa_t const *s0, simsimd_dot_e5m2x64_state_genoa_t const *s1, //
    simsimd_dot_e5m2x64_state_genoa_t const *s2, simsimd_dot_e5m2x64_state_genoa_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction (same logic as Skylake f32x16)
    __m256 a0 = _mm256_add_ps(_mm512_castps512_ps256(s0->sum), _mm512_extractf32x8_ps(s0->sum, 1));
    __m256 a1 = _mm256_add_ps(_mm512_castps512_ps256(s1->sum), _mm512_extractf32x8_ps(s1->sum, 1));
    __m256 a2 = _mm256_add_ps(_mm512_castps512_ps256(s2->sum), _mm512_extractf32x8_ps(s2->sum, 1));
    __m256 a3 = _mm256_add_ps(_mm512_castps512_ps256(s3->sum), _mm512_extractf32x8_ps(s3->sum, 1));
    __m128 b0 = _mm_add_ps(_mm256_castps256_ps128(a0), _mm256_extractf128_ps(a0, 1));
    __m128 b1 = _mm_add_ps(_mm256_castps256_ps128(a1), _mm256_extractf128_ps(a1, 1));
    __m128 b2 = _mm_add_ps(_mm256_castps256_ps128(a2), _mm256_extractf128_ps(a2, 1));
    __m128 b3 = _mm_add_ps(_mm256_castps256_ps128(a3), _mm256_extractf128_ps(a3, 1));
    __m128 t01_lo = _mm_unpacklo_ps(b0, b1), t23_lo = _mm_unpacklo_ps(b2, b3);
    __m128 t01_hi = _mm_unpackhi_ps(b0, b1), t23_hi = _mm_unpackhi_ps(b2, b3);
    __m128 row0 = _mm_movelh_ps(t01_lo, t23_lo), row1 = _mm_movehl_ps(t23_lo, t01_lo);
    __m128 row2 = _mm_movelh_ps(t01_hi, t23_hi), row3 = _mm_movehl_ps(t23_hi, t01_hi);
    _mm_storeu_ps(results, _mm_add_ps(_mm_add_ps(row0, row1), _mm_add_ps(row2, row3)));
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_GENOA

#if SIMSIMD_TARGET_SAPPHIRE
#pragma GCC push_options
#pragma GCC target("avx2", "avx512f", "avx512vl", "bmi2", "avx512bw", "avx512fp16")
#pragma clang attribute push(__attribute__((target("avx2,avx512f,avx512vl,bmi2,avx512bw,avx512fp16"))), \
                             apply_to = function)

SIMSIMD_PUBLIC void simsimd_dot_f16_sapphire(simsimd_f16_t const *a_scalars, simsimd_f16_t const *b_scalars,
                                             simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m512i a_i16_vec, b_i16_vec;
    __m512h ab_vec = _mm512_setzero_ph();

simsimd_dot_f16_sapphire_cycle:
    if (count_scalars < 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_i16_vec = _mm512_maskz_loadu_epi16(mask, a_scalars);
        b_i16_vec = _mm512_maskz_loadu_epi16(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i16_vec = _mm512_loadu_epi16(a_scalars);
        b_i16_vec = _mm512_loadu_epi16(b_scalars);
        a_scalars += 32, b_scalars += 32, count_scalars -= 32;
    }
    ab_vec = _mm512_fmadd_ph(_mm512_castsi512_ph(a_i16_vec), _mm512_castsi512_ph(b_i16_vec), ab_vec);
    if (count_scalars) goto simsimd_dot_f16_sapphire_cycle;

    *result = _mm512_reduce_add_ph(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_f16c_sapphire(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                              simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512i a_vec, b_vec;
    __m512h ab_real_vec = _mm512_setzero_ph();
    __m512h ab_imag_vec = _mm512_setzero_ph();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set1_epi32(0x80000000);
    __m512i const swap_adjacent_vec = _mm512_set_epi8(                  //
        61, 60, 63, 62, 57, 56, 59, 58, 53, 52, 55, 54, 49, 48, 51, 50, // 4th 128-bit lane
        45, 44, 47, 46, 41, 40, 43, 42, 37, 36, 39, 38, 33, 32, 35, 34, // 3rd 128-bit lane
        29, 28, 31, 30, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, // 2nd 128-bit lane
        13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2            // 1st 128-bit lane
    );

simsimd_dot_f16c_sapphire_cycle:
    if (count_pairs < 16) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_epi16(mask, a_pairs);
        b_vec = _mm512_maskz_loadu_epi16(mask, b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_epi16(a_pairs);
        b_vec = _mm512_loadu_epi16(b_pairs);
        a_pairs += 16, b_pairs += 16, count_pairs -= 16;
    }
    // TODO: Consider using `_mm512_fmaddsub` and `_mm512_fcmadd_pch`
    ab_real_vec = _mm512_fmadd_ph(_mm512_castsi512_ph(_mm512_xor_si512(b_vec, sign_flip_vec)),
                                  _mm512_castsi512_ph(a_vec), ab_real_vec);
    ab_imag_vec = _mm512_fmadd_ph(_mm512_castsi512_ph(_mm512_shuffle_epi8(b_vec, swap_adjacent_vec)),
                                  _mm512_castsi512_ph(a_vec), ab_imag_vec);
    if (count_pairs) goto simsimd_dot_f16c_sapphire_cycle;

    // Reduce horizontal sums:
    // TODO: Optimize this with tree-like reductions
    results[0] = _mm512_reduce_add_ph(ab_real_vec);
    results[1] = _mm512_reduce_add_ph(ab_imag_vec);
}

SIMSIMD_PUBLIC void simsimd_vdot_f16c_sapphire(simsimd_f16c_t const *a_pairs, simsimd_f16c_t const *b_pairs,
                                               simsimd_size_t count_pairs, simsimd_distance_t *results) {
    __m512i a_vec, b_vec;
    __m512h ab_real_vec = _mm512_setzero_ph();
    __m512h ab_imag_vec = _mm512_setzero_ph();

    // We take into account, that FMS is the same as FMA with a negative multiplier.
    // To multiply a floating-point value by -1, we can use the `XOR` instruction to flip the sign bit.
    // This way we can avoid the shuffling and the need for separate real and imaginary parts.
    // For the imaginary part of the product, we would need to swap the real and imaginary parts of
    // one of the vectors.
    __m512i const sign_flip_vec = _mm512_set1_epi32(0x80000000);
    __m512i const swap_adjacent_vec = _mm512_set_epi8(                  //
        61, 60, 63, 62, 57, 56, 59, 58, 53, 52, 55, 54, 49, 48, 51, 50, // 4th 128-bit lane
        45, 44, 47, 46, 41, 40, 43, 42, 37, 36, 39, 38, 33, 32, 35, 34, // 3rd 128-bit lane
        29, 28, 31, 30, 25, 24, 27, 26, 21, 20, 23, 22, 17, 16, 19, 18, // 2nd 128-bit lane
        13, 12, 15, 14, 9, 8, 11, 10, 5, 4, 7, 6, 1, 0, 3, 2            // 1st 128-bit lane
    );

simsimd_dot_f16c_sapphire_cycle:
    if (count_pairs < 16) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_pairs * 2);
        a_vec = _mm512_maskz_loadu_epi16(mask, a_pairs);
        b_vec = _mm512_maskz_loadu_epi16(mask, b_pairs);
        count_pairs = 0;
    }
    else {
        a_vec = _mm512_loadu_epi16(a_pairs);
        b_vec = _mm512_loadu_epi16(b_pairs);
        a_pairs += 16, b_pairs += 16, count_pairs -= 16;
    }
    // TODO: Consider using `_mm512_fmaddsub` and `_mm512_fcmadd_pch`
    ab_real_vec = _mm512_fmadd_ph(_mm512_castsi512_ph(a_vec), _mm512_castsi512_ph(b_vec), ab_real_vec);
    a_vec = _mm512_xor_si512(a_vec, sign_flip_vec);
    b_vec = _mm512_shuffle_epi8(b_vec, swap_adjacent_vec);
    ab_imag_vec = _mm512_fmadd_ph(_mm512_castsi512_ph(a_vec), _mm512_castsi512_ph(b_vec), ab_imag_vec);
    if (count_pairs) goto simsimd_dot_f16c_sapphire_cycle;

    // Reduce horizontal sums:
    results[0] = _mm512_reduce_add_ph(ab_real_vec);
    results[1] = _mm512_reduce_add_ph(ab_imag_vec);
}

/*  Convert 32x E4M3 values to 32x F16 values.
 *  Uses optimized path similar to E5M2 but with bias adjustment.
 *  Denormals (exp=0) are flushed to zero (DAZ behavior).
 *
 *  E4M3 format: S EEEE  MMM        (bias=7)
 *  F16 format:  S EEEEE MMMMMMMMMM (bias=15)
 *
 *  The key difference from E5M2F16 (which is trivial) is the bias adjustment:
 *  E5M2 and F16 share bias=15, so just shift. E4M3 needs +8 to exponent.
 */
SIMSIMD_INTERNAL __m512i _simsimd_e4m3_to_f16_sapphire(__m256i fp8) {
    __m512i v = _mm512_cvtepu8_epi16(fp8);
    // Sign: bit 7  bit 15
    __m512i sign = _mm512_and_si512(_mm512_slli_epi16(v, 8), _mm512_set1_epi16((short)0x8000));
    // Exp+mant (7 bits) shifted left 7, then add bias adjustment (8<<10 = 0x2000)
    __m512i low7 = _mm512_and_si512(v, _mm512_set1_epi16(0x7F));
    __m512i exp_mant = _mm512_add_epi16(_mm512_slli_epi16(low7, 7), _mm512_set1_epi16(0x2000));
    // DAZ: use TEST to check if exp bits (bits 6-3) are nonzero - single instruction!
    __mmask32 has_exp = _mm512_test_epi16_mask(v, _mm512_set1_epi16(0x78));
    __m512i masked_exp_mant = _mm512_maskz_mov_epi16(has_exp, exp_mant);
    return _mm512_or_si512(sign, masked_exp_mant);
}

/*  Convert 32x E5M2 values to 32x F16 values.
 *  This is extremely fast because E5M2 and F16 have the same exponent bias (15).
 *  Simply zero-extend to 16-bit and shift left by 8.
 *
 *  E5M2 format: S EEEEE MM         (bias=15)
 *  F16 format:  S EEEEE MMMMMMMMMM (bias=15)
 */
SIMSIMD_INTERNAL __m512i _simsimd_e5m2_to_f16_sapphire(__m256i fp8) {
    __m512i v = _mm512_cvtepu8_epi16(fp8);
    return _mm512_slli_epi16(v, 8);
}

SIMSIMD_PUBLIC void simsimd_dot_e4m3_sapphire(simsimd_e4m3_t const *a_scalars, simsimd_e4m3_t const *b_scalars,
                                              simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m256i a_i8_vec, b_i8_vec;
    __m512h ab_vec = _mm512_setzero_ph();

simsimd_dot_e4m3_sapphire_cycle:
    if (count_scalars < 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_i8_vec = _mm256_maskz_loadu_epi8(mask, a_scalars);
        b_i8_vec = _mm256_maskz_loadu_epi8(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i8_vec = _mm256_loadu_epi8(a_scalars);
        b_i8_vec = _mm256_loadu_epi8(b_scalars);
        a_scalars += 32, b_scalars += 32, count_scalars -= 32;
    }
    // Convert E4M3 to F16 and compute dot product
    __m512i a_f16 = _simsimd_e4m3_to_f16_sapphire(a_i8_vec);
    __m512i b_f16 = _simsimd_e4m3_to_f16_sapphire(b_i8_vec);
    ab_vec = _mm512_fmadd_ph(_mm512_castsi512_ph(a_f16), _mm512_castsi512_ph(b_f16), ab_vec);
    if (count_scalars) goto simsimd_dot_e4m3_sapphire_cycle;

    *result = _mm512_reduce_add_ph(ab_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_e5m2_sapphire(simsimd_e5m2_t const *a_scalars, simsimd_e5m2_t const *b_scalars,
                                              simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m256i a_i8_vec, b_i8_vec;
    __m512h ab_vec = _mm512_setzero_ph();

simsimd_dot_e5m2_sapphire_cycle:
    if (count_scalars < 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_i8_vec = _mm256_maskz_loadu_epi8(mask, a_scalars);
        b_i8_vec = _mm256_maskz_loadu_epi8(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_i8_vec = _mm256_loadu_epi8(a_scalars);
        b_i8_vec = _mm256_loadu_epi8(b_scalars);
        a_scalars += 32, b_scalars += 32, count_scalars -= 32;
    }
    // Convert E5M2 to F16 and compute dot product
    // Note: E5M2 to F16 is extremely fast due to same exponent bias
    __m512i a_f16 = _simsimd_e5m2_to_f16_sapphire(a_i8_vec);
    __m512i b_f16 = _simsimd_e5m2_to_f16_sapphire(b_i8_vec);
    ab_vec = _mm512_fmadd_ph(_mm512_castsi512_ph(a_f16), _mm512_castsi512_ph(b_f16), ab_vec);
    if (count_scalars) goto simsimd_dot_e5m2_sapphire_cycle;

    *result = _mm512_reduce_add_ph(ab_vec);
}

/**
 *  @brief Running state for 32-element dot accumulation over f16 scalars on Sapphire.
 */
typedef struct simsimd_dot_f16x32_state_sapphire_t {
    __m512h sum;
} simsimd_dot_f16x32_state_sapphire_t;

SIMSIMD_INTERNAL void simsimd_dot_f16x32_init_sapphire(simsimd_dot_f16x32_state_sapphire_t *state) {
    state->sum = _mm512_setzero_ph();
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_update_sapphire(simsimd_dot_f16x32_state_sapphire_t *state,
                                                         simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m512h sum = state->sum;
    __m512i a_i16_vec = _mm512_loadu_epi16(a.f16s);
    __m512i b_i16_vec = _mm512_loadu_epi16(b.f16s);
    state->sum = _mm512_fmadd_ph(_mm512_castsi512_ph(a_i16_vec), _mm512_castsi512_ph(b_i16_vec), sum);
}

SIMSIMD_INTERNAL void simsimd_dot_f16x32_finalize_sapphire(                                       //
    simsimd_dot_f16x32_state_sapphire_t const *s0, simsimd_dot_f16x32_state_sapphire_t const *s1, //
    simsimd_dot_f16x32_state_sapphire_t const *s2, simsimd_dot_f16x32_state_sapphire_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for f16 (32 elements  1 scalar each)
    // Step 1: 3216 for all 4 states (extract high 256-bit half and add to low half)
    // Use integer extract and cast since there's no direct _mm512_extractf16x16_ph
    __m256h a0 = _mm256_add_ph(_mm512_castph512_ph256(s0->sum),
                               _mm256_castsi256_ph(_mm512_extracti32x8_epi32(_mm512_castph_si512(s0->sum), 1)));
    __m256h a1 = _mm256_add_ph(_mm512_castph512_ph256(s1->sum),
                               _mm256_castsi256_ph(_mm512_extracti32x8_epi32(_mm512_castph_si512(s1->sum), 1)));
    __m256h a2 = _mm256_add_ph(_mm512_castph512_ph256(s2->sum),
                               _mm256_castsi256_ph(_mm512_extracti32x8_epi32(_mm512_castph_si512(s2->sum), 1)));
    __m256h a3 = _mm256_add_ph(_mm512_castph512_ph256(s3->sum),
                               _mm256_castsi256_ph(_mm512_extracti32x8_epi32(_mm512_castph_si512(s3->sum), 1)));
    // Step 2: 168 for all 4 states (extract high 128-bit half and add to low half)
    __m128h b0 = _mm_add_ph(_mm256_castph256_ph128(a0),
                            _mm_castsi128_ph(_mm256_extracti128_si256(_mm256_castph_si256(a0), 1)));
    __m128h b1 = _mm_add_ph(_mm256_castph256_ph128(a1),
                            _mm_castsi128_ph(_mm256_extracti128_si256(_mm256_castph_si256(a1), 1)));
    __m128h b2 = _mm_add_ph(_mm256_castph256_ph128(a2),
                            _mm_castsi128_ph(_mm256_extracti128_si256(_mm256_castph_si256(a2), 1)));
    __m128h b3 = _mm_add_ph(_mm256_castph256_ph128(a3),
                            _mm_castsi128_ph(_mm256_extracti128_si256(_mm256_castph_si256(a3), 1)));
    // Step 3: 84 for all 4 states (shift right by 8 bytes = 4 f16 elements, then add)
    __m128h c0 = _mm_add_ph(b0, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(b0), 8)));
    __m128h c1 = _mm_add_ph(b1, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(b1), 8)));
    __m128h c2 = _mm_add_ph(b2, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(b2), 8)));
    __m128h c3 = _mm_add_ph(b3, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(b3), 8)));
    // Step 4: 42 for all 4 states (shift right by 4 bytes = 2 f16 elements, then add)
    __m128h d0 = _mm_add_ph(c0, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(c0), 4)));
    __m128h d1 = _mm_add_ph(c1, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(c1), 4)));
    __m128h d2 = _mm_add_ph(c2, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(c2), 4)));
    __m128h d3 = _mm_add_ph(c3, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(c3), 4)));
    // Step 5: 21 for all 4 states (shift right by 2 bytes = 1 f16 element, then add)
    __m128h e0 = _mm_add_ph(d0, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(d0), 2)));
    __m128h e1 = _mm_add_ph(d1, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(d1), 2)));
    __m128h e2 = _mm_add_ph(d2, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(d2), 2)));
    __m128h e3 = _mm_add_ph(d3, _mm_castsi128_ph(_mm_bsrli_si128(_mm_castph_si128(d3), 2)));
    // Extract first f16 element and convert to f32
    results[0] = _mm_cvtss_f32(_mm_cvtsh_ss(_mm_setzero_ps(), e0));
    results[1] = _mm_cvtss_f32(_mm_cvtsh_ss(_mm_setzero_ps(), e1));
    results[2] = _mm_cvtss_f32(_mm_cvtsh_ss(_mm_setzero_ps(), e2));
    results[3] = _mm_cvtss_f32(_mm_cvtsh_ss(_mm_setzero_ps(), e3));
}

/**
 *  @brief Running state for 64-element dot accumulation over e4m3 scalars on Sapphire.
 */
typedef struct simsimd_dot_e4m3x64_state_sapphire_t {
    __m512h sum;
} simsimd_dot_e4m3x64_state_sapphire_t;

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_init_sapphire(simsimd_dot_e4m3x64_state_sapphire_t *state) {
    state->sum = _mm512_setzero_ph();
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_update_sapphire(simsimd_dot_e4m3x64_state_sapphire_t *state,
                                                          simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m512h sum = state->sum;
    __m256i a_i8_vec = _mm256_loadu_epi8(a.e4m3s + 0);
    __m256i b_i8_vec = _mm256_loadu_epi8(b.e4m3s + 0);
    __m512i a_f16 = _simsimd_e4m3_to_f16_sapphire(a_i8_vec);
    __m512i b_f16 = _simsimd_e4m3_to_f16_sapphire(b_i8_vec);
    sum = _mm512_fmadd_ph(_mm512_castsi512_ph(a_f16), _mm512_castsi512_ph(b_f16), sum);
    a_i8_vec = _mm256_loadu_epi8(a.e4m3s + 32);
    b_i8_vec = _mm256_loadu_epi8(b.e4m3s + 32);
    a_f16 = _simsimd_e4m3_to_f16_sapphire(a_i8_vec);
    b_f16 = _simsimd_e4m3_to_f16_sapphire(b_i8_vec);
    state->sum = _mm512_fmadd_ph(_mm512_castsi512_ph(a_f16), _mm512_castsi512_ph(b_f16), sum);
}

SIMSIMD_INTERNAL void simsimd_dot_e4m3x64_finalize_sapphire(                                        //
    simsimd_dot_e4m3x64_state_sapphire_t const *s0, simsimd_dot_e4m3x64_state_sapphire_t const *s1, //
    simsimd_dot_e4m3x64_state_sapphire_t const *s2, simsimd_dot_e4m3x64_state_sapphire_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f16x32 (both contain just __m512h sum)
    simsimd_dot_f16x32_finalize_sapphire(                                                                 //
        (simsimd_dot_f16x32_state_sapphire_t const *)s0, (simsimd_dot_f16x32_state_sapphire_t const *)s1, //
        (simsimd_dot_f16x32_state_sapphire_t const *)s2, (simsimd_dot_f16x32_state_sapphire_t const *)s3, results);
}

/**
 *  @brief Running state for 64-element dot accumulation over e5m2 scalars on Sapphire.
 */
typedef struct simsimd_dot_e5m2x64_state_sapphire_t {
    __m512h sum;
} simsimd_dot_e5m2x64_state_sapphire_t;

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_init_sapphire(simsimd_dot_e5m2x64_state_sapphire_t *state) {
    state->sum = _mm512_setzero_ph();
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_update_sapphire(simsimd_dot_e5m2x64_state_sapphire_t *state,
                                                          simsimd_b512_vec_t a, simsimd_b512_vec_t b) {
    __m512h sum = state->sum;
    __m256i a_i8_vec = _mm256_loadu_epi8(a.e5m2s + 0);
    __m256i b_i8_vec = _mm256_loadu_epi8(b.e5m2s + 0);
    __m512i a_f16 = _simsimd_e5m2_to_f16_sapphire(a_i8_vec);
    __m512i b_f16 = _simsimd_e5m2_to_f16_sapphire(b_i8_vec);
    sum = _mm512_fmadd_ph(_mm512_castsi512_ph(a_f16), _mm512_castsi512_ph(b_f16), sum);
    a_i8_vec = _mm256_loadu_epi8(a.e5m2s + 32);
    b_i8_vec = _mm256_loadu_epi8(b.e5m2s + 32);
    a_f16 = _simsimd_e5m2_to_f16_sapphire(a_i8_vec);
    b_f16 = _simsimd_e5m2_to_f16_sapphire(b_i8_vec);
    state->sum = _mm512_fmadd_ph(_mm512_castsi512_ph(a_f16), _mm512_castsi512_ph(b_f16), sum);
}

SIMSIMD_INTERNAL void simsimd_dot_e5m2x64_finalize_sapphire(                                        //
    simsimd_dot_e5m2x64_state_sapphire_t const *s0, simsimd_dot_e5m2x64_state_sapphire_t const *s1, //
    simsimd_dot_e5m2x64_state_sapphire_t const *s2, simsimd_dot_e5m2x64_state_sapphire_t const *s3, //
    simsimd_f32_t *results) {
    // State is layout-compatible with f16x32 (both contain just __m512h sum)
    simsimd_dot_f16x32_finalize_sapphire(                                                                 //
        (simsimd_dot_f16x32_state_sapphire_t const *)s0, (simsimd_dot_f16x32_state_sapphire_t const *)s1, //
        (simsimd_dot_f16x32_state_sapphire_t const *)s2, (simsimd_dot_f16x32_state_sapphire_t const *)s3, results);
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_SAPPHIRE

#if SIMSIMD_TARGET_ICE
#pragma GCC push_options
#pragma GCC target("avx2", "avx512f", "avx512vl", "bmi2", "avx512bw", "avx512vnni")
#pragma clang attribute push(__attribute__((target("avx2,avx512f,avx512vl,bmi2,avx512bw,avx512vnni"))), \
                             apply_to = function)

SIMSIMD_PUBLIC void simsimd_dot_i8_ice(simsimd_i8_t const *a_scalars, simsimd_i8_t const *b_scalars,
                                       simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m512i a_i16_vec, b_i16_vec;
    __m512i ab_i32_vec = _mm512_setzero_si512();

simsimd_dot_i8_ice_cycle:
    if (count_scalars < 32) {
        __mmask32 mask = (__mmask32)_bzhi_u32(0xFFFFFFFF, count_scalars);
        a_i16_vec = _mm512_cvtepi8_epi16(_mm256_maskz_loadu_epi8(mask, a_scalars));
        b_i16_vec = _mm512_cvtepi8_epi16(_mm256_maskz_loadu_epi8(mask, b_scalars));
        count_scalars = 0;
    }
    else {
        a_i16_vec = _mm512_cvtepi8_epi16(_mm256_lddqu_si256((__m256i const *)a_scalars));
        b_i16_vec = _mm512_cvtepi8_epi16(_mm256_lddqu_si256((__m256i const *)b_scalars));
        a_scalars += 32, b_scalars += 32, count_scalars -= 32;
    }
    // Unfortunately we can't use the `_mm512_dpbusd_epi32` intrinsics here either,
    // as it's asymmetric with respect to the sign of the input arguments:
    //      Signed(ZeroExtend16(a_scalars.byte[4*j]) * SignExtend16(b_scalars.byte[4*j]))
    // So we have to use the `_mm512_dpwssd_epi32` intrinsics instead, upcasting
    // to 16-bit beforehand.
    ab_i32_vec = _mm512_dpwssd_epi32(ab_i32_vec, a_i16_vec, b_i16_vec);
    if (count_scalars) goto simsimd_dot_i8_ice_cycle;

    *result = _mm512_reduce_add_epi32(ab_i32_vec);
}

SIMSIMD_PUBLIC void simsimd_dot_u8_ice(simsimd_u8_t const *a_scalars, simsimd_u8_t const *b_scalars,
                                       simsimd_size_t count_scalars, simsimd_distance_t *result) {
    __m512i a_u8_vec, b_u8_vec;
    __m512i a_i16_low_vec, a_i16_high_vec, b_i16_low_vec, b_i16_high_vec;
    __m512i ab_i32_low_vec = _mm512_setzero_si512();
    __m512i ab_i32_high_vec = _mm512_setzero_si512();
    __m512i const zeros_vec = _mm512_setzero_si512();

simsimd_dot_u8_ice_cycle:
    if (count_scalars < 64) {
        __mmask64 mask = (__mmask64)_bzhi_u64(0xFFFFFFFFFFFFFFFF, count_scalars);
        a_u8_vec = _mm512_maskz_loadu_epi8(mask, a_scalars);
        b_u8_vec = _mm512_maskz_loadu_epi8(mask, b_scalars);
        count_scalars = 0;
    }
    else {
        a_u8_vec = _mm512_loadu_si512(a_scalars);
        b_u8_vec = _mm512_loadu_si512(b_scalars);
        a_scalars += 64, b_scalars += 64, count_scalars -= 64;
    }

    // Upcast `uint8` to `int16`. Unlike the signed version, we can use the unpacking
    // instructions instead of extracts, as they are much faster and more efficient.
    a_i16_low_vec = _mm512_unpacklo_epi8(a_u8_vec, zeros_vec);
    a_i16_high_vec = _mm512_unpackhi_epi8(a_u8_vec, zeros_vec);
    b_i16_low_vec = _mm512_unpacklo_epi8(b_u8_vec, zeros_vec);
    b_i16_high_vec = _mm512_unpackhi_epi8(b_u8_vec, zeros_vec);
    // Unfortunately we can't use the `_mm512_dpbusd_epi32` intrinsics here either,
    // as it's asymmetric with respect to the sign of the input arguments:
    //      Signed(ZeroExtend16(a.byte[4*j]) * SignExtend16(b.byte[4*j]))
    // So we have to use the `_mm512_dpwssd_epi32` intrinsics instead, upcasting
    // to 16-bit beforehand.
    ab_i32_low_vec = _mm512_dpwssd_epi32(ab_i32_low_vec, a_i16_low_vec, b_i16_low_vec);
    ab_i32_high_vec = _mm512_dpwssd_epi32(ab_i32_high_vec, a_i16_high_vec, b_i16_high_vec);
    if (count_scalars) goto simsimd_dot_u8_ice_cycle;

    *result = _mm512_reduce_add_epi32(_mm512_add_epi32(ab_i32_low_vec, ab_i32_high_vec));
}

/**
 *  @brief Running state for 64-element dot accumulation over i8 scalars on Ice Lake.
 */
typedef struct simsimd_dot_i8x64_state_ice_t {
    __m512i sum;
} simsimd_dot_i8x64_state_ice_t;

SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_ice(simsimd_dot_i8x64_state_ice_t *state) {
    state->sum = _mm512_setzero_si512();
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_ice(simsimd_dot_i8x64_state_ice_t *state, simsimd_b512_vec_t a,
                                                   simsimd_b512_vec_t b) {
    __m512i sum = state->sum;
    __m512i a_i16_vec = _mm512_cvtepi8_epi16(_mm256_lddqu_si256((__m256i const *)(a.i8s + 0)));
    __m512i b_i16_vec = _mm512_cvtepi8_epi16(_mm256_lddqu_si256((__m256i const *)(b.i8s + 0)));
    sum = _mm512_dpwssd_epi32(sum, a_i16_vec, b_i16_vec);
    a_i16_vec = _mm512_cvtepi8_epi16(_mm256_lddqu_si256((__m256i const *)(a.i8s + 32)));
    b_i16_vec = _mm512_cvtepi8_epi16(_mm256_lddqu_si256((__m256i const *)(b.i8s + 32)));
    state->sum = _mm512_dpwssd_epi32(sum, a_i16_vec, b_i16_vec);
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_ice(                                 //
    simsimd_dot_i8x64_state_ice_t const *s0, simsimd_dot_i8x64_state_ice_t const *s1, //
    simsimd_dot_i8x64_state_ice_t const *s2, simsimd_dot_i8x64_state_ice_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for i32
    // Step 1: 168 for all 4 states (extract high 256-bit half and add to low half)
    __m256i a0 = _mm256_add_epi32(_mm512_castsi512_si256(s0->sum), _mm512_extracti32x8_epi32(s0->sum, 1));
    __m256i a1 = _mm256_add_epi32(_mm512_castsi512_si256(s1->sum), _mm512_extracti32x8_epi32(s1->sum, 1));
    __m256i a2 = _mm256_add_epi32(_mm512_castsi512_si256(s2->sum), _mm512_extracti32x8_epi32(s2->sum, 1));
    __m256i a3 = _mm256_add_epi32(_mm512_castsi512_si256(s3->sum), _mm512_extracti32x8_epi32(s3->sum, 1));
    // Step 2: 84 for all 4 states (extract high 128-bit half and add to low half)
    __m128i b0 = _mm_add_epi32(_mm256_castsi256_si128(a0), _mm256_extracti128_si256(a0, 1));
    __m128i b1 = _mm_add_epi32(_mm256_castsi256_si128(a1), _mm256_extracti128_si256(a1, 1));
    __m128i b2 = _mm_add_epi32(_mm256_castsi256_si128(a2), _mm256_extracti128_si256(a2, 1));
    __m128i b3 = _mm_add_epi32(_mm256_castsi256_si128(a3), _mm256_extracti128_si256(a3, 1));
    // Step 3: Transpose 44 matrix of partial sums using integer shuffles
    __m128i t01_lo = _mm_unpacklo_epi32(b0, b1);       // s0[0], s1[0], s0[1], s1[1]
    __m128i t23_lo = _mm_unpacklo_epi32(b2, b3);       // s2[0], s3[0], s2[1], s3[1]
    __m128i t01_hi = _mm_unpackhi_epi32(b0, b1);       // s0[2], s1[2], s0[3], s1[3]
    __m128i t23_hi = _mm_unpackhi_epi32(b2, b3);       // s2[2], s3[2], s2[3], s3[3]
    __m128i row0 = _mm_unpacklo_epi64(t01_lo, t23_lo); // s0[0], s1[0], s2[0], s3[0]
    __m128i row1 = _mm_unpackhi_epi64(t01_lo, t23_lo); // s0[1], s1[1], s2[1], s3[1]
    __m128i row2 = _mm_unpacklo_epi64(t01_hi, t23_hi); // s0[2], s1[2], s2[2], s3[2]
    __m128i row3 = _mm_unpackhi_epi64(t01_hi, t23_hi); // s0[3], s1[3], s2[3], s3[3]
    // Step 4: Vertical sum - each lane becomes the final i32 result for one state
    __m128i sum_i32 = _mm_add_epi32(_mm_add_epi32(row0, row1), _mm_add_epi32(row2, row3));
    // Convert to f32 and store
    _mm_storeu_ps(results, _mm_cvtepi32_ps(sum_i32));
}

/**
 *  @brief Running state for 64-element dot accumulation over u8 scalars on Ice Lake.
 */
typedef struct simsimd_dot_u8x64_state_ice_t {
    __m512i sum_low;
    __m512i sum_high;
} simsimd_dot_u8x64_state_ice_t;

SIMSIMD_INTERNAL void simsimd_dot_u8x64_init_ice(simsimd_dot_u8x64_state_ice_t *state) {
    state->sum_low = _mm512_setzero_si512();
    state->sum_high = _mm512_setzero_si512();
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_update_ice(simsimd_dot_u8x64_state_ice_t *state, simsimd_b512_vec_t a,
                                                   simsimd_b512_vec_t b) {
    __m512i sum_low = state->sum_low;
    __m512i sum_high = state->sum_high;
    __m512i const zeros_vec = _mm512_setzero_si512();

    __m512i a_u8_vec = _mm512_loadu_si512(a.u8s);
    __m512i b_u8_vec = _mm512_loadu_si512(b.u8s);
    __m512i a_i16_low = _mm512_unpacklo_epi8(a_u8_vec, zeros_vec);
    __m512i a_i16_high = _mm512_unpackhi_epi8(a_u8_vec, zeros_vec);
    __m512i b_i16_low = _mm512_unpacklo_epi8(b_u8_vec, zeros_vec);
    __m512i b_i16_high = _mm512_unpackhi_epi8(b_u8_vec, zeros_vec);
    sum_low = _mm512_dpwssd_epi32(sum_low, a_i16_low, b_i16_low);
    sum_high = _mm512_dpwssd_epi32(sum_high, a_i16_high, b_i16_high);

    state->sum_low = sum_low;
    state->sum_high = sum_high;
}

SIMSIMD_INTERNAL void simsimd_dot_u8x64_finalize_ice(                                 //
    simsimd_dot_u8x64_state_ice_t const *s0, simsimd_dot_u8x64_state_ice_t const *s1, //
    simsimd_dot_u8x64_state_ice_t const *s2, simsimd_dot_u8x64_state_ice_t const *s3, //
    simsimd_f32_t *results) {
    // First, combine the low and high accumulators for each state
    __m512i sum0 = _mm512_add_epi32(s0->sum_low, s0->sum_high);
    __m512i sum1 = _mm512_add_epi32(s1->sum_low, s1->sum_high);
    __m512i sum2 = _mm512_add_epi32(s2->sum_low, s2->sum_high);
    __m512i sum3 = _mm512_add_epi32(s3->sum_low, s3->sum_high);
    // ILP-optimized 4-way horizontal reduction for i32
    // Step 1: 168 for all 4 states
    __m256i a0 = _mm256_add_epi32(_mm512_castsi512_si256(sum0), _mm512_extracti32x8_epi32(sum0, 1));
    __m256i a1 = _mm256_add_epi32(_mm512_castsi512_si256(sum1), _mm512_extracti32x8_epi32(sum1, 1));
    __m256i a2 = _mm256_add_epi32(_mm512_castsi512_si256(sum2), _mm512_extracti32x8_epi32(sum2, 1));
    __m256i a3 = _mm256_add_epi32(_mm512_castsi512_si256(sum3), _mm512_extracti32x8_epi32(sum3, 1));
    // Step 2: 84 for all 4 states
    __m128i b0 = _mm_add_epi32(_mm256_castsi256_si128(a0), _mm256_extracti128_si256(a0, 1));
    __m128i b1 = _mm_add_epi32(_mm256_castsi256_si128(a1), _mm256_extracti128_si256(a1, 1));
    __m128i b2 = _mm_add_epi32(_mm256_castsi256_si128(a2), _mm256_extracti128_si256(a2, 1));
    __m128i b3 = _mm_add_epi32(_mm256_castsi256_si128(a3), _mm256_extracti128_si256(a3, 1));
    // Step 3: Transpose 44 matrix
    __m128i t01_lo = _mm_unpacklo_epi32(b0, b1);
    __m128i t23_lo = _mm_unpacklo_epi32(b2, b3);
    __m128i t01_hi = _mm_unpackhi_epi32(b0, b1);
    __m128i t23_hi = _mm_unpackhi_epi32(b2, b3);
    __m128i row0 = _mm_unpacklo_epi64(t01_lo, t23_lo);
    __m128i row1 = _mm_unpackhi_epi64(t01_lo, t23_lo);
    __m128i row2 = _mm_unpacklo_epi64(t01_hi, t23_hi);
    __m128i row3 = _mm_unpackhi_epi64(t01_hi, t23_hi);
    // Step 4: Vertical sum and convert to f32
    __m128i sum_i32 = _mm_add_epi32(_mm_add_epi32(row0, row1), _mm_add_epi32(row2, row3));
    _mm_storeu_ps(results, _mm_cvtepi32_ps(sum_i32));
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_ICE

#if SIMSIMD_TARGET_SIERRA
#pragma GCC push_options
#pragma GCC target("avx2", "bmi2", "avx2vnni")
#pragma clang attribute push(__attribute__((target("avx2,bmi2,avx2vnni"))), apply_to = function)

SIMSIMD_PUBLIC void simsimd_dot_i8_sierra(simsimd_i8_t const *a_scalars, simsimd_i8_t const *b_scalars,
                                          simsimd_size_t count_scalars, simsimd_distance_t *result) {

    __m256i ab_i32_vec = _mm256_setzero_si256();
    simsimd_size_t idx_scalars = 0;
    for (; idx_scalars + 32 <= count_scalars; idx_scalars += 32) {
        __m256i a_i8_vec = _mm256_lddqu_si256((__m256i const *)(a_scalars + idx_scalars));
        __m256i b_i8_vec = _mm256_lddqu_si256((__m256i const *)(b_scalars + idx_scalars));
        ab_i32_vec = _mm256_dpbssds_epi32(ab_i32_vec, a_i8_vec, b_i8_vec);
    }

    // Further reduce to a single sum for each vector
    int ab = _simsimd_reduce_i32x8_haswell(ab_i32_vec);

    // Take care of the tail:
    for (; idx_scalars < count_scalars; ++idx_scalars) ab += (int)(a_scalars[idx_scalars]) * b_scalars[idx_scalars];
    *result = ab;
}

/**
 *  @brief Running state for 64-element dot accumulation over i8 scalars on Sierra.
 */
typedef struct simsimd_dot_i8x64_state_sierra_t {
    __m256i sum;
} simsimd_dot_i8x64_state_sierra_t;

SIMSIMD_INTERNAL void simsimd_dot_i8x64_init_sierra(simsimd_dot_i8x64_state_sierra_t *state) {
    state->sum = _mm256_setzero_si256();
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_update_sierra(simsimd_dot_i8x64_state_sierra_t *state, simsimd_b512_vec_t a,
                                                      simsimd_b512_vec_t b) {
    __m256i sum = state->sum;
    sum = _mm256_dpbssds_epi32(sum, _mm256_lddqu_si256((__m256i const *)(a.i8s + 0)),
                               _mm256_lddqu_si256((__m256i const *)(b.i8s + 0)));
    state->sum = _mm256_dpbssds_epi32(sum, _mm256_lddqu_si256((__m256i const *)(a.i8s + 32)),
                                      _mm256_lddqu_si256((__m256i const *)(b.i8s + 32)));
}

SIMSIMD_INTERNAL void simsimd_dot_i8x64_finalize_sierra(                                    //
    simsimd_dot_i8x64_state_sierra_t const *s0, simsimd_dot_i8x64_state_sierra_t const *s1, //
    simsimd_dot_i8x64_state_sierra_t const *s2, simsimd_dot_i8x64_state_sierra_t const *s3, //
    simsimd_f32_t *results) {
    // ILP-optimized 4-way horizontal reduction for i32 in AVX2 (8 elements  1 scalar each)
    // Step 1: 84 for all 4 states (extract high 128-bit half and add to low half)
    __m128i a0 = _mm_add_epi32(_mm256_castsi256_si128(s0->sum), _mm256_extracti128_si256(s0->sum, 1));
    __m128i a1 = _mm_add_epi32(_mm256_castsi256_si128(s1->sum), _mm256_extracti128_si256(s1->sum, 1));
    __m128i a2 = _mm_add_epi32(_mm256_castsi256_si128(s2->sum), _mm256_extracti128_si256(s2->sum, 1));
    __m128i a3 = _mm_add_epi32(_mm256_castsi256_si128(s3->sum), _mm256_extracti128_si256(s3->sum, 1));
    // Step 2: Transpose 44 matrix of partial sums using integer shuffles
    __m128i t01_lo = _mm_unpacklo_epi32(a0, a1);       // s0[0], s1[0], s0[1], s1[1]
    __m128i t23_lo = _mm_unpacklo_epi32(a2, a3);       // s2[0], s3[0], s2[1], s3[1]
    __m128i t01_hi = _mm_unpackhi_epi32(a0, a1);       // s0[2], s1[2], s0[3], s1[3]
    __m128i t23_hi = _mm_unpackhi_epi32(a2, a3);       // s2[2], s3[2], s2[3], s3[3]
    __m128i row0 = _mm_unpacklo_epi64(t01_lo, t23_lo); // s0[0], s1[0], s2[0], s3[0]
    __m128i row1 = _mm_unpackhi_epi64(t01_lo, t23_lo); // s0[1], s1[1], s2[1], s3[1]
    __m128i row2 = _mm_unpacklo_epi64(t01_hi, t23_hi); // s0[2], s1[2], s2[2], s3[2]
    __m128i row3 = _mm_unpackhi_epi64(t01_hi, t23_hi); // s0[3], s1[3], s2[3], s3[3]
    // Step 3: Vertical sum - each lane becomes the final i32 result for one state
    __m128i sum_i32 = _mm_add_epi32(_mm_add_epi32(row0, row1), _mm_add_epi32(row2, row3));
    // Convert to f32 and store
    _mm_storeu_ps(results, _mm_cvtepi32_ps(sum_i32));
}

#pragma clang attribute pop
#pragma GCC pop_options
#endif // SIMSIMD_TARGET_SIERRA
#endif // _SIMSIMD_TARGET_X86

#if !SIMSIMD_DYNAMIC_DISPATCH

SIMSIMD_PUBLIC void simsimd_dot_i8(simsimd_i8_t const *a, simsimd_i8_t const *b, simsimd_size_t n,
                                   simsimd_distance_t *d) {
#if SIMSIMD_TARGET_NEON_I8
    simsimd_dot_i8_neon(a, b, n, d);
#elif SIMSIMD_TARGET_ICE
    simsimd_dot_i8_ice(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_i8_haswell(a, b, n, d);
#else
    simsimd_dot_i8_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_u8(simsimd_u8_t const *a, simsimd_u8_t const *b, simsimd_size_t n,
                                   simsimd_distance_t *d) {
#if SIMSIMD_TARGET_NEON_I8
    simsimd_dot_u8_neon(a, b, n, d);
#elif SIMSIMD_TARGET_ICE
    simsimd_dot_u8_ice(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_u8_haswell(a, b, n, d);
#else
    simsimd_dot_u8_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_f16(simsimd_f16_t const *a, simsimd_f16_t const *b, simsimd_size_t n,
                                    simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE_F16
    simsimd_dot_f16_sve(a, b, n, d);
#elif SIMSIMD_TARGET_NEON_F16
    simsimd_dot_f16_neon(a, b, n, d);
#elif SIMSIMD_TARGET_SAPPHIRE
    simsimd_dot_f16_sapphire(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_f16_haswell(a, b, n, d);
#else
    simsimd_dot_f16_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_bf16(simsimd_bf16_t const *a, simsimd_bf16_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *d) {
#if SIMSIMD_TARGET_GENOA
    simsimd_dot_bf16_genoa(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_bf16_haswell(a, b, n, d);
#elif SIMSIMD_TARGET_NEON_BF16
    simsimd_dot_bf16_neon(a, b, n, d);
#else
    simsimd_dot_bf16_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_e4m3(simsimd_e4m3_t const *a, simsimd_e4m3_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SAPPHIRE
    simsimd_dot_e4m3_sapphire(a, b, n, d);
#elif SIMSIMD_TARGET_GENOA
    simsimd_dot_e4m3_genoa(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_dot_e4m3_skylake(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_e4m3_haswell(a, b, n, d);
#else
    simsimd_dot_e4m3_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_e5m2(simsimd_e5m2_t const *a, simsimd_e5m2_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SAPPHIRE
    simsimd_dot_e5m2_sapphire(a, b, n, d);
#elif SIMSIMD_TARGET_GENOA
    simsimd_dot_e5m2_genoa(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_dot_e5m2_skylake(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_e5m2_haswell(a, b, n, d);
#else
    simsimd_dot_e5m2_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_f32(simsimd_f32_t const *a, simsimd_f32_t const *b, simsimd_size_t n,
                                    simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE
    simsimd_dot_f32_sve(a, b, n, d);
#elif SIMSIMD_TARGET_NEON
    simsimd_dot_f32_neon(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_dot_f32_skylake(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_f32_haswell(a, b, n, d);
#else
    simsimd_dot_f32_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_f64(simsimd_f64_t const *a, simsimd_f64_t const *b, simsimd_size_t n,
                                    simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE
    simsimd_dot_f64_sve(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_dot_f64_skylake(a, b, n, d);
#else
    simsimd_dot_f64_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_f16c(simsimd_f16c_t const *a, simsimd_f16c_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE_F16
    simsimd_dot_f16c_sve(a, b, n, d);
#elif SIMSIMD_TARGET_NEON_F16
    simsimd_dot_f16c_neon(a, b, n, d);
#elif SIMSIMD_TARGET_SAPPHIRE
    simsimd_dot_f16c_sapphire(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_f16c_haswell(a, b, n, d);
#else
    simsimd_dot_f16c_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_bf16c(simsimd_bf16c_t const *a, simsimd_bf16c_t const *b, simsimd_size_t n,
                                      simsimd_distance_t *d) {
#if SIMSIMD_TARGET_GENOA
    simsimd_dot_bf16c_genoa(a, b, n, d);
#elif SIMSIMD_TARGET_NEON_BF16
    simsimd_dot_bf16c_neon(a, b, n, d);
#else
    simsimd_dot_bf16c_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_f32c(simsimd_f32c_t const *a, simsimd_f32c_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE
    simsimd_dot_f32c_sve(a, b, n, d);
#elif SIMSIMD_TARGET_NEON
    simsimd_dot_f32c_neon(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_dot_f32c_skylake(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_dot_f32c_haswell(a, b, n, d);
#else
    simsimd_dot_f32c_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_dot_f64c(simsimd_f64c_t const *a, simsimd_f64c_t const *b, simsimd_size_t n,
                                     simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE
    simsimd_dot_f64c_sve(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_dot_f64c_skylake(a, b, n, d);
#else
    simsimd_dot_f64c_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_vdot_f16c(simsimd_f16c_t const *a, simsimd_f16c_t const *b, simsimd_size_t n,
                                      simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE
    simsimd_vdot_f16c_sve(a, b, n, d);
#elif SIMSIMD_TARGET_NEON_F16
    simsimd_vdot_f16c_neon(a, b, n, d);
#elif SIMSIMD_TARGET_SAPPHIRE
    simsimd_vdot_f16c_sapphire(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_vdot_f16c_haswell(a, b, n, d);
#else
    simsimd_vdot_f16c_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_vdot_bf16c(simsimd_bf16c_t const *a, simsimd_bf16c_t const *b, simsimd_size_t n,
                                       simsimd_distance_t *d) {
#if SIMSIMD_TARGET_GENOA
    simsimd_vdot_bf16c_genoa(a, b, n, d);
#elif SIMSIMD_TARGET_NEON_BF16
    simsimd_vdot_bf16c_neon(a, b, n, d);
#else
    simsimd_vdot_bf16c_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_vdot_f32c(simsimd_f32c_t const *a, simsimd_f32c_t const *b, simsimd_size_t n,
                                      simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE
    simsimd_vdot_f32c_sve(a, b, n, d);
#elif SIMSIMD_TARGET_NEON
    simsimd_vdot_f32c_neon(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_vdot_f32c_skylake(a, b, n, d);
#elif SIMSIMD_TARGET_HASWELL
    simsimd_vdot_f32c_haswell(a, b, n, d);
#else
    simsimd_vdot_f32c_serial(a, b, n, d);
#endif
}
SIMSIMD_PUBLIC void simsimd_vdot_f64c(simsimd_f64c_t const *a, simsimd_f64c_t const *b, simsimd_size_t n,
                                      simsimd_distance_t *d) {
#if SIMSIMD_TARGET_SVE
    simsimd_vdot_f64c_sve(a, b, n, d);
#elif SIMSIMD_TARGET_SKYLAKE
    simsimd_vdot_f64c_skylake(a, b, n, d);
#else
    simsimd_vdot_f64c_serial(a, b, n, d);
#endif
}

#endif // !SIMSIMD_DYNAMIC_DISPATCH

#if defined(__cplusplus)
}
#endif

#endif
